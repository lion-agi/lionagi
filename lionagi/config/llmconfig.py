llmconfig = {
    "model": "gpt-4-1106-preview",
    "frequency_penalty": 0,
    "max_tokens": 4000,
    "n": 1,
    "presence_penalty": 0, 
    "response_format": {"type": "text"}, 
    "seed": None, 
    "stop": None,
    "stream": False,
    "temperature": 0.7, 
    "top_p": 1, 
    "tools": None,
    "tool_choice": "none", 
    "user": None
    }