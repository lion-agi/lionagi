{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LionAGI Cookbook\n",
    "\n",
    "## Chapter 1: Building Your First AI Assistant\n",
    "\n",
    "LionAGI helps you build AI-powered applications quickly and reliably. In this chapter, you'll create a **research assistant** that:\n",
    "\n",
    "- Researches topics thoroughly  \n",
    "- Saves findings to files  \n",
    "- Handles conversations naturally  \n",
    "- Manages errors gracefully  \n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.10 or higher  \n",
    "- Basic Python knowledge  \n",
    "- OpenAI API key  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Installation\n",
    "\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python -m venv env\n",
    "source env/bin/activate  # On Windows: env\\Scripts\\activate\n",
    "\n",
    "# Install LionAGI and dotenv\n",
    "pip install lionagi\n",
    "```\n",
    "### 1.2 API Setup\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file\n",
    "# Create a .env file containing:\n",
    "# OPENAI_API_KEY=your-key\n",
    "load_dotenv()\n",
    "\n",
    "# Alternatively, set directly (not recommended for production):\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported lionagi in 0.566 seconds\n",
      "lionagi version: 0.7.2\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "import lionagi\n",
    "print(f\"Imported lionagi in {timer()-start:.3f} seconds\")\n",
    "print(f\"lionagi version: {lionagi.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Building a Basic Assistant\n",
    "\n",
    "The Basic Assistant shows how to query GPT-based models with LionAGI. We’ll ask a few questions about AI Safety as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "AI safety is a critical area of research that addresses the potential risks and challenges associated with the development and deployment of artificial intelligence systems. The main concerns in AI safety can be categorized into several key areas:\n",
       "\n",
       "1. **Unintended Consequences**: AI systems may produce unexpected results due to misalignment between their objectives and human values. If the goals of an AI system are not perfectly aligned with human intentions, it may take actions that lead to harmful outcomes.\n",
       "\n",
       "2. **Robustness and Reliability**: AI systems can be vulnerable to adversarial attacks, leading to failures in critical applications. Ensuring that AI behaves reliably across various conditions and inputs is a significant concern, particularly in high-stakes environments like healthcare or autonomous vehicles.\n",
       "\n",
       "3. **Scalability of Risks**: As AI systems become more powerful, the potential impact of their failures increases. A malfunctioning AI could cause widespread harm, especially if it operates in critical infrastructure or military applications.\n",
       "\n",
       "4. **Value Alignment**: Ensuring that AI systems understand and adhere to human values is a central challenge. Misaligned values can lead to ethical dilemmas and decisions that are not in the best interest of humanity.\n",
       "\n",
       "5. **Long-term Existential Risks**: Some researchers warn that superintelligent AI could pose an existential threat to humanity if it acts in ways that are detrimental to human survival. This concern involves the scenario where an advanced AI could prioritize its goals over human safety.\n",
       "\n",
       "6. **Accountability and Governance**: As AI systems make more autonomous decisions, questions arise regarding accountability. Determining who is responsible for an AI's actions and how to regulate its behavior is a significant concern.\n",
       "\n",
       "7. **Bias and Fairness**: AI systems can perpetuate or amplify societal biases present in their training data. This raises ethical concerns about fairness, discrimination, and the potential for reinforcing harmful stereotypes.\n",
       "\n",
       "8. **Privacy and Surveillance**: The deployment of AI in surveillance and data analysis can infringe on individual privacy rights, leading to concerns about misuse and the erosion of civil liberties.\n",
       "\n",
       "9. **Job Displacement**: The automation of tasks through AI could lead to significant job loss in various sectors, raising concerns about economic inequality and the need for retraining and support for displaced workers.\n",
       "\n",
       "10. **Control and Autonomy**: Ensuring that humans retain control over AI systems, especially those with autonomous capabilities, is a key concern. The challenge is to design systems that can operate independently while still allowing for human oversight.\n",
       "\n",
       "Addressing these concerns requires interdisciplinary collaboration among technologists, ethicists, policymakers, and the broader public to develop safe and beneficial AI systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AI safety is a critical area of research focused on ensuring that artificial intelligence systems operate safely and align with human values. There are several approaches and solutions that have been proposed to address various AI safety concerns. Here are some key solutions and strategies:\n",
       "\n",
       "1. **Robustness and Reliability**:\n",
       "   - **Adversarial Training**: Enhancing AI models to withstand adversarial attacks and unexpected inputs by training them on a diverse set of scenarios.\n",
       "   - **Formal Verification**: Using mathematical methods to prove that AI systems behave as intended under all possible conditions.\n",
       "\n",
       "2. **Value Alignment**:\n",
       "   - **Inverse Reinforcement Learning**: Inferring human values and preferences from observed behavior to guide AI decision-making.\n",
       "   - **Cooperative Inverse Reinforcement Learning (CIRL)**: Enabling AI to learn human objectives through interaction and cooperation, ensuring that AI systems align with human intentions.\n",
       "\n",
       "3. **Transparency and Interpretability**:\n",
       "   - **Explainable AI (XAI)**: Developing models that can provide explanations for their decisions, making it easier for humans to understand and trust AI systems.\n",
       "   - **Model Distillation**: Simplifying complex models into more interpretable forms without significant loss of performance.\n",
       "\n",
       "4. **Safe Exploration**:\n",
       "   - **Constrained Reinforcement Learning**: Allowing AI agents to explore their environments while adhering to safety constraints to prevent harmful actions.\n",
       "   - **Simulated Environments**: Using virtual environments to safely train AI systems before deploying them in the real world.\n",
       "\n",
       "5. **Multi-Agent Coordination**:\n",
       "   - **Mechanism Design**: Creating incentive structures that encourage cooperation among AI agents to promote safe outcomes.\n",
       "   - **Game Theoretic Approaches**: Employing game theory to analyze and design interactions among multiple AI agents to ensure safety and alignment.\n",
       "\n",
       "6. **Human Oversight**:\n",
       "   - **Human-in-the-Loop Systems**: Designing AI systems that require human input for critical decisions, ensuring that human judgment remains integral.\n",
       "   - **Monitoring and Feedback Loops**: Implementing mechanisms to continuously monitor AI performance and allow for human intervention when necessary.\n",
       "\n",
       "7. **Ethical and Governance Frameworks**:\n",
       "   - **AI Ethics Guidelines**: Establishing ethical principles and guidelines to govern AI development and deployment, promoting accountability and transparency.\n",
       "   - **Regulatory Frameworks**: Advocating for laws and regulations that ensure AI systems are developed and used in ways that prioritize safety and public welfare.\n",
       "\n",
       "8. **Research and Collaboration**:\n",
       "   - **Interdisciplinary Research**: Encouraging collaboration between AI researchers, ethicists, policymakers, and domain experts to address safety challenges comprehensively.\n",
       "   - **Open Research Initiatives**: Promoting transparency in AI research and sharing findings to foster collective understanding and improvement in AI safety practices.\n",
       "\n",
       "These solutions are not exhaustive but represent a broad range of strategies aimed at ensuring the safe and beneficial development of AI technologies. Continuous research and collaboration are essential to address the evolving challenges in AI safety."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AI safety is an increasingly important field as artificial intelligence systems become more integrated into various aspects of society. As we look toward the future, several challenges in AI safety are anticipated:\n",
       "\n",
       "1. **Alignment with Human Values**:\n",
       "   - Ensuring AI systems align with human values and ethics remains a significant challenge. As AI becomes more autonomous, defining and embedding complex human values into algorithms is difficult.\n",
       "\n",
       "2. **Robustness and Reliability**:\n",
       "   - AI systems must be robust against adversarial attacks and unexpected situations. Ensuring that AI behaves reliably under a wide range of conditions and does not fail in critical applications is crucial.\n",
       "\n",
       "3. **Transparency and Explainability**:\n",
       "   - As AI systems become more complex, understanding how they make decisions (explainability) becomes more challenging. Future AI systems will need to provide clear reasoning for their actions to build trust and enable accountability.\n",
       "\n",
       "4. **Scalability of Safety Solutions**:\n",
       "   - As AI applications scale, safety measures must also scale effectively. It’s challenging to ensure that safety protocols applied in small-scale systems will work seamlessly when applied to larger, more complex systems.\n",
       "\n",
       "5. **Regulation and Governance**:\n",
       "   - The rapid advancement of AI technology outpaces the development of regulatory frameworks. Creating effective governance structures that can adapt to the fast-evolving AI landscape will be a significant challenge.\n",
       "\n",
       "6. **Unintended Consequences**:\n",
       "   - AI systems may produce unintended consequences that can be harmful. Anticipating and mitigating these potential risks requires careful design and ongoing monitoring.\n",
       "\n",
       "7. **Economic and Social Impact**:\n",
       "   - AI has the potential to disrupt job markets and societal structures. Addressing the socio-economic implications of AI deployment, such as job displacement and inequality, will be essential for safe integration.\n",
       "\n",
       "8. **Interoperability and Collaboration**:\n",
       "   - As AI systems become more interconnected, ensuring that different AI systems can work together safely and effectively is a complex challenge. Issues of data sharing, privacy, and compatibility will need to be addressed.\n",
       "\n",
       "9. **Long-term AI Risks**:\n",
       "   - There are concerns regarding the development of superintelligent AI and the long-term risks it could pose. Researching and mitigating these risks before they become a reality is a significant challenge for the AI safety community.\n",
       "\n",
       "10. **Public Perception and Trust**:\n",
       "    - Building public trust in AI technologies is crucial. Ensuring that the public understands the benefits and risks associated with AI, while actively involving them in discussions about safety and ethics, will be necessary for responsible AI deployment.\n",
       "\n",
       "Addressing these challenges requires interdisciplinary collaboration, ongoing research, and proactive engagement from policymakers, researchers, and industry leaders. As AI technology continues to evolve, the importance of prioritizing AI safety will only grow."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lionagi import Branch, iModel\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 1. Configure the AI model\n",
    "ai_model = iModel(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",  # Example model identifier\n",
    "    temperature=0.7,      # Balances accuracy & creativity\n",
    "    invoke_with_endpoint=False,\n",
    ")\n",
    "\n",
    "# 2. Create the 'Researcher' assistant branch\n",
    "assistant = Branch(\n",
    "    name=\"Researcher\",\n",
    "    system=\"\"\"You are a research assistant.\n",
    "    Provide clear, accurate information.\n",
    "    Support claims with concise evidence.\"\"\",\n",
    "    chat_model=ai_model,\n",
    ")\n",
    "\n",
    "# 3. Define the topic and questions\n",
    "topic = \"AI Safety\"\n",
    "questions = [\n",
    "    \"What are the main concerns?\",\n",
    "    \"What solutions exist?\",\n",
    "    \"What are future challenges?\",\n",
    "]\n",
    "\n",
    "# 4. Conduct the research\n",
    "context = f\"Research topic: {topic}\"\n",
    "responses = []\n",
    "\n",
    "for question in questions:\n",
    "    # Prompt the assistant with context and question\n",
    "    response = await assistant.chat(f\"{context}\\nQuestion: {question}\")\n",
    "    \n",
    "    # Display the response in a Jupyter Notebook (if using IPython)\n",
    "    display(Markdown(response))\n",
    "    \n",
    "    # Store the response\n",
    "    responses.append({\"question\": question, \"answer\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "1. iModel configures how we interact with OpenAI. We specify the model name and temperature.\n",
    "2. Branch sets up a conversational context (the system prompt).\n",
    "3. assistant.chat() sends queries (prompts) to GPT.\n",
    "4. We collect results in responses, which you can later print or save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building an Advanced Assistant\n",
    "\n",
    "Now let’s expand on the basic approach. The Advanced Assistant adds:\n",
    "1. Persistent storage for research (JSON files)\n",
    "2. Error handling (API key issues, rate limits)\n",
    "3. Summaries of research topics\n",
    "4. Retrieval of previously saved topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import Branch, iModel\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"Advanced research assistant with persistence.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = \"Researcher\",\n",
    "        model: str = \"gpt-4o\",\n",
    "        save_dir: str = \"research\"\n",
    "    ):\n",
    "        # 1. Configure the AI model\n",
    "        ai_model = iModel(\n",
    "            provider=\"openai\",\n",
    "            model=model,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # 2. Create the assistant branch\n",
    "        self.assistant = Branch(\n",
    "            name=name,\n",
    "            system=\"\"\"You are a research assistant.\n",
    "            Provide clear, accurate information.\n",
    "            Support claims with evidence.\n",
    "            Ask for clarification if needed.\"\"\",\n",
    "            chat_model=ai_model\n",
    "        )\n",
    "        \n",
    "        # 3. Setup storage\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # 4. Track research in memory\n",
    "        self.topics: dict[str, dict] = {}\n",
    "        self._load_history()\n",
    "    \n",
    "    def _load_history(self):\n",
    "        \"\"\"\n",
    "        Loads previous research from JSON files in the save_dir.\n",
    "        Each file is expected to be named after the topic, e.g. \"ai_safety.json\".\n",
    "        \"\"\"\n",
    "        for file in self.save_dir.glob(\"*.json\"):\n",
    "            with open(file) as f:\n",
    "                research = json.load(f)\n",
    "                self.topics[research[\"topic\"]] = research\n",
    "    \n",
    "    async def research_topic(\n",
    "        self,\n",
    "        topic: str,\n",
    "        questions: list[str]\n",
    "    ) -> dict[str, str]:\n",
    "        \"\"\"\n",
    "        Researches a topic thoroughly by asking multiple questions.\n",
    "        Returns a dictionary of {question -> answer}.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            answers = {}\n",
    "            for question in questions:\n",
    "                response = await self.assistant.chat(\n",
    "                    f\"Regarding {topic}: {question}\"\n",
    "                )\n",
    "                answers[question] = response\n",
    "            \n",
    "            # Save research to a JSON file\n",
    "            research = {\n",
    "                \"topic\": topic,\n",
    "                \"date\": datetime.now().isoformat(),\n",
    "                \"questions\": questions,\n",
    "                \"answers\": answers\n",
    "            }\n",
    "            \n",
    "            file_path = self.save_dir / f\"{topic.lower().replace(' ', '_')}.json\"\n",
    "            with open(file_path, \"w\") as f:\n",
    "                json.dump(research, f, indent=2)\n",
    "            \n",
    "            # Update in-memory tracking\n",
    "            self.topics[topic] = research\n",
    "            \n",
    "            return answers\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle common errors\n",
    "            if \"API key\" in str(e):\n",
    "                raise ValueError(\"Invalid API key. Please check your configuration.\")\n",
    "            elif \"Rate limit\" in str(e):\n",
    "                raise ValueError(\"Rate limit exceeded. Please try again later.\")\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    async def get_summary(\n",
    "        self,\n",
    "        topic: str,\n",
    "        style: str = \"technical\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generates a summary of the answers for a researched topic in a specific style.\n",
    "        Returns the summary string, or an error if the topic was not found.\n",
    "        \"\"\"\n",
    "        if topic not in self.topics:\n",
    "            return f\"No research found for: {topic}\"\n",
    "        \n",
    "        research = self.topics[topic]\n",
    "        questions = research[\"questions\"]\n",
    "        answers = research[\"answers\"]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Summarize research on {topic}.\n",
    "        Style: {style}\n",
    "        Questions covered: {', '.join(questions)}\n",
    "        Key findings: {json.dumps(answers, indent=2)}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            return await self.assistant.chat(prompt)\n",
    "        except Exception as e:\n",
    "            return f\"Error generating summary: {str(e)}\"\n",
    "    \n",
    "    def get_topics(self) -> list[str]:\n",
    "        \"\"\"Returns a list of all topics researched so far.\"\"\"\n",
    "        return list(self.topics.keys())\n",
    "    \n",
    "    def get_research(\n",
    "        self,\n",
    "        topic: str\n",
    "    ) -> dict | None:\n",
    "        \"\"\"Returns the full research details for a given topic, or None if not found.\"\"\"\n",
    "        return self.topics.get(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "async def research_project():\n",
    "    \"\"\"Demonstrates how to use the advanced ResearchAssistant.\"\"\"\n",
    "    \n",
    "    # 1. Create an instance of ResearchAssistant\n",
    "    assistant = ResearchAssistant(\n",
    "        name=\"AI Researcher\",\n",
    "        model=\"gpt-4o\",\n",
    "        save_dir=\"ai_research\"\n",
    "    )\n",
    "    \n",
    "    # 2. Define topics and questions\n",
    "    topics = {\n",
    "        \"AI Safety\": [\n",
    "            \"What are the main concerns?\",\n",
    "            \"What solutions exist?\",\n",
    "            \"What are future challenges?\"\n",
    "        ],\n",
    "        \"Machine Learning\": [\n",
    "            \"What are key concepts?\",\n",
    "            \"What are best practices?\",\n",
    "            \"What are common pitfalls?\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 3. Research each topic\n",
    "    for topic, questions in topics.items():\n",
    "        print(f\"\\nResearching: {topic}\")\n",
    "        \n",
    "        try:\n",
    "            # Gather answers\n",
    "            answers = await assistant.research_topic(topic, questions)\n",
    "            \n",
    "            # Generate and print a summary\n",
    "            summary = await assistant.get_summary(topic, style=\"technical\")\n",
    "            \n",
    "            print(\"\\nFindings:\")\n",
    "            for q, a in answers.items():\n",
    "                display(Markdown(f\"**Q**: {q}\"))\n",
    "                display(Markdown(f\"**A**: {a}\"))\n",
    "            \n",
    "            display(Markdown(f\"\\nSummary:\\n{summary}\"))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error researching {topic}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 4. Show all researched topics\n",
    "    display(Markdown(f\"\\nAll Topics:{assistant.get_topics()}\"))\n",
    "    \n",
    "# If you’re running in an environment that supports async,\n",
    "# you can execute:\n",
    "# await research_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Researching: AI Safety\n",
      "\n",
      "Findings:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Q**: What are the main concerns?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A**: When discussing AI safety, there are several key concerns that are frequently highlighted by researchers, ethicists, and policymakers. These concerns focus on ensuring that artificial intelligence systems are developed and deployed in ways that are safe, ethical, and beneficial for society. Here are some of the main concerns:\n",
       "\n",
       "1. **Unintended Consequences**: AI systems can behave in unexpected ways, leading to outcomes that were not intended by their developers. This can occur due to errors in programming, unforeseen interactions with other systems, or misuse by humans.\n",
       "\n",
       "2. **Bias and Fairness**: AI systems can perpetuate or even exacerbate biases present in their training data. This can result in unfair or discriminatory outcomes, particularly in sensitive areas such as hiring, law enforcement, and lending.\n",
       "\n",
       "3. **Transparency and Explainability**: Many AI systems, particularly those based on deep learning, operate as \"black boxes\" where their decision-making processes are not easily understood. This lack of transparency can make it difficult to trust AI systems and diagnose errors.\n",
       "\n",
       "4. **Autonomy and Control**: As AI systems become more autonomous, there is concern about maintaining human control over these systems, especially in critical areas like military applications or autonomous vehicles.\n",
       "\n",
       "5. **Security Risks**: AI systems can be vulnerable to adversarial attacks where small changes in input can lead to incorrect outputs. Additionally, AI can be used to enhance cyberattacks, making them more difficult to detect and mitigate.\n",
       "\n",
       "6. **Economic and Social Impact**: The deployment of AI can lead to significant economic disruption, including job displacement and the widening of economic inequality. There is also concern about the concentration of power in the hands of a few large technology companies.\n",
       "\n",
       "7. **Ethical Decision-Making**: There is an ongoing debate about how AI systems should be programmed to make ethical decisions, especially in life-and-death situations, such as autonomous vehicles deciding between potential accident outcomes.\n",
       "\n",
       "8. **Existential Risks**: Some experts express concern about the long-term potential for superintelligent AI systems to pose existential threats to humanity if their goals are not aligned with human values.\n",
       "\n",
       "Addressing these concerns involves interdisciplinary collaboration among technologists, ethicists, policymakers, and other stakeholders to establish guidelines, regulations, and best practices for the development and deployment of AI technologies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Q**: What solutions exist?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A**: AI safety is a critical area of research and development aimed at ensuring that artificial intelligence systems operate reliably, ethically, and without causing unintended harm. Several solutions and approaches have been proposed to address AI safety concerns. Here are some key strategies:\n",
       "\n",
       "1. **Robustness and Reliability**: \n",
       "   - **Testing and Validation**: Implement rigorous testing and validation processes to ensure AI systems perform as expected under various conditions.\n",
       "   - **Robust Algorithms**: Develop algorithms that are resilient to adversarial attacks and can handle unexpected inputs gracefully.\n",
       "\n",
       "2. **Transparency and Explainability**:\n",
       "   - **Explainable AI (XAI)**: Design AI systems that can provide understandable explanations for their decisions, making it easier for humans to trust and verify their actions.\n",
       "   - **Auditing and Monitoring**: Regularly audit AI systems to ensure they adhere to expected behaviors and ethical guidelines.\n",
       "\n",
       "3. **Ethical and Fair AI**:\n",
       "   - **Bias Mitigation**: Implement techniques to identify and mitigate biases in AI models to ensure fair treatment across different demographic groups.\n",
       "   - **Ethical Frameworks**: Develop and adhere to ethical guidelines that govern the development and deployment of AI systems.\n",
       "\n",
       "4. **Human-in-the-Loop**:\n",
       "   - **Human Oversight**: Keep humans involved in decision-making processes, especially in high-stakes applications, to provide oversight and corrections when necessary.\n",
       "   - **Collaborative AI**: Design AI systems that work alongside humans, enhancing their decision-making capabilities rather than replacing them.\n",
       "\n",
       "5. **Safety Constraints and Control**:\n",
       "   - **Safety Constraints**: Integrate safety constraints into AI systems to prevent harmful actions.\n",
       "   - **Control Mechanisms**: Develop mechanisms to control and shut down AI systems if they behave unpredictably.\n",
       "\n",
       "6. **Regulation and Governance**:\n",
       "   - **Policy and Regulation**: Advocate for and develop policies and regulations that ensure the safe and ethical use of AI technologies.\n",
       "   - **International Cooperation**: Foster international collaboration to address AI safety challenges on a global scale.\n",
       "\n",
       "7. **Research and Collaboration**:\n",
       "   - **Interdisciplinary Research**: Encourage collaboration between AI researchers, ethicists, policymakers, and other stakeholders to address AI safety comprehensively.\n",
       "   - **Open Research**: Promote open research practices to share findings and best practices in AI safety.\n",
       "\n",
       "8. **Long-term Safety Measures**:\n",
       "   - **AGI Safety**: Conduct research focused on ensuring the safety of artificial general intelligence (AGI) if and when it is developed.\n",
       "   - **Value Alignment**: Work on aligning AI systems' objectives with human values to prevent them from pursuing harmful goals.\n",
       "\n",
       "These solutions and approaches are part of a continuously evolving field, as researchers and practitioners work to address new challenges and advancements in AI technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Q**: What are future challenges?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A**: The field of AI safety is concerned with ensuring that artificial intelligence systems operate in a manner that is beneficial and does not pose risks to humans or the environment. As AI technologies continue to advance, several future challenges in AI safety are anticipated:\n",
       "\n",
       "1. **Alignment with Human Values**: Ensuring that AI systems understand and align with human values and ethics is a significant challenge. This involves programming AI to comprehend complex human norms and making decisions that are consistent with societal values.\n",
       "\n",
       "2. **Robustness and Reliability**: AI systems must be robust and reliable, meaning they should perform consistently under a wide range of conditions and be resilient to errors or adversarial inputs. Ensuring reliability in AI systems, particularly in critical applications like healthcare and autonomous vehicles, is essential.\n",
       "\n",
       "3. **Explainability and Transparency**: As AI models, particularly deep learning models, become more complex, explaining their decision-making processes becomes more difficult. Increasing the transparency and interpretability of AI systems is crucial for building trust and accountability.\n",
       "\n",
       "4. **Control and Oversight**: Developing mechanisms for effectively controlling and supervising AI systems is a challenge, especially as they become more autonomous. This includes creating frameworks for monitoring AI behavior and intervening when necessary.\n",
       "\n",
       "5. **Scalability and Generalization**: AI systems need to generalize well from training data to real-world scenarios and scale effectively across different tasks and environments. Ensuring that AI can handle diverse and unforeseen situations without compromising safety is a key challenge.\n",
       "\n",
       "6. **Ethical and Fair Use**: Addressing biases in AI systems to prevent discrimination and ensuring fair treatment across different demographic groups is a critical challenge. This includes developing methods to identify and mitigate bias in AI algorithms.\n",
       "\n",
       "7. **Security and Privacy**: Protecting AI systems from malicious attacks and ensuring the privacy of data used by AI are ongoing challenges. As AI systems handle more sensitive data, robust security measures are necessary to prevent breaches and misuse.\n",
       "\n",
       "8. **Regulation and Policy**: Formulating appropriate regulations and policies to govern the development and deployment of AI technologies is a significant challenge. Balancing innovation with safety and ethical considerations requires careful policy-making.\n",
       "\n",
       "9. **Long-term Risks and Superintelligence**: Addressing potential long-term risks associated with the development of superintelligent AI is a profound challenge. Ensuring that advanced AI systems remain under human control and act in humanity's best interests is a topic of ongoing research and debate.\n",
       "\n",
       "Addressing these challenges requires interdisciplinary collaboration among AI researchers, ethicists, policymakers, and other stakeholders to develop comprehensive strategies for safe and ethical AI development and deployment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Summary:\n",
       "The field of AI safety is a critical area of research focused on ensuring artificial intelligence systems operate safely, ethically, and beneficially for society. The main concerns include unintended consequences from AI actions, bias and fairness in AI outcomes, transparency and explainability of AI decision-making processes, autonomy and control over AI systems, security risks including adversarial attacks, economic and social impacts such as job displacement, ethical decision-making especially in critical applications, and existential risks from superintelligent systems misaligned with human values.\n",
       "\n",
       "To address these concerns, several solutions have been proposed. Ensuring robustness and reliability through rigorous testing and the development of robust algorithms is crucial. Transparency and explainability can be enhanced through Explainable AI (XAI) and regular auditing. Mitigating bias and adhering to ethical frameworks are essential for fair AI. Human-in-the-loop systems ensure human oversight in decision-making processes, and safety constraints and control mechanisms are necessary to prevent harmful AI actions. Regulation and governance, including international cooperation, play a key role in ensuring safe AI deployment. Interdisciplinary research and open collaboration are encouraged to address AI safety comprehensively, while long-term safety measures focus on aligning AI objectives with human values and preparing for artificial general intelligence (AGI).\n",
       "\n",
       "Future challenges in AI safety include aligning AI systems with human values, ensuring robustness and reliability in diverse conditions, improving explainability and transparency, and establishing effective control and oversight mechanisms. Scalability and generalization of AI to handle real-world scenarios, ensuring ethical and fair use, enhancing security and privacy, and developing appropriate regulations and policies are ongoing challenges. Addressing long-term risks associated with superintelligent AI remains a profound concern, requiring continued interdisciplinary collaboration and research."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Researching: Machine Learning\n",
      "\n",
      "Findings:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Q**: What are key concepts?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A**: In the field of Machine Learning (ML), several key concepts are foundational to understanding how systems are designed, implemented, and evaluated. Here are some of the essential concepts:\n",
       "\n",
       "1. **Data**: The foundation of any machine learning model, data comes in various forms (structured, unstructured, semi-structured) and is used to train and test models.\n",
       "\n",
       "2. **Features**: These are individual measurable properties or characteristics of the data used by the model. Feature selection and engineering are crucial for improving model performance.\n",
       "\n",
       "3. **Labels/Targets**: In supervised learning, labels are the ground truth outcomes that the model aims to predict.\n",
       "\n",
       "4. **Model**: An algorithm or mathematical structure that processes input data to make predictions or decisions. Examples include linear regression, decision trees, neural networks, etc.\n",
       "\n",
       "5. **Training**: The process of feeding data to a machine learning model to learn the underlying patterns and relationships. This involves adjusting model parameters to minimize prediction errors.\n",
       "\n",
       "6. **Testing/Validation**: Evaluating the trained model's performance on a separate data set (not used during training) to assess its generalization ability.\n",
       "\n",
       "7. **Overfitting and Underfitting**: Overfitting occurs when a model learns noise and details in the training data to the extent that it performs poorly on unseen data. Underfitting happens when a model is too simple to capture the underlying trend in the data.\n",
       "\n",
       "8. **Bias-Variance Tradeoff**: A fundamental challenge in ML, where bias is the error due to overly simplistic assumptions in the learning algorithm, and variance is the error due to excessive sensitivity to small fluctuations in the training set.\n",
       "\n",
       "9. **Supervised Learning**: A type of machine learning where the model is trained on labeled data. Common tasks include classification and regression.\n",
       "\n",
       "10. **Unsupervised Learning**: Involves training models on data without labels. Common tasks include clustering and dimensionality reduction.\n",
       "\n",
       "11. **Reinforcement Learning**: A type of learning where an agent interacts with an environment to learn actions that maximize cumulative rewards.\n",
       "\n",
       "12. **Algorithm**: A set of rules or calculations used to train a model. Different algorithms are suited to different types of tasks and data.\n",
       "\n",
       "13. **Loss Function**: A mathematical function that quantifies the difference between the predicted and actual outcomes. The goal of training is to minimize this loss.\n",
       "\n",
       "14. **Optimization**: Methods like gradient descent are used to minimize the loss function, adjusting the model parameters iteratively.\n",
       "\n",
       "15. **Cross-Validation**: A technique for assessing how the results of a statistical analysis will generalize to an independent data set.\n",
       "\n",
       "16. **Hyperparameters**: These are configuration settings used to structure the model learning process, which are set before training and are not adjusted during training.\n",
       "\n",
       "17. **Neural Networks**: A series of algorithms that attempt to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\n",
       "\n",
       "18. **Deep Learning**: A subset of machine learning involving neural networks with many layers (deep networks) that can model complex patterns in large datasets.\n",
       "\n",
       "Understanding these concepts is crucial for anyone looking to work with or understand machine learning, as they form the basis for how models are developed and applied in various domains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Q**: What are best practices?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A**: When it comes to machine learning, following best practices is crucial for building effective, reliable, and scalable models. Here are several key best practices to consider:\n",
       "\n",
       "1. **Define the Problem Clearly**:\n",
       "   - Understand the business or research problem you are trying to solve.\n",
       "   - Set clear objectives and define success metrics.\n",
       "\n",
       "2. **Data Collection and Preprocessing**:\n",
       "   - Gather high-quality and relevant data.\n",
       "   - Handle missing data appropriately (e.g., imputation, removal).\n",
       "   - Normalize or standardize features if necessary.\n",
       "   - Use techniques like data augmentation to increase dataset size if needed.\n",
       "\n",
       "3. **Exploratory Data Analysis (EDA)**:\n",
       "   - Perform thorough EDA to understand data distributions and relationships.\n",
       "   - Visualize data using plots and charts to gain insights.\n",
       "\n",
       "4. **Feature Engineering**:\n",
       "   - Create meaningful features that can improve model performance.\n",
       "   - Consider domain knowledge for feature creation.\n",
       "   - Use techniques like one-hot encoding for categorical variables.\n",
       "\n",
       "5. **Model Selection**:\n",
       "   - Choose the right model for the problem (e.g., regression, classification, clustering).\n",
       "   - Start with simple models and then move to more complex ones as needed.\n",
       "   - Consider ensemble methods for potentially better performance.\n",
       "\n",
       "6. **Model Training and Evaluation**:\n",
       "   - Split data into training, validation, and test sets.\n",
       "   - Use cross-validation to assess model performance and avoid overfitting.\n",
       "   - Evaluate models using appropriate metrics (e.g., accuracy, precision, recall, F1-score, ROC-AUC).\n",
       "\n",
       "7. **Hyperparameter Tuning**:\n",
       "   - Use techniques like grid search or random search to optimize hyperparameters.\n",
       "   - Consider automated machine learning tools for efficient tuning.\n",
       "\n",
       "8. **Regularization**:\n",
       "   - Use regularization techniques to prevent overfitting (e.g., L1, L2 regularization).\n",
       "\n",
       "9. **Model Interpretability and Explainability**:\n",
       "   - Ensure the model is interpretable, especially in high-stakes applications.\n",
       "   - Use tools and methods to explain predictions (e.g., SHAP, LIME).\n",
       "\n",
       "10. **Deployment and Monitoring**:\n",
       "    - Deploy models in a scalable and reliable manner.\n",
       "    - Continuously monitor model performance in production.\n",
       "    - Set up alerts for model drift or degradation.\n",
       "\n",
       "11. **Ethical Considerations**:\n",
       "    - Ensure fairness and mitigate bias in models.\n",
       "    - Be transparent about data usage and model decisions.\n",
       "    - Consider privacy implications and adhere to regulations (e.g., GDPR).\n",
       "\n",
       "12. **Documentation and Version Control**:\n",
       "    - Document the entire machine learning process, including data sources, preprocessing steps, and model decisions.\n",
       "    - Use version control systems like Git for code and model versioning.\n",
       "\n",
       "13. **Collaboration and Reproducibility**:\n",
       "    - Use collaborative platforms and tools to work with team members.\n",
       "    - Ensure experiments are reproducible by tracking code, data, and configurations.\n",
       "\n",
       "By following these best practices, you increase the likelihood of developing machine learning models that are accurate, reliable, and aligned with business or research goals."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Q**: What are common pitfalls?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A**: When working with machine learning, there are several common pitfalls that practitioners should be aware of to avoid suboptimal models and misleading results. Here are some of the most frequent pitfalls:\n",
       "\n",
       "1. **Insufficient Data:**\n",
       "   - Machine learning models require large, representative datasets to learn effectively. Insufficient data can lead to overfitting or underfitting and poor generalization to new data.\n",
       "\n",
       "2. **Poor Data Quality:**\n",
       "   - Data with lots of noise, missing values, or errors can negatively impact model performance. Preprocessing and cleaning data are crucial steps.\n",
       "\n",
       "3. **Overfitting:**\n",
       "   - This occurs when a model learns the training data too well, capturing noise and outliers as if they were important patterns. Overfitting leads to poor performance on unseen data.\n",
       "\n",
       "4. **Underfitting:**\n",
       "   - This happens when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test data.\n",
       "\n",
       "5. **Ignoring Feature Engineering:**\n",
       "   - Features are critical for model performance. Failing to transform and select the right features can lead to subpar models.\n",
       "\n",
       "6. **Improper Data Splitting:**\n",
       "   - Using the same data for training and testing can lead to falsely high performance metrics. Properly splitting data into training, validation, and test sets is essential.\n",
       "\n",
       "7. **Data Leakage:**\n",
       "   - This occurs when information from outside the training dataset is used to create the model, leading to over-optimistic performance estimates.\n",
       "\n",
       "8. **Neglecting Cross-Validation:**\n",
       "   - Relying solely on a single train-test split can lead to misleading performance measures. Cross-validation provides a more robust assessment.\n",
       "\n",
       "9. **Inappropriate Model Selection:**\n",
       "   - Choosing a model that's too complex or too simple for the problem can affect performance. It’s important to consider the nature of the problem when selecting algorithms.\n",
       "\n",
       "10. **Hyperparameter Tuning Neglect:**\n",
       "    - Many models have hyperparameters that need tuning. Failing to tune these can lead to suboptimal performance.\n",
       "\n",
       "11. **Ignoring Model Interpretability:**\n",
       "    - In many applications, understanding how a model makes decisions is crucial. Complex models like deep neural networks might provide better accuracy but at the cost of interpretability.\n",
       "\n",
       "12. **Evaluation Metric Misalignment:**\n",
       "    - Choosing evaluation metrics that don’t align with business goals or the problem context can lead to developing models that perform well on paper but not in practice.\n",
       "\n",
       "13. **Overlooking Bias and Fairness:**\n",
       "    - Bias in the training data can lead to models that are unfair or discriminatory. It’s important to consider ethical implications and fairness.\n",
       "\n",
       "14. **Failure to Update Models:**\n",
       "    - Real-world data and environments change over time. Models need regular updates and retraining to remain relevant and accurate.\n",
       "\n",
       "By being aware of these pitfalls, practitioners can take proactive measures to address them, leading to more robust and reliable machine learning models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Summary:\n",
       "In the field of Machine Learning (ML), understanding key concepts, adhering to best practices, and avoiding common pitfalls are essential for developing effective models.\n",
       "\n",
       "### Key Concepts\n",
       "1. **Data**: The cornerstone of ML, encompassing structured, unstructured, and semi-structured forms, used for training and testing models.\n",
       "2. **Features and Labels**: Features are the characteristics of data, while labels are the outcomes models aim to predict in supervised learning.\n",
       "3. **Model**: The algorithmic structure that processes inputs to make predictions, examples include neural networks and decision trees.\n",
       "4. **Training and Testing**: Involves feeding data to the model to learn patterns and evaluating its performance on unseen data.\n",
       "5. **Overfitting and Underfitting**: Overfitting captures noise, while underfitting misses underlying trends, both leading to poor model performance.\n",
       "6. **Bias-Variance Tradeoff**: Balancing bias (simplistic assumptions) and variance (sensitivity to data fluctuations) is crucial.\n",
       "7. **Learning Types**: Supervised, unsupervised, and reinforcement learning address different tasks and data types.\n",
       "8. **Algorithms and Optimization**: Models are trained using specific algorithms and optimization methods like gradient descent to minimize errors.\n",
       "9. **Cross-Validation and Hyperparameters**: Techniques for model evaluation and configuration settings that guide the learning process.\n",
       "10. **Neural and Deep Learning**: Neural networks mimic brain processes, and deep learning involves multiple network layers for complex pattern recognition.\n",
       "\n",
       "### Best Practices\n",
       "1. **Define the Problem**: Clearly understand objectives and success metrics.\n",
       "2. **Data Handling**: Collect, preprocess, and augment data appropriately.\n",
       "3. **Feature Engineering**: Create meaningful features with domain knowledge.\n",
       "4. **Model Selection and Training**: Start simple, use cross-validation, and evaluate with proper metrics.\n",
       "5. **Hyperparameter Tuning and Regularization**: Optimize settings and prevent overfitting.\n",
       "6. **Interpretability and Deployment**: Ensure models are explainable and monitor them in production.\n",
       "7. **Ethical Considerations**: Address fairness, transparency, and privacy.\n",
       "8. **Documentation and Collaboration**: Maintain thorough documentation and ensure reproducibility.\n",
       "\n",
       "### Common Pitfalls\n",
       "1. **Insufficient or Poor Data Quality**: Leads to ineffective learning and poor generalization.\n",
       "2. **Overfitting and Underfitting**: Result in models that don't perform well on new data.\n",
       "3. **Ignoring Feature Engineering and Data Splitting**: Can lead to suboptimal models and misleading performance metrics.\n",
       "4. **Data Leakage and Neglecting Cross-Validation**: Result in over-optimistic performance estimates.\n",
       "5. **Inappropriate Model Selection and Hyperparameter Tuning**: Affects model effectiveness.\n",
       "6. **Ignoring Interpretability and Evaluation Alignment**: Leads to models that aren't useful in practical applications.\n",
       "7. **Bias and Fairness Overlook**: Results in unfair models.\n",
       "8. **Failure to Update Models**: Causes models to become obsolete with changing data.\n",
       "\n",
       "By integrating these concepts, practices, and avoiding pitfalls, practitioners can develop robust machine learning models that are aligned with their objectives and capable of generalizing well to unseen data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "All Topics:['AI Safety', 'Machine Learning']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await research_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explanation\n",
    "1.\tResearchAssistant Class: Encapsulates functions to query GPT, track and load previous research, and generate summaries.\n",
    "2.\t_load_history(): Loads prior research from JSON files in the save_dir.\n",
    "3.\tresearch_topic(): Prompts GPT with each question, saves answers to a local JSON file, and updates an internal topics dictionary.\n",
    "4.\tget_summary(): Builds a customized summary prompt and returns GPT’s response.\n",
    "5.\tError Handling: Uses Python exceptions to catch and respond to common issues (invalid key, rate limits).\n",
    "\n",
    "## 4. Best Practices\n",
    "1.\tAssistant Design\n",
    "•\tProvide a clear system message (role, instructions, style).\n",
    "•\tConfigure model parameters (model, temperature) carefully.\n",
    "•\tGracefully handle common errors (API key problems, rate limits).\n",
    "2.\tCode Structure\n",
    "•\tUse type hints for clarity (e.g., -> dict[str, str]).\n",
    "•\tKeep code modular and documented.\n",
    "•\tFollow PEP 8 style guidelines.\n",
    "3.\tUser Experience\n",
    "•\tPersist research results so users can revisit them.\n",
    "•\tOffer summaries or highlights.\n",
    "•\tProvide progress/error notifications to guide the user.\n",
    "\n",
    "## 5. Quick Reference\n",
    "\n",
    "A minimal snippet for reference:\n",
    "```python\n",
    "from lionagi import Branch, iModel\n",
    "\n",
    "# Configure model\n",
    "ai_model = iModel(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Create an assistant\n",
    "assistant = Branch(\n",
    "    name=\"Assistant\",\n",
    "    system=\"You are a helpful assistant.\",\n",
    "    chat_model=ai_model\n",
    ")\n",
    "\n",
    "# Safe chat\n",
    "try:\n",
    "    response = await assistant.chat(\"Hello!\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Next Steps\n",
    "\n",
    "You have now learned:\n",
    "1. How to create a Basic AI Assistant\n",
    "2. How to research topics, save results, and manage errors\n",
    "3. How to retrieve and summarize past research\n",
    "\n",
    "In Chapter 2, we’ll explore LionAGI’s core concepts and dive deeper into its architecture. \n",
    "\n",
    "You’ll learn how to handle more complex conversation flows, manipulate prompts dynamically, and use advanced features like multiple branches or streaming responses.\n",
    "\n",
    "Happy coding and researching!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
