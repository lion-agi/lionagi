{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is based on a rUv tutorial\n",
    "\n",
    "Reference:  https://gist.github.com/ruvnet/5cf24851841a120198f43e9639dba7a5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `LionAGI` Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Experts (MoE)\n",
    "\n",
    "A **Mixture of Experts (MoE)** is a machine learning approach designed to enhance model performance by using multiple specialized models, called \"experts.\" A gating model dynamically selects the most relevant expert(s) for each input, allowing the system to leverage the most appropriate expertise, thus improving overall accuracy and efficiency.\n",
    "\n",
    "#### How MoE Works\n",
    "\n",
    "1. **Training Experts**:\n",
    "   - Multiple expert models are trained, each specializing in different aspects of the input data or different tasks.\n",
    "   \n",
    "2. **Gating Model**:\n",
    "   - A neural network that dynamically assigns weights to these experts based on the input's features.\n",
    "   - Routes the data to the most suitable expert(s).\n",
    "\n",
    "3. **Combining Outputs**:\n",
    "   - The outputs from the selected experts are combined, usually through a weighted sum, to produce the final result.\n",
    "   - This enables the system to handle complex and diverse tasks more effectively than a single model.\n",
    "\n",
    "#### Benefits of MoE\n",
    "\n",
    "- **Scalability**:\n",
    "  - MoE can scale model capacity without a proportional increase in computational cost.\n",
    "  - Activates only a subset of experts for each input, maintaining high performance while managing resource usage.\n",
    "  \n",
    "- **Efficiency**:\n",
    "  - Ideal for applications requiring high throughput and low latency, such as real-time translation and large-scale recommendation systems.\n",
    "  \n",
    "- **Accuracy**:\n",
    "  - Achieves more accurate and tailored outputs by leveraging specialized knowledge from different experts.\n",
    "\n",
    "MoE is a versatile tool that improves the efficiency and effectiveness of AI models in a wide range of applications, making it a powerful approach in modern machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will go through how to implement such a system using agentic framework `lionagi`. \n",
    "\n",
    "First we need to install the package, using `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lionagi==0.2.4     # if in ipynb\n",
    "# !pip install lionagi==0.2.4       # if on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.4\n"
     ]
    }
   ],
   "source": [
    "# let us import lionagi and check the version\n",
    "import lionagi as li\n",
    "\n",
    "print(li.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging, which is for debugging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, the `mixture-of-experts` works by having multiple models working on the same task, \n",
    "\n",
    "but with different configuration. We will randomly create these configuration, including\n",
    "- model (gpt-3.5-turbo, gpt-4o, gpt-4-turbo)\n",
    "- temperature\n",
    "- top_p\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "models = [\n",
    "    \"google/gemini-pro-1.5\",\n",
    "    \"google/gemini-flash-1.5\",\n",
    "    \"openai/gpt-4o\",\n",
    "    \"openai/gpt-4o-mini\",\n",
    "    \"meta-llama/llama-3.1-405b-instruct\"\n",
    "]\n",
    "\n",
    "def get_random_config(\n",
    "    temperature_range = (0.7, 1.2),\n",
    "    top_p_range = (0.8, 1.0),\n",
    "    frequency_penalty_range = (0.0, 0.5),\n",
    "    presence_penalty_range = (0.0, 0.5),\n",
    "    max_tokens = 200,  \n",
    "):\n",
    "    return {\n",
    "        \"provider\": \"openrouter\",\n",
    "        \"api_key_schema\": \"OPENROUTER_API_KEY\",\n",
    "        \"model\": random.choice(models),\n",
    "        \"temperature\": random.uniform(*temperature_range),\n",
    "        \"top_p\": random.uniform(*top_p_range),\n",
    "        \"frequency_penalty\": random.uniform(*frequency_penalty_range),\n",
    "        \"presence_penalty\": random.uniform(*presence_penalty_range),\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "# a helper function to check does the model output end with a complete sentence\n",
    "\n",
    "def check_output_completeness(output: str) -> bool:\n",
    "    if output.endswith(\".\") or output.endswith(\"!\") or output.endswith(\"?\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert Models\n",
    "\n",
    "We will use `iModel` from `lionagi` as base class for the expert model and extend functionalities. \n",
    "\n",
    "`iModel` in `lionagi` helps interaction of the system with AI Models such as LLMs. It can, \n",
    "- call API endpoint, such as chat completions, embeddings, \n",
    "- calculate perplexity score, (a measure used in information)\n",
    "- token rate limit control\n",
    "- configuration customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the `ExpertModel` class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "class ExpertModel(li.iModel):\n",
    "    \n",
    "    def __init__(self, total_reward=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # total reward is the sum of intrinsic and extrinsic rewards\n",
    "        # it is used as a reinforcement signal\n",
    "        self.total_reward = total_reward\n",
    "    \n",
    "    # a class method to generate a random expert model instance\n",
    "    @classmethod\n",
    "    def random_expert(\n",
    "        cls, \n",
    "        temperature_range = (0.7, 1.2),\n",
    "        top_p_range = (0.8, 1.0),\n",
    "        frequency_penalty_range = (0.0, 0.5),\n",
    "        presence_penalty_range = (0.0, 0.5),\n",
    "        max_tokens = 1000,  \n",
    "    ):\n",
    "        config = get_random_config(\n",
    "            temperature_range=temperature_range,\n",
    "            top_p_range=top_p_range,\n",
    "            frequency_penalty_range=frequency_penalty_range,\n",
    "            presence_penalty_range=presence_penalty_range,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return cls(**config)\n",
    "\n",
    "    # the main function to call chat completion\n",
    "    # where expert generates output based on instruction and context\n",
    "    async def generate_complete_output(\n",
    "        self,\n",
    "        instruction=None, \n",
    "        context=None,\n",
    "        system=None,\n",
    "        idx=0,\n",
    "    ):\n",
    "        # in Branch, we can chat with the model, by\n",
    "        # passing it into the imodel parameteter, which in this case is self\n",
    "        # since the class inherits from iModel\n",
    "        branch = li.Branch(system=system, imodel=self)\n",
    "        print(f\"Generating output for Expert {idx+1}...\")\n",
    "        \n",
    "        # this is the chat function to get model output\n",
    "        output = await branch.chat(instruction=instruction, context=context)\n",
    "        \n",
    "        # we check if the output is complete\n",
    "        if not check_output_completeness(output):\n",
    "            \n",
    "            # if not complete, we ask the model to continue and complete the sentence\n",
    "            output += await branch.chat('continue and complete the previous sentence')\n",
    "\n",
    "        # we now display the configuration of the expert model\n",
    "        # and show the output\n",
    "        config = self.to_dict()\n",
    "        print(f\"\\nLLM Parameters for Expert {idx+1}:\")\n",
    "        print(\"------------------------\")\n",
    "        print(f\"Model: {config['model']}\")\n",
    "        print(f\"Max Tokens per chunk: {config['max_tokens']}\")\n",
    "        print(f\"Temperature: {config['temperature']}\")\n",
    "        print(f\"Top P: {config['top_p']}\")\n",
    "        print(f\"Frequency Penalty: {config['frequency_penalty']}\")\n",
    "        print(f\"Presence Penalty: {config['presence_penalty']}\")\n",
    "        print(\"------------------------\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # a second component of mixture of experts is to choose the best output\n",
    "    # out of a group of expert outputs\n",
    "    # here we will lionagi branch.direct to score the outputs\n",
    "    async def select_best_output(self, candidates, context, n_judge=3):\n",
    "        \n",
    "        context = {\"background\": context}\n",
    "        \n",
    "        async def inner_score(candidate):\n",
    "            # since we are using asyncio, we need to copy the context\n",
    "            _context = context.copy()\n",
    "            \n",
    "            # we add the candidate to the context\n",
    "            _context[\"candidate\"] = candidate\n",
    "\n",
    "            branch = li.Branch(imodel=self)\n",
    "            form = await branch.direct(\n",
    "                system=\"Act as a critical judge\", \n",
    "                instruction=\"Basing on context, score the text for relevance and coherence\",\n",
    "                context = _context,\n",
    "                score=True,\n",
    "                score_range=(0, 1),\n",
    "                score_num_digits=3,\n",
    "                retries=3,\n",
    "            )\n",
    "            return form.score\n",
    "        \n",
    "        # we use a number of judges to score the outputs\n",
    "        # and take the average score to compare with other outputs\n",
    "        # we run the scoring in parallel for all judges\n",
    "        async def get_avg_score(candidate):\n",
    "            task = [inner_score(candidate) for _ in range(n_judge)]\n",
    "            return sum(await asyncio.gather(*task)) / n_judge\n",
    "        \n",
    "        print(\"Scoring the outputs...\")\n",
    "        \n",
    "        # we run the scoring in parallel for all candidates\n",
    "        # you can use alcall to run a function across all inputs in parallel\n",
    "        tasks = [get_avg_score(candidate) for candidate in candidates]\n",
    "        scores = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # we now have the scores for all candidates\n",
    "        # let us sort them and return the best output\n",
    "        outputs = []\n",
    "        for idx, candidate in enumerate(candidates):\n",
    "            outputs.append((idx, candidate, scores[idx]))\n",
    "        \n",
    "        return sorted(outputs, key=lambda x: x[2], reverse=True)[0]\n",
    "    \n",
    "    \n",
    "    # a third component of mixture of experts is to assign intrinsic reward\n",
    "    # to the expert outputs from the scores and additional reward evaluation\n",
    "    async def assign_intrinsic_reward(self, expert_output, context):\n",
    "        \n",
    "        form = await li.direct.select(\n",
    "            system=\"Act as a critical judge\",\n",
    "            imodel=self,\n",
    "            instruction=\"Basing on context, select a judgement for the model output\",\n",
    "            context={\"background\": context, \"candidate\": expert_output},\n",
    "            choices=[\"highly effective\", \"effective\", \"moderate\", \"poor\", \"bad\"],\n",
    "        )\n",
    "\n",
    "        reward_selection = form.selection.lower().strip()\n",
    "        match reward_selection:\n",
    "            case \"highly effective\":\n",
    "                return 1.0\n",
    "            case \"effective\":\n",
    "                return 0.7\n",
    "            case \"moderate\":\n",
    "                return 0.5\n",
    "            case \"poor\":\n",
    "                return 0.2\n",
    "            case _:\n",
    "                return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture of Experts\n",
    "\n",
    "Now we are ready to create a `MixtureOfExperts` class to optimize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts:\n",
    "    \n",
    "    # we initialize the mixture of experts with a number of experts\n",
    "    # then we include the gating model and reward model\n",
    "    # the gating model is used to select the best output from the experts\n",
    "    # the reward model is used to assign intrinsic reward to the expert outputs\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_experts: int = 4, \n",
    "        min_iterations: int = 3, \n",
    "        learning_rate: float = 0.1, \n",
    "        discount_factor: float = 0.95, \n",
    "        exploration_rate: float = 0.2, \n",
    "        max_tokens: int = 1000,\n",
    "    ):\n",
    "        self.num_experts=num_experts\n",
    "        self.experts = [\n",
    "            ExpertModel.random_expert(max_tokens=max_tokens) for _ in range(num_experts)\n",
    "        ]\n",
    "        self.gating_model = ExpertModel.random_expert()\n",
    "        self.reward_model = ExpertModel.random_expert()\n",
    "        self.min_iterations = min_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.selection_history = []\n",
    "    \n",
    "    # the main function to generate expert outputs\n",
    "    async def generate_expert_outputs(\n",
    "        self, instruction, context, system=None, n_judge=3):\n",
    "        tasks = [\n",
    "            expert.generate_complete_output(instruction, context, system, idx) \n",
    "            for idx, expert in enumerate(self.experts)]\n",
    "        candidates = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # first we select the best output from the experts\n",
    "        selected_idx, selected_output, _ = await self.gating_model.select_best_output(\n",
    "            candidates=candidates, \n",
    "            context=context, \n",
    "            n_judge=n_judge)\n",
    "        \n",
    "        # then we calculate the intrinsic reward for the selected output\n",
    "        intrinsic_reward = await self.reward_model.assign_intrinsic_reward(\n",
    "            expert_output=selected_output, context=context)\n",
    "        \n",
    "        # if the selected expert has been selected before\n",
    "        # we randomly select an expert to discriminate repetition\n",
    "        if selected_idx in self.selection_history:\n",
    "            selected_idx = random.randint(0, self.num_experts - 1)\n",
    "        \n",
    "        self.selection_history.append(selected_idx)\n",
    "        print(\"Expert output generation complete!\")\n",
    "        logging.info(\"All expert outputs have been generated and the most relevant expert has been selected.\")\n",
    "\n",
    "        return selected_output, selected_idx, intrinsic_reward\n",
    "\n",
    "    # we add a check to determine if the training should stop\n",
    "    def check_termination_condition(self, iteration: int, total_reward: float) -> bool:\n",
    "        if iteration >= self.min_iterations and total_reward >= 10.0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # exploration rate manages the exploration-exploitation trade-off by varying \n",
    "    # the system's willingness to try different experts.\n",
    "    def update_exploration_rate(self, iteration: int):\n",
    "        self.exploration_rate = max(0.1, 1.0 - (iteration / self.min_iterations))\n",
    "\n",
    "    def update_expert_rewards(self, selected_expert_index: int, reward: float):\n",
    "        experts_values = [expert.total_reward for expert in self.experts]\n",
    "        reward = self.learning_rate * (\n",
    "            reward \n",
    "            + self.discount_factor * max(experts_values) \n",
    "            - self.experts[selected_expert_index].total_reward\n",
    "        )\n",
    "        self.experts[selected_expert_index].total_reward += reward\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Acme Corporation is exploring investment opportunities in emerging technologies. The board seeks insights into which technologies could potentially transform their industry over the next decade.\"\n",
    "instruction = \"Evaluate the potential impact and investment viability of artificial intelligence (AI), blockchain, quantum computing, and biotechnology.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output for Expert 1...\n",
      "Generating output for Expert 2...\n",
      "Generating output for Expert 3...\n",
      "Generating output for Expert 4...\n",
      "\n",
      "LLM Parameters for Expert 3:\n",
      "------------------------\n",
      "Model: google/gemini-pro-flash\n",
      "Max Tokens per chunk: 1000\n",
      "Temperature: 0.8958890919885891\n",
      "Top P: 0.8717363999866303\n",
      "Frequency Penalty: 0.31753362792610845\n",
      "Presence Penalty: 0.18864066902256277\n",
      "------------------------\n",
      "\n",
      "LLM Parameters for Expert 4:\n",
      "------------------------\n",
      "Model: openai/gpt-4o-mini\n",
      "Max Tokens per chunk: 1000\n",
      "Temperature: 0.9787904352444845\n",
      "Top P: 0.9068555273368921\n",
      "Frequency Penalty: 0.18585972529939276\n",
      "Presence Penalty: 0.027041010312771396\n",
      "------------------------\n",
      "\n",
      "LLM Parameters for Expert 2:\n",
      "------------------------\n",
      "Model: meta-llama/llama-3.1-405b-instruct\n",
      "Max Tokens per chunk: 1000\n",
      "Temperature: 1.188752399240009\n",
      "Top P: 0.8016784950838419\n",
      "Frequency Penalty: 0.4295613205518591\n",
      "Presence Penalty: 0.08466937287002219\n",
      "------------------------\n",
      "\n",
      "LLM Parameters for Expert 1:\n",
      "------------------------\n",
      "Model: openai/gpt-4o\n",
      "Max Tokens per chunk: 1000\n",
      "Temperature: 0.921520628750152\n",
      "Top P: 0.8167970330865056\n",
      "Frequency Penalty: 0.3977296086785502\n",
      "Presence Penalty: 0.4144445131020428\n",
      "------------------------\n",
      "Scoring the outputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 11:51:54,976 - INFO - All expert outputs have been generated and the most relevant expert has been selected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert output generation complete!\n",
      "Iteration 1 - Selected Expert: 0, Intrinsic Reward: 1.0\n",
      "Expert Values: [0, 0, 0, 0]\n",
      "Final Expert Output:\n",
      "### Evaluation of Emerging Technologies for Acme Corporation\n",
      "\n",
      "#### 1. Artificial Intelligence (AI)\n",
      "\n",
      "**Potential Impact:**\n",
      "- **Automation and Efficiency:** AI can significantly enhance productivity by automating routine tasks, leading to cost savings and increased efficiency.\n",
      "- **Data Analytics:** AI-driven data analytics can provide deeper insights into customer behavior, market trends, and operational inefficiencies.\n",
      "- **Personalization:** AI enables hyper-personalized customer experiences, driving higher engagement and satisfaction.\n",
      "- **Innovation:** AI fosters innovation in product development, supply chain optimization, and decision-making processes.\n",
      "\n",
      "**Investment Viability:**\n",
      "- **Market Growth:** The AI market is projected to grow substantially over the next decade, offering lucrative investment opportunities.\n",
      "- **Applications Across Industries:** AI's versatility means it can be applied across various sectors including finance, healthcare, retail, and manufacturing.\n",
      "- **Maturity Level:** Many AI technologies are already mature and integrated into business processes, reducing the risk associated with investment.\n",
      "\n",
      "**Recommendation:** High potential impact with strong investment viability. Investing in AI could yield significant returns through improved efficiencies and innovative capabilities.\n",
      "\n",
      "#### 2. Blockchain\n",
      "\n",
      "**Potential Impact:**\n",
      "- **Transparency and Security:** Blockchain provides a secure and transparent way to record transactions, reducing fraud and enhancing trust.\n",
      "- **Supply Chain Management:** It can revolutionize supply chains by ensuring traceability of products from origin to consumer.\n",
      "- **Smart Contracts:** Automating contract execution through smart contracts can reduce administrative overhead and enhance compliance.\n",
      "\n",
      "**Investment Viability:**\n",
      "- **Emerging Use Cases:** While blockchain offers promising applications (e.g., cryptocurrencies), many use cases are still in exploratory phases.\n",
      "- **Regulatory Landscape:** The regulatory environment for blockchain is still evolving, which could pose risks.\n",
      "- **Integration Challenges:** Integrating blockchain into existing systems may require significant changes in infrastructure.\n",
      "\n",
      "**Recommendation:** Moderate potential impact with speculative investment viability. Consider small-scale investments or partnerships to explore specific use cases relevant to Acme Corporation.\n",
      "\n",
      "#### 3. Quantum Computing\n",
      "\n",
      "**Potential Impact:**\n",
      "- **Computational Power:** Quantum computing promises exponential increases in computational power, solving problems currently infeasible for classical computers.\n",
      "- **Breakthroughs in R&D:** It can accelerate research in materials science, pharmaceuticals, cryptography, and complex simulations.\n",
      "- **Optimization Problems:** Quantum algorithms could optimize logistics, financial portfolios, and supply chains far more efficiently than current methods.\n",
      "\n",
      "**Investment Viability:**\n",
      "- **Early Stage Technology:** Quantum computing is still in the experimental stage with limited practical applications at present.\n",
      "- **High Cost of Entry:** Developing or accessing quantum computing resources requires substantial capital investment.\n",
      "- **Long-Term Horizon:** Significant breakthroughs may take years or even decades to materialize fully.\n",
      "\n",
      "**Recommendation:** High potential impact but long-term speculative investment viability. Monitor advancements closely; consider strategic partnerships with quantum research firms or gradual investments as technology matures.\n",
      "\n",
      "#### 4. Biotechnology\n",
      "\n",
      "**Potential Impact:**\n",
      "- **Healthcare Innovation:** Biotechnology drives advancements in medical treatments, personalized medicine, and diagnostic tools.\n",
      "- **Agriculture Improvements:** Biotech innovations can lead to higher crop yields, pest-resistant plants, and sustainable farming practices.\n",
      "- **Environmental Solutions:** Biotech offers solutions for environmental challenges such as waste management and biofuels.\n",
      "\n",
      "**Investment Viability:**\n",
      "- **Strong Market Demand:** There is consistent demand for biotech solutions across healthcare and agriculture sectors.\n",
      "- **Regulatory Hurdles:** Biotech innovations often face stringent regulatory approvals which can delay time-to-market.\n",
      "- **Research Intensive Field:** Success in biotechnology typically requires significant investment in R&D.\n",
      "\n",
      "**Recommendation:** High potential impact with promising investment viability. Focus on niche areas within biotech that align with Acme Corporationâ€™s strategic interests or partner with established biotech firms to leverage their expertise.\n",
      "\n",
      "### Conclusion\n",
      "For Acme Corporation:\n",
      "1. Prioritize investments in Artificial Intelligence due to its immediate applicability and broad industry impact.\n",
      "2. Explore targeted opportunities within Biotechnology where there is alignment with strategic goals or existing expertise.\n",
      "3. Monitor developments in Quantum Computing for future opportunities but proceed cautiously given its nascent stage.\n",
      "4. Investigate specific Blockchain applications that could enhance transparency or security but remain cautious about widespread adoption due to integration challenges.\n",
      "\n",
      "Strategic diversification across these technologies will position Acme Corporation advantageously for future technological transformations while managing risks effectively.\n",
      "------------------------\n",
      "Expert Reward: 0.5292271322850404, Total Reward: 1.5292271322850404\n",
      "Generating output for Expert 1...\n",
      "Generating output for Expert 2...\n",
      "Generating output for Expert 3...\n",
      "Generating output for Expert 4...\n",
      "\n",
      "LLM Parameters for Expert 3:\n",
      "------------------------\n",
      "Model: google/gemini-pro-flash\n",
      "Max Tokens per chunk: 1000\n",
      "Temperature: 0.8958890919885891\n",
      "Top P: 0.8717363999866303\n",
      "Frequency Penalty: 0.31753362792610845\n",
      "Presence Penalty: 0.18864066902256277\n",
      "------------------------\n",
      "\n",
      "LLM Parameters for Expert 4:\n",
      "------------------------\n",
      "Model: openai/gpt-4o-mini\n",
      "Max Tokens per chunk: 1000\n",
      "Temperature: 0.9787904352444845\n",
      "Top P: 0.9068555273368921\n",
      "Frequency Penalty: 0.18585972529939276\n",
      "Presence Penalty: 0.027041010312771396\n",
      "------------------------\n",
      "\n",
      "LLM Parameters for Expert 1:\n",
      "------------------------\n",
      "Model: openai/gpt-4o\n",
      "Max Tokens per chunk: 1000\n",
      "Temperature: 0.921520628750152\n",
      "Top P: 0.8167970330865056\n",
      "Frequency Penalty: 0.3977296086785502\n",
      "Presence Penalty: 0.4144445131020428\n",
      "------------------------\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(moe\u001b[38;5;241m.\u001b[39mmin_iterations):\n\u001b[0;32m----> 5\u001b[0m     final_output, selected_expert_index, intrinsic_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m moe\u001b[38;5;241m.\u001b[39mgenerate_expert_outputs(instruction, context)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Selected Expert: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_expert_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Intrinsic Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintrinsic_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpert Values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [expert\u001b[38;5;241m.\u001b[39mtotal_reward \u001b[38;5;28;01mfor\u001b[39;00m expert \u001b[38;5;129;01min\u001b[39;00m moe\u001b[38;5;241m.\u001b[39mexperts])\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mMixtureOfExperts.generate_expert_outputs\u001b[0;34m(self, instruction, context, system, n_judge)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_expert_outputs\u001b[39m(\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m, instruction, context, system\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_judge\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     31\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m         expert\u001b[38;5;241m.\u001b[39mgenerate_complete_output(instruction, context, system, idx) \n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, expert \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts)]\n\u001b[0;32m---> 34\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# first we select the best output from the experts\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     selected_idx, selected_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgating_model\u001b[38;5;241m.\u001b[39mselect_best_output(\n\u001b[1;32m     38\u001b[0m         candidates\u001b[38;5;241m=\u001b[39mcandidates, \n\u001b[1;32m     39\u001b[0m         context\u001b[38;5;241m=\u001b[39mcontext, \n\u001b[1;32m     40\u001b[0m         n_judge\u001b[38;5;241m=\u001b[39mn_judge)\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mExpertModel.generate_complete_output\u001b[0;34m(self, instruction, context, system, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating output for Expert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# this is the chat function to get model output\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m branch\u001b[38;5;241m.\u001b[39mchat(instruction\u001b[38;5;241m=\u001b[39minstruction, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# we check if the output is complete\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_output_completeness(output):\n\u001b[1;32m     51\u001b[0m     \n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# if not complete, we ask the model to continue and complete the sentence\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/core/session/directive_mixin.py:126\u001b[0m, in \u001b[0;36mDirectiveMixin.chat\u001b[0;34m(self, instruction, context, system, sender, recipient, requested_fields, form, tools, invoke_tool, return_form, strict, rulebook, imodel, clear_messages, use_annotation, retries, delay, backoff_factor, default, timeout, timing, return_branch, images, image_path, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlionagi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageUtil\n\u001b[1;32m    124\u001b[0m     images \u001b[38;5;241m=\u001b[39m ImageUtil\u001b[38;5;241m.\u001b[39mread_image_to_base64(image_path)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m directive\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    127\u001b[0m     instruction\u001b[38;5;241m=\u001b[39minstruction,\n\u001b[1;32m    128\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    129\u001b[0m     sender\u001b[38;5;241m=\u001b[39msender,\n\u001b[1;32m    130\u001b[0m     recipient\u001b[38;5;241m=\u001b[39mrecipient,\n\u001b[1;32m    131\u001b[0m     requested_fields\u001b[38;5;241m=\u001b[39mrequested_fields,\n\u001b[1;32m    132\u001b[0m     form\u001b[38;5;241m=\u001b[39mform,\n\u001b[1;32m    133\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    134\u001b[0m     invoke_tool\u001b[38;5;241m=\u001b[39minvoke_tool,\n\u001b[1;32m    135\u001b[0m     return_form\u001b[38;5;241m=\u001b[39mreturn_form,\n\u001b[1;32m    136\u001b[0m     strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[1;32m    137\u001b[0m     use_annotation\u001b[38;5;241m=\u001b[39muse_annotation,\n\u001b[1;32m    138\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    139\u001b[0m     delay\u001b[38;5;241m=\u001b[39mdelay,\n\u001b[1;32m    140\u001b[0m     backoff_factor\u001b[38;5;241m=\u001b[39mbackoff_factor,\n\u001b[1;32m    141\u001b[0m     default\u001b[38;5;241m=\u001b[39mdefault,\n\u001b[1;32m    142\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    143\u001b[0m     timing\u001b[38;5;241m=\u001b[39mtiming,\n\u001b[1;32m    144\u001b[0m     clear_messages\u001b[38;5;241m=\u001b[39mclear_messages,\n\u001b[1;32m    145\u001b[0m     return_branch\u001b[38;5;241m=\u001b[39mreturn_branch,\n\u001b[1;32m    146\u001b[0m     images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    148\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/core/unit/unit.py:103\u001b[0m, in \u001b[0;36mUnit.chat\u001b[0;34m(self, instruction, context, system, sender, recipient, branch, requested_fields, form, tools, invoke_tool, return_form, strict, rulebook, imodel, clear_messages, use_annotation, return_branch, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mAsynchronously performs a chat operation.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    Any: The processed response.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mretry_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m rcall(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat,\n\u001b[1;32m    105\u001b[0m     instruction\u001b[38;5;241m=\u001b[39minstruction,\n\u001b[1;32m    106\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    107\u001b[0m     system\u001b[38;5;241m=\u001b[39msystem,\n\u001b[1;32m    108\u001b[0m     sender\u001b[38;5;241m=\u001b[39msender,\n\u001b[1;32m    109\u001b[0m     recipient\u001b[38;5;241m=\u001b[39mrecipient,\n\u001b[1;32m    110\u001b[0m     branch\u001b[38;5;241m=\u001b[39mbranch,\n\u001b[1;32m    111\u001b[0m     requested_fields\u001b[38;5;241m=\u001b[39mrequested_fields,\n\u001b[1;32m    112\u001b[0m     form\u001b[38;5;241m=\u001b[39mform,\n\u001b[1;32m    113\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    114\u001b[0m     invoke_tool\u001b[38;5;241m=\u001b[39minvoke_tool,\n\u001b[1;32m    115\u001b[0m     return_form\u001b[38;5;241m=\u001b[39mreturn_form,\n\u001b[1;32m    116\u001b[0m     strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[1;32m    117\u001b[0m     rulebook\u001b[38;5;241m=\u001b[39mrulebook,\n\u001b[1;32m    118\u001b[0m     imodel\u001b[38;5;241m=\u001b[39mimodel,\n\u001b[1;32m    119\u001b[0m     clear_messages\u001b[38;5;241m=\u001b[39mclear_messages,\n\u001b[1;32m    120\u001b[0m     use_annotation\u001b[38;5;241m=\u001b[39muse_annotation,\n\u001b[1;32m    121\u001b[0m     return_branch\u001b[38;5;241m=\u001b[39mreturn_branch,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    123\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/libs/ln_func_call.py:377\u001b[0m, in \u001b[0;36mrcall\u001b[0;34m(func, retries, delay, backoff_factor, default, timeout, timing, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timing:\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m _tcall(\n\u001b[1;32m    372\u001b[0m                 func, \u001b[38;5;241m*\u001b[39margs, err_msg\u001b[38;5;241m=\u001b[39merr_msg, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    373\u001b[0m             ),\n\u001b[1;32m    374\u001b[0m             SysUtil\u001b[38;5;241m.\u001b[39mget_now(datetime_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m start,\n\u001b[1;32m    375\u001b[0m         )\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _tcall(func, \u001b[38;5;241m*\u001b[39margs, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    379\u001b[0m     last_exception \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/libs/ln_func_call.py:492\u001b[0m, in \u001b[0;36m_tcall\u001b[0;34m(func, delay, err_msg, ignore_err, timing, default, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_coroutine_func(func):\n\u001b[0;32m--> 492\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    494\u001b[0m duration \u001b[38;5;241m=\u001b[39m SysUtil\u001b[38;5;241m.\u001b[39mget_now(datetime_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/core/unit/unit_mixin.py:460\u001b[0m, in \u001b[0;36mDirectiveMixin._chat\u001b[0;34m(self, instruction, context, system, sender, recipient, branch, requested_fields, form, tools, invoke_tool, return_form, strict, rulebook, imodel, images, clear_messages, use_annotation, timeout, return_branch, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     instruction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    432\u001b[0m ):\n\u001b[1;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    Handles the chat operation.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m        Any: The processed response.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_chat(\n\u001b[1;32m    461\u001b[0m         context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    462\u001b[0m         instruction\u001b[38;5;241m=\u001b[39minstruction,\n\u001b[1;32m    463\u001b[0m         system\u001b[38;5;241m=\u001b[39msystem,\n\u001b[1;32m    464\u001b[0m         sender\u001b[38;5;241m=\u001b[39msender,\n\u001b[1;32m    465\u001b[0m         recipient\u001b[38;5;241m=\u001b[39mrecipient,\n\u001b[1;32m    466\u001b[0m         requested_fields\u001b[38;5;241m=\u001b[39mrequested_fields,\n\u001b[1;32m    467\u001b[0m         form\u001b[38;5;241m=\u001b[39mform,\n\u001b[1;32m    468\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    469\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[1;32m    470\u001b[0m         invoke_tool\u001b[38;5;241m=\u001b[39minvoke_tool,\n\u001b[1;32m    471\u001b[0m         return_form\u001b[38;5;241m=\u001b[39mreturn_form,\n\u001b[1;32m    472\u001b[0m         strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[1;32m    473\u001b[0m         rulebook\u001b[38;5;241m=\u001b[39mrulebook,\n\u001b[1;32m    474\u001b[0m         imodel\u001b[38;5;241m=\u001b[39mimodel,\n\u001b[1;32m    475\u001b[0m         use_annotation\u001b[38;5;241m=\u001b[39muse_annotation,\n\u001b[1;32m    476\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    477\u001b[0m         branch\u001b[38;5;241m=\u001b[39mbranch,\n\u001b[1;32m    478\u001b[0m         clear_messages\u001b[38;5;241m=\u001b[39mclear_messages,\n\u001b[1;32m    479\u001b[0m         return_branch\u001b[38;5;241m=\u001b[39mreturn_branch,\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/core/unit/unit_mixin.py:389\u001b[0m, in \u001b[0;36mDirectiveMixin._base_chat\u001b[0;34m(self, instruction, system, context, sender, recipient, requested_fields, form, tools, images, invoke_tool, return_form, strict, rulebook, imodel, use_annotation, branch, clear_messages, return_branch, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     branch\u001b[38;5;241m.\u001b[39mset_system(system)\n\u001b[1;32m    375\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_config(\n\u001b[1;32m    376\u001b[0m     system\u001b[38;5;241m=\u001b[39msystem,\n\u001b[1;32m    377\u001b[0m     instruction\u001b[38;5;241m=\u001b[39minstruction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    387\u001b[0m )\n\u001b[0;32m--> 389\u001b[0m payload, completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chatcompletion(\n\u001b[1;32m    390\u001b[0m     imodel\u001b[38;5;241m=\u001b[39mimodel, branch\u001b[38;5;241m=\u001b[39mbranch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    393\u001b[0m imodel \u001b[38;5;241m=\u001b[39m imodel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimodel\n\u001b[1;32m    394\u001b[0m out_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output(\n\u001b[1;32m    395\u001b[0m     payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    396\u001b[0m     completion\u001b[38;5;241m=\u001b[39mcompletion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m     costs\u001b[38;5;241m=\u001b[39mimodel\u001b[38;5;241m.\u001b[39mcosts,\n\u001b[1;32m    406\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/core/unit/unit_mixin.py:125\u001b[0m, in \u001b[0;36mDirectiveMixin._call_chatcompletion\u001b[0;34m(self, imodel, branch, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m imodel \u001b[38;5;241m=\u001b[39m imodel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimodel\n\u001b[1;32m    124\u001b[0m branch \u001b[38;5;241m=\u001b[39m branch \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m imodel\u001b[38;5;241m.\u001b[39mcall_chat_completion(branch\u001b[38;5;241m.\u001b[39mto_chat_messages(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/core/collections/model.py:280\u001b[0m, in \u001b[0;36miModel.call_chat_completion\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_limit:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelLimitExceededError(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeds the limit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_limit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     )\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mserve_chat(\n\u001b[1;32m    281\u001b[0m     messages, required_tokens\u001b[38;5;241m=\u001b[39mnum_tokens, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    282\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/integrations/provider/openrouter.py:133\u001b[0m, in \u001b[0;36mOpenRouterService.serve_chat\u001b[0;34m(self, messages, required_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m payload \u001b[38;5;241m=\u001b[39m PayloadPackage\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[1;32m    127\u001b[0m     msgs,\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoints[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_api(\n\u001b[1;32m    134\u001b[0m         payload, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, required_tokens\u001b[38;5;241m=\u001b[39mrequired_tokens\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m payload, completion\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/libs/ln_api.py:870\u001b[0m, in \u001b[0;36mBaseService.call_api\u001b[0;34m(self, payload, endpoint, method, required_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has not initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m http_session:\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoints[endpoint]\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39m_call_api(\n\u001b[1;32m    871\u001b[0m         http_session\u001b[38;5;241m=\u001b[39mhttp_session,\n\u001b[1;32m    872\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m    873\u001b[0m         base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url,\n\u001b[1;32m    874\u001b[0m         api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[1;32m    875\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    876\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    877\u001b[0m         required_tokens\u001b[38;5;241m=\u001b[39mrequired_tokens,\n\u001b[1;32m    878\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    879\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/lionagi/libs/ln_api.py:614\u001b[0m, in \u001b[0;36mBaseRateLimiter._call_api\u001b[0;34m(self, http_session, endpoint, base_url, api_key, max_attempts, method, payload, required_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m method \u001b[38;5;241m=\u001b[39m APIUtil\u001b[38;5;241m.\u001b[39mapi_method(http_session, method)\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m method(\n\u001b[1;32m    610\u001b[0m     url\u001b[38;5;241m=\u001b[39m(base_url \u001b[38;5;241m+\u001b[39m endpoint),\n\u001b[1;32m    611\u001b[0m     headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[1;32m    612\u001b[0m     json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    613\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m--> 614\u001b[0m     response_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response_json:\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response_json\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/aiohttp/client_reqrep.py:1189\u001b[0m, in \u001b[0;36mClientResponse.json\u001b[0;34m(self, encoding, loads, content_type)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read and decodes JSON response.\"\"\"\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content_type:\n\u001b[1;32m   1192\u001b[0m     ctype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(hdrs\u001b[38;5;241m.\u001b[39mCONTENT_TYPE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/aiohttp/client_reqrep.py:1129\u001b[0m, in \u001b[0;36mClientResponse.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traces:\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m trace\u001b[38;5;241m.\u001b[39msend_response_chunk_received(\n\u001b[1;32m   1132\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body\n\u001b[1;32m   1133\u001b[0m             )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/aiohttp/streams.py:383\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    381\u001b[0m blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadany()\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/aiohttp/streams.py:405\u001b[0m, in \u001b[0;36mStreamReader.readany\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# TODO: should be `if` instead of `while`\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# because waiter maybe triggered on chunk end,\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# without feeding any data\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadany\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_nowait(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lion-core-dev/lib/python3.12/site-packages/aiohttp/streams.py:312\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[0;32m--> 312\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "moe = MixtureOfExperts()\n",
    "max_tokens = 1000\n",
    "\n",
    "for iteration in range(moe.min_iterations):\n",
    "    final_output, selected_expert_index, intrinsic_reward = await moe.generate_expert_outputs(instruction, context)\n",
    "\n",
    "    print(f\"Iteration {iteration+1} - Selected Expert: {selected_expert_index}, Intrinsic Reward: {intrinsic_reward}\")\n",
    "    print(\"Expert Values:\", [expert.total_reward for expert in moe.experts])\n",
    "    print(\"Final Expert Output:\")\n",
    "    print(final_output)\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    # # Get reward from an external expert\n",
    "    # expert_reward = float(input(f\"Enter expert reward for iteration {iteration+1}: \"))\n",
    "\n",
    "    expert_reward = random.uniform(0.3, 1)\n",
    "\n",
    "    # Combine intrinsic and expert rewards\n",
    "    total_reward = intrinsic_reward + expert_reward\n",
    "\n",
    "    print(f\"Expert Reward: {expert_reward}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # Update the value estimate of the selected expert\n",
    "    moe.update_expert_rewards(selected_expert_index, total_reward)\n",
    "\n",
    "    # Update exploration rate based on the current iteration\n",
    "    moe.update_exploration_rate(iteration)\n",
    "\n",
    "    # Check termination condition\n",
    "    if moe.check_termination_condition(iteration, total_reward):\n",
    "        print(\"Termination condition met. Stopping the process.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
