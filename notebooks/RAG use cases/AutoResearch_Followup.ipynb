{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Savior 2\n",
    "\n",
    "Auto-explorative research with **Retrieval Augumented Generation(RAG)** and **AutoFollowup**, using agent framework `lionagi` with `llama_index` as ToolBox\n",
    "\n",
    "**WARNING** : This notebook uses `gpt-4-turbo-preview` for workflow, and can get ***very expensive***\n",
    "\n",
    "You can:\n",
    "1. change workflow model, but that will require context length being managed\n",
    "2. Reduce the number of steps and the number of queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us conduct a research session. We will, \n",
    "- download 20 papers from arxiv as our primary inspiration\n",
    "- give our researcher 5 abstracts to **read** and propose some ideas to explore\n",
    "- ask researcher to **look up** relevant terms and information from **sources**\n",
    "- draft plans and points and present final research proposal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi llama-index llama_hub unstructured pypdf arxiv wikipedia google-search 'unstructured[pdf]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you would like to ignore logging\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.312'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lionagi\n",
    "\n",
    "lionagi.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.25.post1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index.core\n",
    "\n",
    "llama_index.core.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Large Language Model applications in blockchain\"\n",
    "question = \"Research on using LLM for blockchain data time series analysis on high frequency decentralized finance data\"\n",
    "num_papers = 20\n",
    "\n",
    "persist_dir = \".storage/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Data QA Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. ArXiv Index\n",
    "\n",
    "We will download papers using our research topic from **ArXiv**, a popular pre-publish platform for research papers\n",
    "\n",
    "with `LlamaIndex` and build it into a searchable index.\n",
    "\n",
    "- [ArXiv Official Website](https://arxiv.org)\n",
    "\n",
    "- [LlamaIndex Official Website](https://www.llamaindex.ai)\n",
    "\n",
    "- [ArXivReader on LlamaHub](https://llamahub.ai/l/papers-arxiv?from=all) \n",
    "\n",
    "**llama-index arxiv reader is having some issues, please directly download those papers from arxiv.com**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4-turbo-preview\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=OpenAIEmbeddingModelType.TEXT_EMBED_3_LARGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "# from llama_index.core.text_splitter import SentenceSplitter\n",
    "\n",
    "# reader = SimpleDirectoryReader('./papers', required_exts=['.pdf'])\n",
    "# documents = reader.load_data()\n",
    "\n",
    "# splitter = SentenceSplitter(chunk_size=2048, chunk_overlap=50)\n",
    "# nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# index = VectorStoreIndex(nodes)\n",
    "# index.storage_context.persist(persist_dir=\"./arxiv_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already built an index and stored it by running above codes,\n",
    "\n",
    "you can build from storage using the following, just need to find the `index_id` in the **index_store** file\n",
    "\n",
    "- but you **still need** have some abstract of paper or other things as the starting context for `Session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "\n",
    "# you need to update this to your own index id\n",
    "index_id = \"dff2e0fe-2b51-4043-ba7c-498029c22bbd\"\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./arxiv_index\")\n",
    "index = load_index_from_storage(storage_context, index_id=index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "reranker = LLMRerank(choice_batch_size=10, top_n=5)\n",
    "arxiv_engine = index.as_query_engine(node_postprocessors=[reranker], similarity_top_k=3, response_mode= \"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Textbooks\n",
    "\n",
    "use a couple pdf textbooks as references and build a Knowledge Graph Index\n",
    "\n",
    "- [Dive into Deep Learning](https://d2l.ai)\n",
    "\n",
    "- [Blockchain for Dummies - IBM edition](http://gunkelweb.com/coms465/texts/ibm_blockchain.pdf)\n",
    "\n",
    "- [KnowledgeGraphIndex](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import KnowledgeGraphIndex\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=SimpleGraphStore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader, Document\n",
    "\n",
    "# reader = SimpleDirectoryReader(input_files=['d2l-en.pdf'])\n",
    "# docs1 = reader.load_data()\n",
    "# documents1 = [Document(text=\"\".join([x.text for x in docs1]))]\n",
    "\n",
    "# reader = SimpleDirectoryReader(input_files=['ibm_blockchain.pdf'])\n",
    "# docs2 = reader.load_data()\n",
    "# documents2 = [Document(text=\"\".join([x.text for x in docs2]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will take quite a while, I suggest you to minimize the notebook for 0.5-1 hours, or change the source to be shorter in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2l_index = KnowledgeGraphIndex.from_documents(\n",
    "#     documents1,\n",
    "#     max_triplets_per_chunk=2,\n",
    "#     storage_context=storage_context,\n",
    "#     include_embeddings=True,\n",
    "# )\n",
    "\n",
    "# d2l_index.storage_context.persist(f'{persist_dir}/d2l/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc_index = KnowledgeGraphIndex.from_documents(\n",
    "#     documents2,\n",
    "#     max_triplets_per_chunk=2,\n",
    "#     storage_context=storage_context,\n",
    "#     include_embeddings=True,\n",
    "# )\n",
    "\n",
    "# bc_index.storage_context.persist(f'{persist_dir}/bc_ibm/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import KnowledgeGraphIndex, load_index_from_storage\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=SimpleGraphStore(), persist_dir=f'{persist_dir}/d2l/')\n",
    "index_id = '7c52b76a-bd85-4aa8-a167-ab4f468b7dc9' \n",
    "d2l_index = load_index_from_storage(storage_context=storage_context, index_id=index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(graph_store=SimpleGraphStore(), persist_dir=f'{persist_dir}/bc_ibm/')\n",
    "index_id = '4850f9e9-158c-44d3-a3f0-1bfec72dfc6e'\n",
    "bc_index = load_index_from_storage(storage_context=storage_context, index_id=index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "reranker = LLMRerank(choice_batch_size=10, top_n=5)\n",
    "d2l_engine = d2l_index.as_query_engine(node_postprocessors=[reranker], similarity_top_k=3, response_mode= \"tree_summarize\")\n",
    "bc_engine = bc_index.as_query_engine(node_postprocessors=[reranker], similarity_top_k=3, response_mode= \"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. google and wikipedia\n",
    "\n",
    "we will use Google and Wikipedia to clarify certain domain specific terms with LlamaIndex `OpenAI agent`\n",
    "\n",
    "in order to use google search engine you will have to register with google and get an `API_KEY` and a `google_engine`\n",
    "\n",
    "- [Instruction on Getting Google Search API and Engine](https://developers.google.com/custom-search/v1/overview)\n",
    "- [Google Tools on LlamaHub](https://llamahub.ai/l/tools-google_search?from=all)\n",
    "- [Wiki Tools on LlamaHub](https://llamahub.ai/l/tools-wikipedia?from=all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once after you get the API_KEY and Search_Engine save them to `.env` file, \n",
    "\n",
    "GOOGLE_API_KEY='...'\n",
    "\n",
    "GOOGLE_CSE_ID='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-tools-google\n",
    "# %pip install llama-index-tools-wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "google_key_scheme = 'GOOGLE_API_KEY'\n",
    "google_engine_scheme = 'GOOGLE_CSE_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model='gpt-4-turbo-preview', temperature=0.1)\n",
    "\n",
    "# we will create agents for google search and wikipedia querying\n",
    "def create_google_agent(\n",
    "    google_api_key=os.getenv(google_key_scheme), \n",
    "    google_engine=os.getenv(google_engine_scheme), \n",
    "    verbose=False\n",
    "):\n",
    "    from llama_index.agent.openai import OpenAIAgent\n",
    "    from llama_index.core.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "    from llama_index.tools.google import GoogleSearchToolSpec\n",
    "\n",
    "    api_key = google_api_key\n",
    "    search_engine = google_engine\n",
    "    google_spec = GoogleSearchToolSpec(key=api_key, engine=search_engine)\n",
    "\n",
    "    # Wrap the google search tool as it returns large payloads\n",
    "    tools = LoadAndSearchToolSpec.from_defaults(\n",
    "        google_spec.to_tool_list()[0],\n",
    "    ).to_tool_list()\n",
    "\n",
    "    agent = OpenAIAgent.from_tools(tools, verbose=verbose, llm=llm)\n",
    "    return agent\n",
    "\n",
    "def create_wiki_agent(verbose=False):\n",
    "    from llama_index.tools.wikipedia import WikipediaToolSpec\n",
    "    from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "    tool_spec = WikipediaToolSpec()\n",
    "\n",
    "    agent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=verbose, llm=llm)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tools\n",
    "\n",
    "Now we have set up the needed components for tools, let us set up tools for LLM to use. \n",
    "\n",
    "\n",
    "1. write a function definition for the tool, and add google style docstring\n",
    "2. keep track of the responses for source checking\n",
    "3. make these functions into `lionagi.Tool` object\n",
    "\n",
    "we will define the functions as `asynchorous` functions so we can run parallel queries concurrently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_arxiv = []\n",
    "responses_d2l = []\n",
    "responses_bc = []\n",
    "\n",
    "async def query_arxiv(query: str):\n",
    "    \"\"\"\n",
    "    Query a vector index built with papers from arxiv. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = await arxiv_engine.aquery(query)\n",
    "    responses_arxiv.append(response)\n",
    "    \n",
    "    return str(response.response)\n",
    "\n",
    "async def query_d2l(query: str):\n",
    "    \"\"\"\n",
    "    Query a index built from machine learning textbooks. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = await d2l_engine.aquery(query)\n",
    "    responses_d2l.append(response)\n",
    "    \n",
    "    return str(response.response)\n",
    "        \n",
    "async def query_bc(query: str):\n",
    "    \"\"\"\n",
    "    Query a index built from blockchain textbooks. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = await bc_engine.aquery(query)\n",
    "    responses_bc.append(response)\n",
    "    \n",
    "    return str(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_google = []\n",
    "responses_wiki = []\n",
    "\n",
    "# ask gpt to write you google format docstring\n",
    "async def query_google(query: str):\n",
    "    \"\"\"\n",
    "    Search Google and retrieve a natural language answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: A natural language answer obtained from Google search results.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an issue with making the request or parsing the response.\n",
    "    \"\"\"\n",
    "    google_agent = create_google_agent()\n",
    "    response = await google_agent.achat(query)\n",
    "    responses_google.append(response)\n",
    "    return str(response.response)\n",
    "\n",
    "async def query_wiki(query: str):\n",
    "    \"\"\"\n",
    "    Search Wikipedia and retrieve a natural language answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: A natural language answer obtained from Google search results.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an issue with making the request or parsing the response.\n",
    "    \"\"\"\n",
    "    wiki_agent = create_wiki_agent()\n",
    "    response = await wiki_agent.achat(query)\n",
    "    responses_wiki.append(response)\n",
    "    return str(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Workflow and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the functions into LionAGI `Tool` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `func_to_tool` function converts a function with google or rest style docstring into a `Tool` object, \n",
    "\n",
    "which can be used during a `Session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "system = {\n",
    "    \"persona\": \"you are a helpful assistant, perform as a researcher\",\n",
    "    \"notice\": f\"your research topic is on {topic}, and the question is {question}\",\n",
    "    \"requirements\": \"Think step by step\",\n",
    "    \"responsibilities\": \"Researching a specific topic and question, explore and provide specific findings and insights\",\n",
    "    \"deliverable\": \"technical only, ~ 1000-1500 words, briefly explain the core concepts and rationale behind, retain from being vague or general, target audience is highly sophisticated and can judge your work. \"\n",
    "}\n",
    "\n",
    "instruct = f\"\"\"\n",
    "read a few paper abstracts, carefully propose a few unique, creative, pratical and achieveable solutions to \n",
    "solve the research question on the specific topics, notice you can use the query tools in parallel, but your questions need to be all different and specific to the tools. Your final deliverable needs to be highly specific and technical. you have to \n",
    "use every tool at least once, but as extensively as you can. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters\n",
    "Ching Chang , Wei-Yao Wang , Wen-Chih Peng and Tien-Fu Chen\n",
    "National Yang Ming Chiao Tung University, Hsinchu, Taiwan\n",
    "blacksnail789521.cs10@nycu.edu.tw, sf1638.cs05@nctu.edu.tw, {wcpeng, tfchen}@cs.nycu.edu.tw\n",
    "Abstract\n",
    "Multivariate time-series forecasting is vital in vari-\n",
    "ous domains, e.g., economic planning and weather\n",
    "prediction. Deep train-from-scratch models have\n",
    "exhibited effective performance yet require large\n",
    "amounts of data, which limits real-world applica-\n",
    "bility. Recently, researchers have leveraged the rep-\n",
    "resentation learning transferability of pre-trained\n",
    "Large Language Models (LLMs) to handle limited\n",
    "non-linguistic datasets effectively. However, incor-\n",
    "porating LLMs with time-series data presents chal-\n",
    "lenges of limited adaptation due to different com-\n",
    "positions between time-series and linguistic data,\n",
    "and the inability to process multi-scale temporal in-\n",
    "formation. To tackle these challenges, we propose\n",
    "LLM4TS, a framework for time-series forecasting\n",
    "with pre-trained LLMs. LLM4TS consists of a two-\n",
    "stage fine-tuning strategy: the time-series align-\n",
    "ment stage to align LLMs with the nuances of time-\n",
    "series data, and the forecasting fine-tuning stage for\n",
    "downstream time-series forecasting tasks. Further-\n",
    "more, our framework features a novel two-level ag-\n",
    "gregation method that integrates multi-scale tempo-\n",
    "ral data within pre-trained LLMs, enhancing their\n",
    "ability to interpret time-specific information. In ex-\n",
    "periments across 7 time-series forecasting datasets,\n",
    "LLM4TS is superior to existing state-of-the-art\n",
    "methods compared with trained-from-scratch mod-\n",
    "els in full-shot scenarios, and also achieves an av-\n",
    "erage improvement of 6.84% in MSE in few-shot\n",
    "scenarios. In addition, evaluations compared with\n",
    "different self-supervised learning approaches high-\n",
    "light LLM4TS’s effectiveness with representation\n",
    "learning in forecasting tasks.\n",
    "1 Introduction\n",
    "Forecasting is a vital task in multivariate time-series analy-\n",
    "sis, not only for its ability to operate without manual label-\n",
    "ing but also for its importance in practical applications such\n",
    "as economic planning [Lai et al., 2018] and weather predic-\n",
    "tion [Zhou et al., 2021]. Recently, numerous deep train-from-\n",
    "scratch models have been developed for time-series forecast-\n",
    "ing [Nie et al., 2023], although some lean towards unsuper-\n",
    "vised representation learning [Chang et al., 2023] and transfer\n",
    "learning [Zhang et al., 2022; Zhou et al., 2023]. Generally,\n",
    "these approaches aim to employ adept representation learn-\n",
    "ers: first extracting rich representations from the time-series\n",
    "data and then using these representations for forecasting.\n",
    "Achieving an adept representation learner requires suffi-\n",
    "cient training data [Hoffmann et al., 2022], yet in real-world\n",
    "scenarios, there is often a lack of large-scale time-series\n",
    "datasets. For instance, in industrial manufacturing, the sen-\n",
    "sor data for different products cannot be combined for further\n",
    "analysis, leading to limited data for each product type [Yeh\n",
    "et al., 2019]. Recent research has pivoted towards pre-trained\n",
    "LLMs in Natural Language Processing (NLP) [Radford et al.,\n",
    "2019; Touvron et al., 2023] , exploiting their robust represen-\n",
    "tation learning and few-shot learning capabilities. Moreover,\n",
    "these LLMs can adapt to non-linguistic datasets (e.g., images\n",
    "[Lu et al., 2021], audio [Ghosal et al., 2023], tabular data\n",
    "[Hegselmann et al., 2023], and time-series data [Zhou et al.,\n",
    "2023]) by fine-tuning with only a few parameters and limited\n",
    "data. While LLMs are renowned for their exceptional trans-\n",
    "fer learning capabilities across various fields, the domain-\n",
    "specific nuances of time-series data introduce two challenges\n",
    "in leveraging these models for time-series forecasting.\n",
    "The first challenge of employing LLMs for time-series\n",
    "forecasting is their limited adaptation to the unique charac-\n",
    "teristics of time-series data due to LLMs’ initial pre-training\n",
    "focus on the linguistic corpus. While LLMs have been both\n",
    "practically and theoretically proven [Zhou et al., 2023] to be\n",
    "effective in transfer learning across various modalities thanks\n",
    "to their data-independent self-attention mechanism, their pri-\n",
    "mary focus on general text during pre-training causes a short-\n",
    "fall in recognizing key time-series patterns and nuances cru-\n",
    "cial for accurate forecasting. This limitation is evident in ar-\n",
    "eas such as meteorology and electricity forecasting [Zhou et\n",
    "al., 2021], where failing to account for weather patterns and\n",
    "energy consumption trends leads to inaccurate predictions.\n",
    "The second challenge lies in the limited capacity to process\n",
    "multi-scale temporal information. While LLMs are adept at\n",
    "understanding the sequence and context of words, they strug-\n",
    "gle to understand temporal information due to the lack of uti-\n",
    "lizing multi-scale time-related data such as time units (e.g.,\n",
    "seconds, minutes, hours, etc.) and specific dates (e.g., holi-\n",
    "days, significant events). This temporal information is vital\n",
    "in time-series analysis for identifying and predicting patterns\n",
    "arXiv:2308.08469v5  [cs.LG]  18 Jan 2024\n",
    "[Wu et al., 2021]; for instance, in energy management, it is\n",
    "used to address consumption spikes during daytime and in\n",
    "summer/winter, in contrast to the lower demand during the\n",
    "night and in milder seasons [Zhou et al., 2021]. This under-\n",
    "scores the importance of models adept at interpreting multi-\n",
    "scale temporal patterns (hourly to seasonal) for precise energy\n",
    "demand forecasting. However, most LLMs (e.g., [Radford et\n",
    "al., 2019; Touvron et al., 2023]) built on top of the Trans-\n",
    "former architecture do not naturally incorporate multi-scale\n",
    "temporal information, leading to models that fail to capture\n",
    "crucial variations across different time scales.\n",
    "To address the above issues, we propose LLM4TS,\n",
    "a framework for time-series forecasting with pre-trained\n",
    "LLMs. Regarding the first challenge, our framework intro-\n",
    "duces a two-stage fine-tuning approach: the time-series align-\n",
    "ment stage and the forecasting fine-tuning stage. The first\n",
    "stage focuses on aligning the LLMs with the characteristics of\n",
    "time-series data by utilizing the autoregressive objective, en-\n",
    "abling the fine-tuned LLMs to adapt to time-series represen-\n",
    "tations. The second stage is incorporated to learn correspond-\n",
    "ing time-series forecasting tasks. In this manner, our model\n",
    "supports effective performance in full- and few-shot scenar-\n",
    "ios. Notably, throughout both stages, most parameters in the\n",
    "pre-trained LLMs are frozen, thus preserving the model’s in-\n",
    "herent representation learning capability. To overcome the\n",
    "limitation of LLMs in integrating multi-scale temporal infor-\n",
    "mation, we introduce a novel two-level aggregation strategy.\n",
    "This approach embeds multi-scale temporal information into\n",
    "the patched time-series data, ensuring that each patch not only\n",
    "represents the series values but also encapsulates the critical\n",
    "time-specific context. Consequently, LLM4TS emerges as\n",
    "a data-efficient time-series forecaster, demonstrating robust\n",
    "few-shot performance across various datasets (Figure 1).\n",
    "In summary, the paper’s main contributions are as follows:\n",
    "• Aligning LLMs Toward Time-Series Data: To the\n",
    "best of our knowledge, LLM4TS is the first method that\n",
    "aligns pre-trained Large Language Models with time-\n",
    "series characteristics, effectively utilizing existing rep-\n",
    "resentation learning and few-shot learning capabilities.\n",
    "• Multi-Scale Temporal Information in LLMs: To\n",
    "adapt to time-specific information, a two-level aggrega-\n",
    "tion method is proposed to integrate multi-scale tempo-\n",
    "ral data within pre-trained LLMs.\n",
    "• Robust Performance in Forecasting: LLM4TS ex-\n",
    "cels in 7 real-world time-series forecasting benchmarks,\n",
    "outperforming state-of-the-art methods, including those\n",
    "trained from scratch. It also demonstrates strong few-\n",
    "shot capabilities, particularly with only 5% of data,\n",
    "where it surpasses the best baseline that uses 10% of\n",
    "data. This efficiency makes LLM4TS highly relevant for\n",
    "practical, real-world forecasting applications\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import Session\n",
    "\n",
    "# set up a researcher session\n",
    "researcher = Session(system, tools=[query_arxiv, query_bc, query_d2l, query_google, query_wiki])\n",
    "\n",
    "# invoke the task for researcher\n",
    "out = await researcher.followup(instruct, context=context, max_followup=3, temperature=0.7, auto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Leveraging Large Language Models (LLMs) for blockchain data time series analysis, particularly in high-frequency decentralized finance (DeFi) data, presents a novel approach to addressing the unique challenges and exploiting the opportunities within this rapidly evolving sector. This technical exploration synthesizes insights from recent research, blockchain technology fundamentals, machine learning techniques, and current advancements in the field to propose practical, achievable solutions for enhancing time series forecasting in DeFi.\n",
       "\n",
       "### 1. **Enhanced Forecasting and Trend Analysis**\n",
       "\n",
       "#### Rationale:\n",
       "LLMs, with their superior ability to understand complex patterns and generate natural language, can be trained on historical DeFi transaction data to forecast future market trends and asset prices. This capability is crucial in the volatile and fast-paced DeFi market, where accurate predictions can significantly impact investment decisions and risk management strategies.\n",
       "\n",
       "#### Application:\n",
       "Develop a fine-tuned LLM specifically for interpreting high-frequency DeFi data. This model should be trained on a comprehensive dataset encompassing transaction histories, token price movements, liquidity pool dynamics, and smart contract interactions, among other relevant metrics. By treating future values in the time series as \"text\" to be predicted, the LLM can generate forecasts with a higher degree of accuracy.\n",
       "\n",
       "### 2. **Real-time Anomaly Detection and Fraud Prevention**\n",
       "\n",
       "#### Rationale:\n",
       "Anomaly detection in high-frequency trading environments is critical for identifying market manipulation, fraud, or system errors in real-time. LLMs' capability to monitor and analyze transaction data for unusual patterns can enhance the security and integrity of DeFi platforms.\n",
       "\n",
       "#### Application:\n",
       "Implement an LLM-based monitoring system that continuously scans the blockchain for anomalies in transaction patterns, smart contract executions, and liquidity pool behaviors. Utilizing NLP techniques, this system can also analyze social media and news articles for potential market manipulation signals, providing a comprehensive anomaly detection mechanism.\n",
       "\n",
       "### 3. **Sentiment Analysis for Market Dynamics Understanding**\n",
       "\n",
       "#### Rationale:\n",
       "Market sentiment, driven by news articles, social media, and community discussions, plays a significant role in influencing DeFi asset prices and trends. LLMs can analyze textual data to gauge market sentiment towards specific DeFi projects or the overall market, offering valuable insights into market dynamics.\n",
       "\n",
       "#### Application:\n",
       "Create a sentiment analysis tool that leverages LLMs to process and interpret vast amounts of textual data related to DeFi. This tool should analyze news headlines, social media posts, forum discussions, and other relevant sources to provide real-time sentiment scores. These scores can then be integrated into trading algorithms to inform investment strategies.\n",
       "\n",
       "### 4. **Automating Regulatory Compliance and Reporting**\n",
       "\n",
       "#### Rationale:\n",
       "Ensuring compliance with evolving regulatory standards is a significant challenge for DeFi platforms. LLMs can automate the compliance process by analyzing transaction data against regulatory requirements, generating reports, and alerting for suspicious activities.\n",
       "\n",
       "#### Application:\n",
       "Develop a compliance automation tool that uses LLMs to interpret transaction data in the context of current regulatory frameworks. This tool should be capable of generating compliance reports, identifying potential non-compliance issues, and flagging transactions that require further investigation.\n",
       "\n",
       "### 5. **Multi-Scale Temporal Information Integration**\n",
       "\n",
       "#### Rationale:\n",
       "Incorporating multi-scale temporal information is vital for accurately predicting market behaviors that depend on various time frames, from minutes to seasons. Most existing LLMs struggle to integrate such temporal information effectively.\n",
       "\n",
       "#### Application:\n",
       "Introduce a novel two-level aggregation strategy within the LLM framework to embed multi-scale temporal information into the analysis process. This involves creating temporal embeddings that represent different time scales and integrating these embeddings with transaction data, allowing the LLM to better understand and forecast based on time-specific information.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "The application of Large Language Models in analyzing high-frequency decentralized finance data offers promising avenues for enhancing forecasting accuracy, detecting anomalies, understanding market sentiment, ensuring regulatory compliance, and integrating multi-scale temporal information. By addressing the unique challenges of DeFi data analysis with these innovative solutions, stakeholders can unlock significant value, driving forward the evolution of decentralized financial markets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>role</th>\n",
       "      <th>sender</th>\n",
       "      <th>recipient</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3f85d42fcbe567d6bcd39768c86a5c50</td>\n",
       "      <td>2024_03_27T22_55_36_369373+00_00</td>\n",
       "      <td>system</td>\n",
       "      <td>system</td>\n",
       "      <td>assistant</td>\n",
       "      <td>{\"system_info\": {\"persona\": \"you are a helpful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26c4659dd2a8b97137d0121068ef6bc5</td>\n",
       "      <td>2024_03_27T22_55_36_370134+00_00</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"instruction\": {\"NOTICE\": \"\\n    In the curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72f4240550e3630c7c9b1bc9bd67addb</td>\n",
       "      <td>2024_03_27T22_55_41_908084+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_request</td>\n",
       "      <td>action</td>\n",
       "      <td>{\"action_request\": [{\"action\": \"action_query_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0db110123358c7eb67f1ba3d2d5e1b7f</td>\n",
       "      <td>2024_03_27T22_56_33_181238+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_arxiv\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>448c3883920226c1f515578b23175f76</td>\n",
       "      <td>2024_03_27T22_56_33_182386+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_bc\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27b44d79932bb22aecffd294a4542329</td>\n",
       "      <td>2024_03_27T22_56_33_182878+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_d2l\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4a1b547dee925a2c8a99217beafd9f7c</td>\n",
       "      <td>2024_03_27T22_56_33_183354+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d274218cff51754e6c67cfd612ffd3b8</td>\n",
       "      <td>2024_03_27T22_56_33_183807+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_wiki\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>338097ed5e2c413a2c6993fd0ad08adb</td>\n",
       "      <td>2024_03_27T22_56_33_184357+00_00</td>\n",
       "      <td>user</td>\n",
       "      <td>user</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"instruction\": \"\\n    In the current task, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a3b901fc55534386d263830bb59ace85</td>\n",
       "      <td>2024_03_27T22_57_05_794459+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>main</td>\n",
       "      <td>user</td>\n",
       "      <td>{\"response\": \"Leveraging Large Language Models...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            node_id                         timestamp  \\\n",
       "0  3f85d42fcbe567d6bcd39768c86a5c50  2024_03_27T22_55_36_369373+00_00   \n",
       "1  26c4659dd2a8b97137d0121068ef6bc5  2024_03_27T22_55_36_370134+00_00   \n",
       "2  72f4240550e3630c7c9b1bc9bd67addb  2024_03_27T22_55_41_908084+00_00   \n",
       "3  0db110123358c7eb67f1ba3d2d5e1b7f  2024_03_27T22_56_33_181238+00_00   \n",
       "4  448c3883920226c1f515578b23175f76  2024_03_27T22_56_33_182386+00_00   \n",
       "5  27b44d79932bb22aecffd294a4542329  2024_03_27T22_56_33_182878+00_00   \n",
       "6  4a1b547dee925a2c8a99217beafd9f7c  2024_03_27T22_56_33_183354+00_00   \n",
       "7  d274218cff51754e6c67cfd612ffd3b8  2024_03_27T22_56_33_183807+00_00   \n",
       "8  338097ed5e2c413a2c6993fd0ad08adb  2024_03_27T22_56_33_184357+00_00   \n",
       "9  a3b901fc55534386d263830bb59ace85  2024_03_27T22_57_05_794459+00_00   \n",
       "\n",
       "        role           sender  recipient  \\\n",
       "0     system           system  assistant   \n",
       "1       user             user       main   \n",
       "2  assistant   action_request     action   \n",
       "3  assistant  action_response       main   \n",
       "4  assistant  action_response       main   \n",
       "5  assistant  action_response       main   \n",
       "6  assistant  action_response       main   \n",
       "7  assistant  action_response       main   \n",
       "8       user             user       main   \n",
       "9  assistant             main       user   \n",
       "\n",
       "                                             content  \n",
       "0  {\"system_info\": {\"persona\": \"you are a helpful...  \n",
       "1  {\"instruction\": {\"NOTICE\": \"\\n    In the curre...  \n",
       "2  {\"action_request\": [{\"action\": \"action_query_a...  \n",
       "3  {\"action_response\": {\"function\": \"query_arxiv\"...  \n",
       "4  {\"action_response\": {\"function\": \"query_bc\", \"...  \n",
       "5  {\"action_response\": {\"function\": \"query_d2l\", ...  \n",
       "6  {\"action_response\": {\"function\": \"query_google...  \n",
       "7  {\"action_response\": {\"function\": \"query_wiki\",...  \n",
       "8  {\"instruction\": \"\\n    In the current task, yo...  \n",
       "9  {\"response\": \"Leveraging Large Language Models...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "researcher.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seem like the assistant gave answer during the ReAct, instead of presenting outcome in the end, let us check what it did"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Improve work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = await researcher.chat(\n",
    "    \"\"\"\n",
    "    you asked a lot of good questions and got plenty answers, please integrate your \n",
    "    conversation, be a lot more technical, you will be rewarded with 500 dollars for \n",
    "    great work, and punished for subpar work, take a deep breath, you can do it\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Leveraging Large Language Models (LLMs) in the analysis of high-frequency blockchain data, particularly within the decentralized finance (DeFi) domain, presents a multifaceted opportunity to enhance predictive analytics, anomaly detection, sentiment analysis, and regulatory compliance. This technical exposition delves into the practical application of LLMs for time series forecasting of DeFi data, drawing upon insights from recent research, blockchain fundamentals, and advancements in machine learning techniques. The proposed solutions aim to address the unique challenges inherent in the DeFi ecosystem, such as high volatility, complex smart contract interactions, and the rapid pace of transactions.\n",
       "\n",
       "### Enhanced Forecasting and Trend Analysis with LLMs\n",
       "\n",
       "#### Technical Framework:\n",
       "\n",
       "1. **Data Preprocessing and Representation**: Convert high-frequency DeFi transaction data into a structured format amenable to sequence modeling. This involves encoding numerical features (e.g., transaction amounts, token prices) and categorical features (e.g., token types, transaction types) into embeddings that capture the underlying temporal dynamics.\n",
       "\n",
       "2. **Model Architecture**: Utilize a Transformer-based LLM, fine-tuned for the specific nuances of DeFi data. The model should incorporate attention mechanisms to weigh the importance of different parts of the input data sequence, allowing it to capture long-range dependencies and complex patterns in the data.\n",
       "\n",
       "3. **Training Strategy**: Employ a combination of supervised learning for known historical trends and unsupervised or self-supervised learning to uncover latent patterns within the data. Techniques like contrastive learning can be particularly useful in self-supervised settings to enhance the model's ability to differentiate between normal and anomalous patterns.\n",
       "\n",
       "4. **Forecasting Approach**: Frame the forecasting task as a sequence generation problem, where the model predicts future values of the time series based on past observations. This can be facilitated by using an autoregressive model structure where predictions for time \\(t\\) are conditioned on observations up to time \\(t-1\\).\n",
       "\n",
       "### Real-time Anomaly Detection and Fraud Prevention\n",
       "\n",
       "#### Technical Framework:\n",
       "\n",
       "1. **Anomaly Detection Module**: Integrate an anomaly detection component within the LLM framework that specifically targets irregularities in transaction patterns, smart contract executions, and liquidity pool behaviors. This module should leverage the model's ability to understand the normal sequence of transactions and flag deviations in real-time.\n",
       "\n",
       "2. **Adaptive Thresholding**: Implement dynamic thresholding mechanisms based on statistical modeling of transaction data distributions. These thresholds can adapt to changing market conditions and transaction volumes, improving the sensitivity and specificity of anomaly detection.\n",
       "\n",
       "3. **Integration with External Data Sources**: Augment the LLM's capabilities by incorporating external data sources, such as social media feeds, news headlines, and regulatory alerts. This can enhance the model's ability to detect coordinated fraud attempts or market manipulation schemes that may not be evident from transaction data alone.\n",
       "\n",
       "### Sentiment Analysis for Market Dynamics Understanding\n",
       "\n",
       "#### Technical Framework:\n",
       "\n",
       "1. **NLP-Based Sentiment Analysis**: Leverage the LLM's natural language processing capabilities to analyze textual data related to DeFi projects and the broader market. This involves training the model on a dataset of financial news articles, social media posts, and forum discussions, labeled with sentiment scores.\n",
       "\n",
       "2. **Contextual Sentiment Scoring**: Develop a scoring system that accounts for the context and relevance of the textual data to specific DeFi projects or market trends. This requires the model to not only assess the sentiment of the text but also its significance in relation to current market dynamics.\n",
       "\n",
       "3. **Integration with Time Series Analysis**: Combine sentiment analysis outputs with traditional time series forecasting models to provide a more holistic view of market trends. Sentiment scores can serve as additional features in the forecasting model, potentially improving prediction accuracy.\n",
       "\n",
       "### Automating Regulatory Compliance and Reporting\n",
       "\n",
       "#### Technical Framework:\n",
       "\n",
       "1. **Regulatory Rule Encoding**: Encode regulatory requirements and compliance rules as structured data that can be interpreted by the LLM. This includes rules for anti-money laundering (AML), know your customer (KYC) procedures, and transaction reporting obligations.\n",
       "\n",
       "2. **Compliance Monitoring Module**: Design a module within the LLM framework that continuously monitors transaction data for compliance with encoded regulatory rules. This module should be capable of generating real-time alerts for potential non-compliance incidents and automating the generation of compliance reports.\n",
       "\n",
       "3. **Smart Contract Analysis**: Utilize the LLM to analyze and verify the security and compliance of smart contracts underpinning DeFi transactions. This involves interpreting the contract's code and logic in the context of regulatory requirements and identifying potential vulnerabilities or compliance issues.\n",
       "\n",
       "### Multi-Scale Temporal Information Integration\n",
       "\n",
       "#### Technical Framework:\n",
       "\n",
       "1. **Temporal Embeddings**: Develop temporal embeddings that capture multi-scale time-related information, such as time of day, day of the week, and seasonality. These embeddings should be integrated with the transaction data embeddings to provide the LLM with a richer context for analysis.\n",
       "\n",
       "2. **Two-Level Aggregation Strategy**: Implement a two-level aggregation approach within the LLM framework. The first level aggregates data at a finer granularity (e.g., minute-level), capturing short-term patterns and anomalies. The second level aggregates data at a coarser granularity (e.g., daily or weekly), allowing the model to understand longer-term trends and seasonality.\n",
       "\n",
       "3. **Dynamic Time Warping (DTW)**: Employ DTW techniques to align time series data of different lengths and scales, facilitating the comparison and analysis of temporal patterns across different time frames. This can enhance the LLM's ability to forecast based on multi-scale temporal information.\n",
       "\n",
       "In conclusion, applying LLMs to the analysis of high-frequency DeFi data offers a comprehensive framework for enhancing forecasting accuracy, detecting anomalies, understanding market sentiment, ensuring regulatory compliance, and integrating multi-scale temporal information. By addressing the unique challenges of DeFi data analysis with these advanced technical solutions, stakeholders can unlock significant value and drive forward the evolution of decentralized financial markets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. check the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = researcher.default_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>role</th>\n",
       "      <th>sender</th>\n",
       "      <th>recipient</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0db110123358c7eb67f1ba3d2d5e1b7f</td>\n",
       "      <td>2024_03_27T22_56_33_181238+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_arxiv\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>448c3883920226c1f515578b23175f76</td>\n",
       "      <td>2024_03_27T22_56_33_182386+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_bc\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27b44d79932bb22aecffd294a4542329</td>\n",
       "      <td>2024_03_27T22_56_33_182878+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_d2l\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4a1b547dee925a2c8a99217beafd9f7c</td>\n",
       "      <td>2024_03_27T22_56_33_183354+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_google...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d274218cff51754e6c67cfd612ffd3b8</td>\n",
       "      <td>2024_03_27T22_56_33_183807+00_00</td>\n",
       "      <td>assistant</td>\n",
       "      <td>action_response</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"action_response\": {\"function\": \"query_wiki\",...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            node_id                         timestamp  \\\n",
       "0  0db110123358c7eb67f1ba3d2d5e1b7f  2024_03_27T22_56_33_181238+00_00   \n",
       "1  448c3883920226c1f515578b23175f76  2024_03_27T22_56_33_182386+00_00   \n",
       "2  27b44d79932bb22aecffd294a4542329  2024_03_27T22_56_33_182878+00_00   \n",
       "3  4a1b547dee925a2c8a99217beafd9f7c  2024_03_27T22_56_33_183354+00_00   \n",
       "4  d274218cff51754e6c67cfd612ffd3b8  2024_03_27T22_56_33_183807+00_00   \n",
       "\n",
       "        role           sender recipient  \\\n",
       "0  assistant  action_response      main   \n",
       "1  assistant  action_response      main   \n",
       "2  assistant  action_response      main   \n",
       "3  assistant  action_response      main   \n",
       "4  assistant  action_response      main   \n",
       "\n",
       "                                             content  \n",
       "0  {\"action_response\": {\"function\": \"query_arxiv\"...  \n",
       "1  {\"action_response\": {\"function\": \"query_bc\", \"...  \n",
       "2  {\"action_response\": {\"function\": \"query_d2l\", ...  \n",
       "3  {\"action_response\": {\"function\": \"query_google...  \n",
       "4  {\"action_response\": {\"function\": \"query_wiki\",...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch.action_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 5 tool usages during this ReAct session, let us take a look at some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool used is: query_bc\n",
      "question asked: {'query': 'challenges and solutions in analyzing high frequency decentralized finance data'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Analyzing high-frequency decentralized finance (DeFi) data presents several challenges, primarily due to the nature of blockchain technology and the characteristics of DeFi markets. These challenges include:\n",
       "\n",
       "1. **Data Volume and Velocity**: DeFi platforms generate vast amounts of data with each transaction. The high frequency of transactions adds to the complexity, requiring robust systems for data capture and storage.\n",
       "\n",
       "2. **Data Transparency and Accessibility**: While blockchain is inherently transparent, accessing and interpreting the data can be challenging. The decentralized nature means data is spread across numerous nodes, making aggregation and analysis complex.\n",
       "\n",
       "3. **Data Integrity and Security**: Ensuring the integrity and security of data is paramount. Blockchain technology is secure by design, but the analysis process must maintain this security, especially when integrating data from multiple sources.\n",
       "\n",
       "4. **Smart Contract Complexity**: DeFi transactions often involve complex smart contracts. Understanding and analyzing the logic and outcomes of these contracts require specialized knowledge and tools.\n",
       "\n",
       "5. **Regulatory and Compliance Issues**: Navigating the evolving regulatory landscape of DeFi and ensuring compliance add another layer of complexity to data analysis.\n",
       "\n",
       "Solutions to these challenges leverage blockchain technology's inherent strengths and innovative approaches to data analysis:\n",
       "\n",
       "1. **Distributed Ledger Technology (DLT)**: Utilizing DLT for data storage and management can help manage the volume and velocity of DeFi data, ensuring integrity and security.\n",
       "\n",
       "2. **Smart Contracts for Data Management**: Implementing smart contracts can automate data aggregation and analysis processes, reducing the complexity and increasing efficiency.\n",
       "\n",
       "3. **Advanced Analytics and Machine Learning**: Employing advanced analytics techniques and machine learning can help in interpreting complex data patterns, making sense of vast datasets generated by DeFi platforms.\n",
       "\n",
       "4. **Decentralized Data Marketplaces**: Participating in or creating decentralized data marketplaces can improve access to DeFi data, allowing for more comprehensive analysis.\n",
       "\n",
       "5. **Collaboration with Regulatory Bodies**: Working closely with regulators to understand compliance requirements can guide the development of analysis tools that automatically check for compliance, reducing the regulatory burden.\n",
       "\n",
       "6. **Educational and Training Programs**: Developing educational programs to improve understanding of DeFi and blockchain analytics can help analysts and organizations better navigate the complexities of DeFi data.\n",
       "\n",
       "By addressing these challenges with targeted solutions, stakeholders can unlock the full potential of DeFi data, driving innovation and growth in the decentralized finance sector."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lionagi.libs import convert, nested\n",
    "\n",
    "res_dict1 = convert.to_dict(branch.action_response.content.iloc[1])\n",
    "\n",
    "func_ = nested.nget(res_dict1, ['action_response', 'function'])\n",
    "args_ = nested.nget(res_dict1, ['action_response', 'arguments'])\n",
    "output_ = nested.nget(res_dict1, ['action_response', 'output'])\n",
    "\n",
    "print(f\"Tool used is: {func_}\")\n",
    "print(f\"question asked: {args_}\")\n",
    "Markdown(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool used is: query_google\n",
      "question asked: {'query': 'recent advancements in time series forecasting using LLMs in decentralized finance'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Recent advancements in time series forecasting using Large Language Models (LLMs) in decentralized finance (DeFi) have been significant, reflecting the broader trend of applying cutting-edge AI techniques to financial markets. Key advancements include:\n",
       "\n",
       "1. **Improved Accuracy**: LLMs have been fine-tuned to better understand and predict market trends by analyzing vast amounts of historical data, leading to more accurate forecasts of asset prices, interest rates, and market movements.\n",
       "\n",
       "2. **Natural Language Processing (NLP) Enhancements**: The integration of NLP techniques allows LLMs to analyze news articles, social media posts, and other textual data to gauge market sentiment and predict its impact on future market trends. This is particularly useful in the volatile DeFi space, where market sentiment can significantly influence asset prices.\n",
       "\n",
       "3. **Anomaly Detection**: LLMs are employed to identify unusual patterns or anomalies in time series data that could indicate market manipulation, fraud, or emerging trends, which is crucial for risk management and regulatory compliance in DeFi platforms.\n",
       "\n",
       "4. **Real-time Analysis**: The ability to process and analyze data in real-time has improved, enabling DeFi platforms to make quicker decisions based on the latest market information. This is important in the fast-paced DeFi sector, where market conditions can change rapidly.\n",
       "\n",
       "5. **Customization and Personalization**: LLMs have been adapted to create personalized investment strategies for users by analyzing their preferences, risk tolerance, and past behavior, enhancing the user experience and potentially leading to higher returns for individual investors.\n",
       "\n",
       "6. **Interoperability and Integration**: Progress has been made in making LLMs more interoperable with various blockchain platforms and DeFi applications, allowing for seamless integration and more comprehensive analysis across different data sources and platforms.\n",
       "\n",
       "These advancements demonstrate the potential of LLMs to transform time series forecasting in decentralized finance, making it more accurate, efficient, and personalized. As these technologies continue to evolve, they are likely to play an increasingly central role in the DeFi ecosystem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict2 = convert.to_dict(branch.action_response.content.iloc[3])\n",
    "\n",
    "func_ = nested.nget(res_dict2, ['action_response', 'function'])\n",
    "args_ = nested.nget(res_dict2, ['action_response', 'arguments'])\n",
    "output_ = nested.nget(res_dict2, ['action_response', 'output'])\n",
    "\n",
    "print(f\"Tool used is: {func_}\")\n",
    "print(f\"question asked: {args_}\")\n",
    "Markdown(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Save the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 messages saved to data/logs/main_messages_20240327190213.csv\n",
      "3 logs saved to data/logs/main_log_20240327190213.csv\n"
     ]
    }
   ],
   "source": [
    "researcher.to_csv_file()\n",
    "researcher.log_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi.libs import convert, func_call\n",
    "\n",
    "responses = convert.to_list([responses_arxiv, responses_d2l, responses_bc, responses_google, responses_wiki], flatten=True)\n",
    "responses_dicts = func_call.lcall(responses, lambda x: {\"response\": str(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = convert.to_df(responses_dicts)\n",
    "responses_df.to_csv(\"research_query_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters\n",
    "\n",
    "Ching Chang , Wei-Yao Wang , Wen-Chih Peng and Tien-Fu Chen\n",
    "\n",
    "National Yang Ming Chiao Tung University, Hsinchu, Taiwan\n",
    "\n",
    "blacksnail789521.cs10@nycu.edu.tw, sf1638.cs05@nctu.edu.tw, {wcpeng, tfchen}@cs.nycu.edu.tw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
