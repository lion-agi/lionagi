{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Savior 2\n",
    "\n",
    "Auto-explorative research with RAG critics cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi llama-index unstructured pypdf wikipedia googlesearch 'unstructured[pdf]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you need to download `lionhub` package and install locally to run the following codes\n",
    "\n",
    "- github link: https://github.com/lion-agi/lionhub\n",
    "\n",
    "you can use \n",
    "`pip install -e <dir-to-unzipped-folder>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you would like to ignore logging\n",
    "\n",
    "# import logging\n",
    "# logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Large Language Model Applications in Blockchain\"\n",
    "question = \"Research on building a system of trust integrating Large Language Model with blockchain\"\n",
    "num_papers = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionhub import LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download papers from arxiv and read them into nodes\n",
    "# if to_datanode is True, you get Lion DataNode, \n",
    "# which you can do:  DataNode.to_llama_index() & DataNode.from_llama_index()\n",
    "# similar usage for langchain loaders\n",
    "load_config = {\n",
    "    \"reader\": \"ArxivReader\", \n",
    "    \"reader_type\": \"llama_index\", \n",
    "    \"load_kwargs\": {\"search_query\": topic, \"max_results\": num_papers}, \n",
    "    \"to_datanode\": False        \n",
    "}                               \n",
    "\n",
    "arxiv_doc_ = li.load(**load_config)\n",
    "\n",
    "arxiv_docs = arxiv_doc_[:-num_papers]\n",
    "arxiv_summary = arxiv_doc_[-num_papers:]\n",
    "\n",
    "# chunk the papers into chunks\n",
    "chunk_config = {\n",
    "    \"chunker\": \"SentenceSplitter\", \n",
    "    \"chunker_type\": 'llama_index',\n",
    "    \"chunker_kwargs\": {'chunk_size': 512, 'chunk_overlap':20},\n",
    "    \"to_datanode\": False\n",
    "}\n",
    "\n",
    "arxiv_chunks = li.chunk(arxiv_docs, **chunk_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_index, arxiv_engine = LlamaIndex.vector_index(\n",
    "    arxiv_chunks, rerank_=True, get_engine=True\n",
    ")\n",
    "\n",
    "arxiv_index.storage_context.persist('.storage/arvix/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can build from storage like this, just need to find the index_id in the index_store file\n",
    "\n",
    "# arxiv_index, arxiv_engine = LlamaIndex.vector_index(\n",
    "#     from_storage=True, \n",
    "#     strorage_context_kwargs={\"persist_dir\": '.storage/arvix/'},\n",
    "#     index_id=\"41ca1905-433e-418c-a0a3-5d5f912d0ac3\",\n",
    "#     rerank_=True, get_engine=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Textbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "book1 = Path(\"d2l-en.pdf\")\n",
    "book2 = Path(\"Blockchain basic.pdf\")\n",
    "\n",
    "load_config1 = {\n",
    "    \"reader\": \"UnstructuredReader\", \n",
    "    \"reader_type\": \"llama_index\", \n",
    "    \"load_args\": [book1],\n",
    "    \"to_datanode\": False\n",
    "}\n",
    "\n",
    "load_config2 = load_config1.copy()\n",
    "load_config2.update({\"load_args\": [book2]})\n",
    "\n",
    "docs1 = li.load(**load_config1)\n",
    "docs2 = li.load(**load_config2)\n",
    "\n",
    "from llama_index import Document\n",
    "\n",
    "documents1 = [Document(text=\"\".join([x.text for x in docs1]))]\n",
    "documents2 = [Document(text=\"\".join([x.text for x in docs2]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a triplet extraction funciton using `Transformers`\n",
    "\n",
    "- [Transformers Doc](https://huggingface.co/docs/transformers/index)\n",
    "- [Make Meaningful KG from Open Source REBEL model](https://medium.com/@haiyangli_38602/make-meaningful-knowledge-graph-from-opensource-rebel-model-6f9729a55527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from lionhub import KGraph\n",
    "\n",
    "device = 'mps:0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model.to(device)\n",
    "\n",
    "def extract_triplets_wiki(text):\n",
    "    kb = KGraph.text_to_wiki_kb(\n",
    "        text, model=model, tokenizer=tokenizer, device=device\n",
    "    )\n",
    "    tubs = [(i['head'], i['type'], i['tail']) for i in kb.relations]\n",
    "    return tubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will take quite a while, I suggest you to minimize the notebook for a couple hours, or change the source to be shorter in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config1 = {\n",
    "    \"input_\": documents1,\n",
    "    \"service_context_kwargs\": {\"chunk_size\": 512},\n",
    "    \"kg_triplet_extract_fn\": extract_triplets_wiki, \n",
    "    \"index_kwargs\": {\"max_triplets_per_chunk\": 2, \"include_embeddings\": True},\n",
    "    \"rerank_\": True, \n",
    "    \"get_engine\": True,\n",
    "}\n",
    "\n",
    "d2l_index, d2l_engine = LlamaIndex.kg_index(**index_config1)\n",
    "d2l_index.storage_context.persist('.storage/d2l/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config2 = {\n",
    "    \"input_\": documents2,\n",
    "    \"service_context_kwargs\": {\"chunk_size\": 512},\n",
    "    \"kg_triplet_extract_fn\": extract_triplets_wiki, \n",
    "    \"index_kwargs\": {\"max_triplets_per_chunk\": 2, \"include_embeddings\": True},\n",
    "    \"rerank_\": True, \n",
    "    \"get_engine\": True,\n",
    "}\n",
    "\n",
    "bc_index, bc_engine = LlamaIndex.kg_index(**index_config2)\n",
    "bc_index.storage_context.persist(persist_dir='.storage/bc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_config11 = {\n",
    "#     \"from_storage\": True, \n",
    "#     \"strorage_context_kwargs\": {\"persist_dir\": '.storage/d2l/'}, \n",
    "#     \"index_id\": \"cac9b853-6b30-4fc3-9a18-8592901df717\",\n",
    "#     \"get_engine\": True,\n",
    "# }\n",
    "\n",
    "# index_config22 = {\n",
    "#     \"from_storage\": True, \n",
    "#     \"strorage_context_kwargs\": {\"persist_dir\": '.storage/bc/'}, \n",
    "#     \"index_id\": \"bcd41f43-7af0-4001-a3c0-c1a753cdeaac\",\n",
    "#     \"get_engine\": True,\n",
    "# }\n",
    "\n",
    "# d2l_index, d2l_engine = LlamaIndex.kg_index(**index_config11)\n",
    "# bc_index, bc_engine = LlamaIndex.kg_index(**index_config22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. google and wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionhub import Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_google = []\n",
    "responses_wiki = []\n",
    "\n",
    "# ask gpt to write you google format docstring\n",
    "def query_google(query: str):\n",
    "    \"\"\"\n",
    "    Search Google and retrieve a natural language answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: A natural language answer obtained from Google search results.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an issue with making the request or parsing the response.\n",
    "    \"\"\"\n",
    "    google_agent = Search.create_google_engine()\n",
    "    response = google_agent.chat(query)\n",
    "    responses_google.append(response)\n",
    "    return str(response)\n",
    "\n",
    "def query_wiki(query: str):\n",
    "    \"\"\"\n",
    "    Search Wikipedia and retrieve a natural language answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: A natural language answer obtained from Google search results.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an issue with making the request or parsing the response.\n",
    "    \"\"\"\n",
    "    response = Search.wiki_search(query)\n",
    "    responses_wiki.append(response)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. make into tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_arxiv = []\n",
    "responses_d2l = []\n",
    "responses_bc = []\n",
    "\n",
    "def query_arxiv(query: str):\n",
    "    \"\"\"\n",
    "    Query a vector index built with papers from arxiv. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = arxiv_engine.query(query)\n",
    "    responses_arxiv.append(response)\n",
    "    \n",
    "    return str(response.response)\n",
    "\n",
    "def query_d2l(query: str):\n",
    "    \"\"\"\n",
    "    Query a index built from machine learning textbooks. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = d2l_engine.query(query)\n",
    "    responses_d2l.append(response)\n",
    "    return str(response.response)\n",
    "        \n",
    "def query_bc(query: str):\n",
    "    \"\"\"\n",
    "    Query a index built from blockchain textbooks. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = bc_engine.query(query)\n",
    "    responses_bc.append(response)\n",
    "    return str(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [query_google, query_wiki, query_arxiv, query_d2l, query_bc]\n",
    "tools = li.lcall(funcs, li.func_to_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Design Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. PROMPTS - Researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1: Abstract Summary\n",
    "json_format1 = {\n",
    "    \"summary\": \"Brief summary of paper abstract.\",\n",
    "    \"core points\": \"Key points from the paper.\",\n",
    "    \"relevance to research question\": \n",
    "        \"Explanation of paper's relevance to a specific research question.\"\n",
    "}\n",
    "\n",
    "# Prompt 2: Reflections and Evaluation\n",
    "json_format2 = {\n",
    "    **json_format1,\n",
    "    \"reflections\": \"Reflections on the feedback received and improvements made.\"\n",
    "}\n",
    "\n",
    "# Prompt 3: Brainstorming for Research Question\n",
    "json_format3 = {\n",
    "    **json_format2,\n",
    "    \"Paper\": \"Name of the paper.\",\n",
    "    \"Authors\": \"List of authors.\",\n",
    "    \"key points for further investigation\": \"Points to explore further.\",\n",
    "    \"reasoning\": \"Reasoning behind the selection of these points.\"\n",
    "}\n",
    "\n",
    "# Prompt 4: Final Deliverable Presentation\n",
    "json_format4 = {\n",
    "    **json_format3,\n",
    "    \"next steps\": \"Proposed next steps based on the research.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Researcher System Configuration\n",
    "researcher_system = {\n",
    "    \"persona\": \"World-class researcher\",\n",
    "    \"requirements\": \"Clear, precise answers with a confident tone.\",\n",
    "    \"responsibilities\": \"Researching a specific topic and question.\",\n",
    "    \"notice\": \"Collaboration with critics for alignment with project goals.\"\n",
    "}\n",
    "\n",
    "# Instruction Set for Researcher\n",
    "\n",
    "researcher_instruct1 = {\n",
    "    \"task step\": \"1\",\n",
    "    \"task name\": \"Read Paper Abstracts\",\n",
    "    \"task objective\": \"Initial understanding of papers.\",\n",
    "    \"deliverable\": {\n",
    "        \"delivery required\": \"yes\", \n",
    "        \"format\": json_format1\n",
    "    }\n",
    "}\n",
    "\n",
    "researcher_instruct2 = {\n",
    "    \"task step\": \"2\",\n",
    "    \"task name\": \"Reflect on Feedback\",\n",
    "    \"task objective\": \"Improve understanding and relevance.\",\n",
    "    \"deliverable\": {\n",
    "        \"delivery required\": \"yes\", \n",
    "        \"format\": json_format2\n",
    "    }\n",
    "}\n",
    "\n",
    "researcher_instruct3 = {\n",
    "    \"task step\": \"3\",\n",
    "    \"task name\": \"Brainstorm for Research Question\",\n",
    "    \"task objective\": \"Ideas for research question assistance.\",\n",
    "    \"deliverable\": {\n",
    "        \"delivery required\": \"yes\", \n",
    "        \"format\": json_format3\n",
    "    }\n",
    "}\n",
    "\n",
    "researcher_instruct4 = {\n",
    "    \"task step\": \"4\",\n",
    "    \"task name\": \"Final Deliverable Presentation\",\n",
    "    \"task objective\": \"Present final research output.\",\n",
    "    \"deliverable\": {\n",
    "        \"delivery required\": \"yes\", \n",
    "        \"format\": json_format4\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. PROMPTS - critic cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic_system(name_):\n",
    "    tool_manual = {\n",
    "        \"notice\": f\"\"\"\n",
    "            Use the QA bot for each task. It queries an index of {name_} \n",
    "            and allows up to 3 queries per task.\n",
    "        \"\"\"\n",
    "    }\n",
    "    system = {\n",
    "        \"name\": f\"Critic_{name_}\",\n",
    "        \"resource\": f\"Resource: {name_}\",\n",
    "        \"persona\": \"World-class researcher\",\n",
    "        \"responsibilities\": \"\"\"\n",
    "            Check the quality of plans and code. Give feedback.\n",
    "        \"\"\",\n",
    "        \"requirements\": f\"\"\"\n",
    "            As Critic_{name_}, verify the accuracy of plans and claims.\n",
    "        \"\"\",\n",
    "        \"tools\": tool_manual\n",
    "    }\n",
    "    return system\n",
    "\n",
    "def get_critic_stage1(name_):\n",
    "    return {\n",
    "        \"task step\": \"1\",\n",
    "        \"task name\": \"Verify Claim\",\n",
    "        \"description\": f\"\"\"\n",
    "            Research on a specific topic using the {name_} index. Verify claims and report findings.\n",
    "        \"\"\",\n",
    "        \"deliverable\": \"\"\"\n",
    "            Provide a detailed report with your findings. Include references and your name.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "def get_critic_stage2(step_num):\n",
    "    return {\n",
    "        \"task step\": str(step_num),\n",
    "        \"task name\": \"Peer-Review for Improvement\",\n",
    "        \"description\": \"\"\"\n",
    "            Review and argue against findings from steps 2, 3, and 4. Use facts and sources.\n",
    "        \"\"\",\n",
    "        \"deliverable\": \"\"\"\n",
    "            Provide detailed reviews for each critic. Include references and your name.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "def get_critic_stage3(step_num):\n",
    "    return {\n",
    "        \"task step\": str(step_num),\n",
    "        \"task name\": \"Finalize and Review\",\n",
    "        \"description\": \"\"\"\n",
    "            Write your final peer review. Consider all feedback and research relevance.\n",
    "        \"\"\",\n",
    "        \"deliverable\": \"\"\"\n",
    "            Present a comprehensive report of 1200+ words. Include your findings and name.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "def get_critic_config(name_, tools):\n",
    "    return {\n",
    "        \"name\": f\"critic_{name_}\",\n",
    "        \"system\": get_critic_system(name_),\n",
    "        'step1': get_critic_stage1(name_),\n",
    "        'step2': get_critic_stage2(2),\n",
    "        'step3': get_critic_stage2(3),\n",
    "        'step4': get_critic_stage2(4),\n",
    "        'step5': get_critic_stage3(5), \n",
    "        \"tools\": tools\n",
    "    }\n",
    "\n",
    "# [query_google, query_wiki, query_arxiv, query_d2l, query_bc]\n",
    "critic_configs = {\n",
    "    \"critic_google\": get_critic_config(\"google\", tools[0]),\n",
    "    \"critic_wiki\": get_critic_config(\"wiki\", tools[1]),\n",
    "    \"critic_arxiv\": get_critic_config(\"arxiv\", tools[2]),\n",
    "    \"critic_d2l\": get_critic_config(\"d2l\", tools[3]),\n",
    "    \"critic_bc\": get_critic_config(\"bc\", tools[4])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. write workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = ['google', 'wiki', 'arxiv', 'd2l', 'bc']\n",
    "critic_names = [f\"critic_{t_}\" for t_ in tool_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a researcher session\n",
    "researcher = li.Session(system=researcher_system)\n",
    "\n",
    "# create a new branch for each critic\n",
    "for idx, name_ in enumerate(critic_names):\n",
    "    researcher.new_branch(\n",
    "        branch_name=li.nget(critic_configs, [name_, \"name\"]),\n",
    "        tools=li.nget(critic_configs, [name_, \"tools\"]), \n",
    "        system=li.nget(critic_configs, [name_, \"system\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "async def critic_workflow(context):\n",
    "    researcher.new_branch('critic_chat')\n",
    "\n",
    "    async def _inner(step_num=None, _context=None):        \n",
    "        f = lambda branch_, step_: [f\"{branch_}\", f\"step{step_}\"]\n",
    "        \n",
    "        async def _func(name_):\n",
    "            config_ = {\n",
    "                'instruction': li.nget(critic_configs, f(name_, step_num)), \n",
    "                'num': 5, 'to_': name_, 'temperature': 0.3, 'tools': True,\n",
    "            }\n",
    "            if _context is not None:\n",
    "                config_.update({'context': _context})\n",
    "            try:\n",
    "                out = await researcher.auto_followup(**config_)\n",
    "                return out\n",
    "            except:\n",
    "                return 'somehow failed'\n",
    "\n",
    "        out_ = await li.alcall(critic_names, _func)\n",
    "        return li.to_list(out_, flatten=True)\n",
    "\n",
    "    def update_group_chat():\n",
    "        df_ = lambda name_: researcher.branches[name_].filter_messages_by(sender='assistant').copy()\n",
    "        \n",
    "        for name_ in critic_names:\n",
    "            df1 = df_(name_)\n",
    "            if len(df1) > 0:\n",
    "                researcher.branches['critic_chat'].extend(df1)\n",
    "        \n",
    "        lst = li.to_list([li.to_list(\n",
    "                [df_(name_) if df_(name_) is not None else None], \n",
    "                flatten=True, dropna=True\n",
    "            )\n",
    "            for name_ in critic_names], flatten=True, dropna=True)\n",
    "        \n",
    "        dfs = pd.concat(lst)\n",
    "        \n",
    "        researcher.branches['critic_chat'].extend(dfs)\n",
    "        for name_ in critic_names:\n",
    "            researcher.branches[name_].extend(dfs)\n",
    "            \n",
    "    await _inner(1, _context=context)\n",
    "    update_group_chat()\n",
    "    \n",
    "    for i in range(1,5):\n",
    "        await _inner(i+1)\n",
    "        update_group_chat()\n",
    "    \n",
    "    msgs = researcher.branches['critic_chat'].messages.copy()\n",
    "    researcher.delete_branch('critic_chat')\n",
    "    return msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depending on your prompts, this might take a long while as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Instruction 1: read abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_context1 = str(arxiv_summary[1])\n",
    "\n",
    "r1 = await researcher.chat(\n",
    "    instruction=researcher_instruct1, \n",
    "    context=r_context1, \n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch critic_chat is deleted.\n"
     ]
    }
   ],
   "source": [
    "c_context1 = {\n",
    "    \"paper_summary\": r_context1, \n",
    "    \"researcher_work\": str(r1)\n",
    "}\n",
    "\n",
    "reports1 = await critic_workflow(c_context1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Instruction 2: reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch critic_chat is deleted.\n"
     ]
    }
   ],
   "source": [
    "r_context2 = str(\n",
    "    [i.content for _, i in reports1.iloc[-10:].iterrows()]\n",
    ")\n",
    "\n",
    "r2_1 = await researcher.chat(\n",
    "    instruction=researcher_instruct2, \n",
    "    context=r_context2,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "r2_2 = await researcher.chat(\n",
    "    instruction = \"integrate critics comments, reflect on your work and try again. \"\n",
    ")\n",
    "\n",
    "c_context2 = {\n",
    "    \"researcher_work\": str(r2_2)\n",
    "}\n",
    "\n",
    "reports2 = await critic_workflow(c_context2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Instruction 3: brainstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch critic_chat is deleted.\n"
     ]
    }
   ],
   "source": [
    "r_context3 = str(\n",
    "    [i.content for _, i in reports1.iloc[-10:].iterrows()]\n",
    ")\n",
    "\n",
    "r3 = await researcher.chat(\n",
    "    instruction=researcher_instruct3, \n",
    "    context=r_context3\n",
    ")\n",
    "\n",
    "r3_2 = await researcher.chat(\n",
    "    instruction = \"integrate critics comments, reflect on your work and try again. \"\n",
    ")\n",
    "\n",
    "c_context3 = {\n",
    "    \"researcher_work\": str(r3_2)\n",
    "}\n",
    "\n",
    "reports3 = await critic_workflow(c_context3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Instruction 4: output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_context4 = str(\n",
    "    [i.content for _, i in reports1.iloc[-10:].iterrows()]\n",
    ")\n",
    "\n",
    "r4 = await researcher.chat(\n",
    "    instruction=researcher_instruct4, \n",
    "    context=r_context4\n",
    ")\n",
    "\n",
    "r4_2 = await researcher.chat(\n",
    "    instruction = \"integrate critics comments, reflect on your work and present final output to user, be as long as possible\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_messages = [\n",
    "    i.messages.copy().dropna(how='all') for _, i in researcher.branches.items() \n",
    "    if len(i.messages) > 1\n",
    "]\n",
    "ttl_df = pd.concat(ttl_messages).drop_duplicates(subset=['node_id', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_df.to_csv('all_critics_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 396 entries, 0 to 106\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   node_id    396 non-null    object        \n",
      " 1   role       396 non-null    object        \n",
      " 2   sender     396 non-null    object        \n",
      " 3   timestamp  396 non-null    datetime64[ns]\n",
      " 4   content    396 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 18.6+ KB\n"
     ]
    }
   ],
   "source": [
    "ttl_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of reponses from all assistants\n",
    "\n",
    "len(ttl_df[ttl_df.sender == 'assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of queries answered\n",
    "\n",
    "len(ttl_df[ttl_df.sender == 'action_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check each tool uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of arxiv queries: 25\n",
      "total number of d2l queries: 31\n",
      "total number of bc queries: 9\n",
      "total number of google queries: 18\n",
      "total number of wiki queries: 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of arxiv queries: {len(responses_arxiv)}\")\n",
    "print(f\"total number of d2l queries: {len(responses_d2l)}\")\n",
    "print(f\"total number of bc queries: {len(responses_bc)}\")\n",
    "print(f\"total number of google queries: {len(responses_google)}\")\n",
    "print(f\"total number of wiki queries: {len(responses_wiki)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Improve outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5 = await researcher.chat('please be more specific in terms of technical details, use your conversation context to supplement. for example, introduce which technologies, which references should be used where and include some reasoning. you can do it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I appreciate your encouragement and the opportunity to delve into a more technically detailed research proposal. Utilizing the context of our previous conversations, letâ€™s articulate a comprehensive plan that integrates Large Language Models (LLMs) with blockchain technology to foster a system of trust.\n",
      "\n",
      "**Title:**\n",
      "\"Design and Implementation of a Trust-Enhancing Framework via LLM-Blockchain Integration\"\n",
      "\n",
      "**Abstract:**\n",
      "This research investigates the technical feasibility and implementation strategies for integrating LLMs with blockchain networks to construct a robust system of trust. By harnessing the linguistic and cognitive capabilities of LLMs and the immutable record-keeping features of blockchain, we aim to develop a platform that ensures data integrity, facilitates transparent transactions, and mitigates the risks of malicious activities across blockchain systems.\n",
      "\n",
      "**Introduction:**\n",
      "Blockchain technology has laid the foundation for creating secure and decentralized digital ledgers. However, as the technology proliferates, the need for more sophisticated trust mechanisms becomes evident. The integration of LLMs, with their advanced natural language processing and understanding capabilities, into blockchain ecosystems holds the promise of revolutionizing trust in these networks. This integration will allow for real-time analysis and validation of on-chain data, smart contracts, and transactions, thus enhancing the overall security and reliability of blockchain platforms.\n",
      "\n",
      "**Research Objectives:**\n",
      "1. Develop a standardized cross-chain query language enabling LLMs to access and interpret data from disparate blockchain networks.\n",
      "2. Engineer a framework that allows LLMs to autonomously perform trust operations, such as transaction verification, fraud detection, and smart contract compliance, across multiple blockchain infrastructures.\n",
      "3. Evaluate the added value of LLMs in the context of blockchain by measuring improvements in data integrity, fraud prevention, and operational compliance.\n",
      "\n",
      "**Literature Review:**\n",
      "An in-depth literature review will form the backbone of the research, focusing on areas such as blockchain interoperability protocols, consensus algorithms, data privacy and security in distributed networks, and the application of AI in trust and verification processes. Key references will be drawn from sources like the Bitcoin and Ethereum whitepapers, IBM's Hyperledger Fabric documentation, Google's BERT language model research, and case studies of cross-chain solutions like Cosmos and Polkadot.\n",
      "\n",
      "**Methodology:**\n",
      "The methodology for this research will encompass a multi-phased approach:\n",
      "\n",
      "1. **Requirement Analysis:**\n",
      "   - Cataloging interoperability challenges and trust issues identified within existing blockchain systems.\n",
      "   - Defining the technical requirements for the LLMs' operation within blockchain systems, including the necessary computational resources and access permissions.\n",
      "\n",
      "2. **Design of Cross-Chain Query Language:**\n",
      "   - Leveraging existing query languages like SQL and blockchain-specific languages such as Solidity to inform the development of a new language schema that encompasses the needs of multiple blockchain systems.\n",
      "   - Ensuring compatibility with different blockchain data models, including UTXO (used by Bitcoin) and account/balance (used by Ethereum).\n",
      "\n",
      "3. **LLM Adaptation and Training:**\n",
      "   - Customizing existing LLMs (e.g., GPT-3, BERT) to understand and generate blockchain-relevant text.\n",
      "   - Training these models on datasets constructed from blockchain transaction logs, smart contract code, and other relevant on-chain data.\n",
      "\n",
      "4. **System Architecture Development:**\n",
      "   - Designing a microservices-based architecture that allows LLMs to interface with blockchain nodes through APIs and webhooks.\n",
      "   - Incorporating event-driven triggers to enable LLMs to respond to real-time blockchain events, such as the creation of a new block or the submission of a smart contract.\n",
      "\n",
      "5. **Prototype Development and Iteration:**\n",
      "   - Developing a prototype system that integrates the cross-chain query language and LLMs in a controlled test environment.\n",
      "   - Applying agile development methodologies to iterate on the system based on real-world data and user feedback.\n",
      "\n",
      "6. **Security and Scalability Analysis:**\n",
      "   - Conducting thorough penetration testing and security analysis to identify and remediate potential vulnerabilities.\n",
      "   - Assessing the scalability of the system, with a focus on the LLMs' response times and accuracy as the volume of blockchain activity scales up.\n",
      "\n",
      "7. **Testing and Evaluation:**\n",
      "   - Deploying the prototype in a sandbox environment that simulates a real-world blockchain ecosystem.\n",
      "   - Evaluating the system's performance through quantitative metrics (e.g., transaction verification time, smart contract analysis accuracy) and qualitative user feedback.\n",
      "\n",
      "8. **Documentation and Dissemination:**\n",
      "   - Documenting the research findings, the developed cross-chain query language specifications, LLM training methods, and the overall system architecture.\n",
      "   - Publishing the results in academic journals, presenting at conferences, and releasing open-source code for community feedback and collaboration.\n",
      "\n",
      "**Expected Outcomes:**\n",
      "A comprehensive framework for LLM-blockchain integration, including a new query language, trained LLMs, and an extensible system architecture, will be the tangible outcomes of this research. The framework will serve as a blueprint for building trust-enhancing applications on blockchain networks.\n",
      "\n",
      "**Potential Challenges:**\n",
      "- Developing a query language that is secure, efficient, and compatible with a variety of blockchain protocols.\n",
      "- Training LLMs to accurately interpret the context and nuances of blockchain data while ensuring user privacy.\n",
      "- Ensuring the computational efficiency of LLMs within the resource-constrained blockchain environments.\n",
      "\n",
      "**Timeline:**\n",
      "The research will follow a detailed 24-month timeline, starting with a six-month period focused on requirements gathering and literature review, followed by phases of design, development, and iterative testing, each lasting approximately four to six months.\n",
      "\n",
      "**Budget:**\n",
      "A comprehensive budget will include allocations for personnel, computational resources (e.g., cloud services for LLM training and deployment), data acquisition costs, software development tools, security audits, and dissemination efforts.\n",
      "\n",
      "**Conclusion:**\n",
      "This research proposal presents a clear and actionable plan for addressing the challenge of building trust in blockchain systems through the integration of LLMs. Through meticulous planning, thorough technical exploration, and a commitment to open collaboration, this project aims to push the frontiers of what is possible at the intersection of AI and blockchain technology.\n",
      "\n",
      "---\n",
      "\n",
      "This highly detailed and technical research proposal incorporates elements from the previous discussion, including the importance of a cross-chain query language and the role of LLMs in trust operations on blockchain networks. It is designed to be a practical, feasible blueprint for groundbreaking research in the field.\n"
     ]
    }
   ],
   "source": [
    "print(r5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
