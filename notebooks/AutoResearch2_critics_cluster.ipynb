{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Savior 2\n",
    "\n",
    "Auto-explorative research with RAG critics cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lionagi llama-index unstructured pypdf wikipedia google-search 'unstructured[pdf]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you would like to ignore logging\n",
    "\n",
    "# import logging\n",
    "# logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Large Language Model Applications in Blockchain\"\n",
    "question = \"Research on building a system of trust integrating Large Language Model with blockchain\"\n",
    "num_papers = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download paper and build index\n",
    "from llama_index import download_loader\n",
    "\n",
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "loader = ArxivReader()\n",
    "\n",
    "documents, abstracts = loader.load_papers_and_abstracts(search_query=topic, max_results=num_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import  VectorStoreIndex, ServiceContext\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "llm = OpenAI(model='gpt-4-turbo-preview', temperature=0.1)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    text_splitter=text_splitter, llm=llm)\n",
    "\n",
    "arxiv_index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context)\n",
    "\n",
    "arxiv_engine = arxiv_index.as_query_engine(\n",
    "    similarity_top_k=3, response_mode= \"tree_summarize\")\n",
    "\n",
    "arxiv_index.storage_context.persist('.storage/arxiv/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # you can build from storage like this, just need to find the index_id in the index_store file\n",
    "# from llama_index import StorageContext, load_index_from_storage\n",
    "# from llama_index.llms import OpenAI\n",
    "\n",
    "# llm = OpenAI(model='gpt-4-turbo-preview', temperature=0.1)\n",
    "# service_context = service_context.from_defaults(llm=llm)\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(persist_dir='.storage/arvix1/')\n",
    "# index_id = '789e33ff-f14c-4d36-a7eb-827896b2bd3b'\n",
    "\n",
    "# arxiv_index = load_index_from_storage(\n",
    "#     storage_context=storage_context, index_id=index_id, service_context=service_context)\n",
    "\n",
    "# arxiv_engine = arxiv_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Textbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1 = \"d2l-en.pdf\"\n",
    "book2 = \"Blockchain basic.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lion/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lion/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader, Document\n",
    "UnstructuredReader = download_loader(\"UnstructuredReader\")\n",
    "loader = UnstructuredReader()\n",
    "\n",
    "documents1 = loader.load_data(book1)\n",
    "documents2 = loader.load_data(book2)\n",
    "\n",
    "documents1 = [Document(text=\"\".join([x.text for x in documents1]))]\n",
    "documents2 = [Document(text=\"\".join([x.text for x in documents2]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customized Triplets extration function using transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a triplet extraction funciton using `Transformers`\n",
    "\n",
    "- [Transformers Doc](https://huggingface.co/docs/transformers/index)\n",
    "- [Build KG with Wikipedia filtering](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/knowledge_graph2.html)\n",
    "- [Make Meaningful KG from Open Source REBEL model](https://medium.com/@haiyangli_38602/make-meaningful-knowledge-graph-from-opensource-rebel-model-6f9729a55527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"Babelscape/rebel-large\",\n",
    "    tokenizer=\"Babelscape/rebel-large\",\n",
    "    # comment this line to run on CPU\n",
    "    device=\"mps:0\",\n",
    ")\n",
    "\n",
    "def extract_triplets(input_text):\n",
    "    text = triplet_extractor.tokenizer.batch_decode(\n",
    "        [\n",
    "            triplet_extractor(\n",
    "                input_text, return_tensors=True, return_text=False\n",
    "            )[0][\"generated_token_ids\"]\n",
    "        ]\n",
    "    )[0]\n",
    "\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = \"\", \"\", \"\", \"\"\n",
    "    text = text.strip()\n",
    "    current = \"x\"\n",
    "    for token in (\n",
    "        text.replace(\"<s>\", \"\")\n",
    "        .replace(\"<pad>\", \"\")\n",
    "        .replace(\"</s>\", \"\")\n",
    "        .split()\n",
    "    ):\n",
    "        if token == \"<triplet>\":\n",
    "            current = \"t\"\n",
    "            if relation != \"\":\n",
    "                triplets.append(\n",
    "                    (subject.strip(), relation.strip(), object_.strip())\n",
    "                )\n",
    "                relation = \"\"\n",
    "            subject = \"\"\n",
    "        elif token == \"<subj>\":\n",
    "            current = \"s\"\n",
    "            if relation != \"\":\n",
    "                triplets.append(\n",
    "                    (subject.strip(), relation.strip(), object_.strip())\n",
    "                )\n",
    "            object_ = \"\"\n",
    "        elif token == \"<obj>\":\n",
    "            current = \"o\"\n",
    "            relation = \"\"\n",
    "        else:\n",
    "            if current == \"t\":\n",
    "                subject += \" \" + token\n",
    "            elif current == \"s\":\n",
    "                object_ += \" \" + token\n",
    "            elif current == \"o\":\n",
    "                relation += \" \" + token\n",
    "\n",
    "    if subject != \"\" and relation != \"\" and object_ != \"\":\n",
    "        triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "\n",
    "    return triplets\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "class WikiFilter:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "\n",
    "    def filter(self, candidate_entity):\n",
    "        # check the cache to avoid network calls\n",
    "        if candidate_entity in self.cache:\n",
    "            return self.cache[candidate_entity][\"title\"]\n",
    "\n",
    "        # pull the page from wikipedia -- if it exists\n",
    "        try:\n",
    "            page = wikipedia.page(candidate_entity, auto_suggest=False)\n",
    "            entity_data = {\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"summary\": page.summary,\n",
    "            }\n",
    "\n",
    "            # cache the page title and original entity\n",
    "            self.cache[candidate_entity] = entity_data\n",
    "            self.cache[page.title] = entity_data\n",
    "\n",
    "            return entity_data[\"title\"]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "wiki_filter = WikiFilter()\n",
    "\n",
    "def extract_triplets_wiki(text):\n",
    "    relations = extract_triplets(text)\n",
    "\n",
    "    filtered_relations = []\n",
    "    for relation in relations:\n",
    "        (subj, rel, obj) = relation\n",
    "        filtered_subj = wiki_filter.filter(subj)\n",
    "        filtered_obj = wiki_filter.filter(obj)\n",
    "\n",
    "        # skip if at least one entity not linked to wiki\n",
    "        if filtered_subj is None and filtered_obj is None:\n",
    "            continue\n",
    "\n",
    "        filtered_relations.append(\n",
    "            (\n",
    "                filtered_subj or subj,\n",
    "                rel,\n",
    "                filtered_obj or obj,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return filtered_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will take quite a while, I suggest you to minimize the notebook for a couple hours, or change the source to be shorter in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, KnowledgeGraphIndex\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.1, model=\"gpt-4-turbo-preview\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=256)\n",
    "\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents1,\n",
    "    max_triplets_per_chunk=3,\n",
    "    kg_triplet_extract_fn=extract_triplets_wiki,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "d2l_engine = arxiv_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")\n",
    "d2l_index.storage_context.persist('.storage/d2l/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lion/Documents/GitHub/ideal-cat/.conda/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/lion/Documents/GitHub/ideal-cat/.conda/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "bc_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents2,\n",
    "    max_triplets_per_chunk=3,\n",
    "    kg_triplet_extract_fn=extract_triplets_wiki,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "d2l_engine = arxiv_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")\n",
    "bc_index.storage_context.persist('.storage/bc/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import load_index_from_storage\n",
    "# from llama_index.storage.storage_context import StorageContext\n",
    "# from llama_index.graph_stores import SimpleGraphStore\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     graph_store=SimpleGraphStore(), persist_dir= '.storage/d2l/')\n",
    "\n",
    "# # <change to your own index id, can find it in index store>\n",
    "# index_id = 'cac9b853-6b30-4fc3-9a18-8592901df717' \n",
    "\n",
    "# d2l_index = load_index_from_storage(storage_context=storage_context, index_id=index_id)\n",
    "# d2l_engine = d2l_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    graph_store=SimpleGraphStore(), persist_dir= '.storage/bc/')\n",
    "\n",
    "# <change to your own index id, can find it in index store>\n",
    "index_id = 'bcd41f43-7af0-4001-a3c0-c1a753cdeaac' \n",
    "\n",
    "bc_index = load_index_from_storage(storage_context=storage_context, index_id=index_id,)\n",
    "bc_engine = bc_index.as_query_engine(similarity_top_k=3, response_mode= \"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. google and wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "google_key_scheme = 'GOOGLE_API_KEY'\n",
    "google_engine_scheme = 'GOOGLE_CSE_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_google_engine(\n",
    "    google_api_key=os.getenv(google_key_scheme), \n",
    "    google_engine=os.getenv(google_engine_scheme), \n",
    "    verbose=False\n",
    "):\n",
    "    try:\n",
    "        from llama_index.agent import OpenAIAgent\n",
    "        from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "        from llama_hub.tools.google_search.base import GoogleSearchToolSpec\n",
    "\n",
    "        api_key = google_api_key\n",
    "        search_engine = google_engine\n",
    "        google_spec = GoogleSearchToolSpec(key=api_key, engine=search_engine)\n",
    "\n",
    "        # Wrap the google search tool as it returns large payloads\n",
    "        tools = LoadAndSearchToolSpec.from_defaults(\n",
    "            google_spec.to_tool_list()[0],\n",
    "        ).to_tool_list()\n",
    "\n",
    "        # Create the Agent with our tools\n",
    "        agent = OpenAIAgent.from_tools(tools, verbose=verbose)\n",
    "        return agent\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise ImportError(f\"Error in importing OpenAIAgent from llama_index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_query(query: str, lang: str = 'en'):\n",
    "    import wikipedia\n",
    "    from llama_index import Document, VectorStoreIndex\n",
    "\n",
    "    wikipedia.set_lang(lang)\n",
    "\n",
    "    res = wikipedia.search(query, results=1)\n",
    "    if len(res) == 0:\n",
    "        return \"No search results.\"\n",
    "    try:\n",
    "        wikipedia_page = wikipedia.page(res[0], auto_suggest=False)\n",
    "    except wikipedia.PageError:\n",
    "        return f\"Unable to load page {res[0]}.\"\n",
    "    content = wikipedia_page.content\n",
    "\n",
    "    documents = [Document(text=content)]\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(query)\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_arxiv = []\n",
    "responses_d2l = []\n",
    "responses_bc = []\n",
    "\n",
    "def query_arxiv(query: str):\n",
    "    \"\"\"\n",
    "    Query a vector index built with papers from arxiv. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = arxiv_engine.query(query)\n",
    "    responses_arxiv.append(response)\n",
    "    \n",
    "    return str(response.response)\n",
    "\n",
    "def query_d2l(query: str):\n",
    "    \"\"\"\n",
    "    Query a index built from machine learning textbooks. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = d2l_engine.query(query)\n",
    "    responses_d2l.append(response)\n",
    "    return str(response.response)\n",
    "        \n",
    "def query_bc(query: str):\n",
    "    \"\"\"\n",
    "    Query a index built from blockchain textbooks. It takes \n",
    "    natural language query, and give natural language response. \n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to get an answer from the index\n",
    "\n",
    "    Returns:\n",
    "        str: The query response from index\n",
    "    \"\"\"\n",
    "    response = bc_engine.query(query)\n",
    "    responses_bc.append(response)\n",
    "    return str(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_google = []\n",
    "responses_wiki = []\n",
    "\n",
    "# ask gpt to write you google format docstring\n",
    "def query_google(query: str):\n",
    "    \"\"\"\n",
    "    Search Google and retrieve a natural language answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: A natural language answer obtained from Google search results.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an issue with making the request or parsing the response.\n",
    "    \"\"\"\n",
    "    google_agent = create_google_engine()\n",
    "    response = google_agent.chat(query)\n",
    "    responses_google.append(response)\n",
    "    return str(response)\n",
    "\n",
    "def query_wiki(query: str):\n",
    "    \"\"\"\n",
    "    Search Wikipedia and retrieve a natural language answer to a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to find an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: A natural language answer obtained from Google search results.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an issue with making the request or parsing the response.\n",
    "    \"\"\"\n",
    "    response = wiki_query(query)\n",
    "    responses_wiki.append(response)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make them into LionAGI tool objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li\n",
    "\n",
    "funcs = [query_google, query_wiki, query_arxiv, query_d2l, query_bc]\n",
    "tools = li.lcall(funcs, li.func_to_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. PROMPTS - Researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1: Abstract Summary\n",
    "json_format1 = {\n",
    "    \"summary\": \"Brief summary of paper abstract.\",\n",
    "    \"core points\": \"Key points from the paper.\",\n",
    "    \"relevance to research question\": \n",
    "        \"Explanation of paper's relevance to a specific research question.\"\n",
    "}\n",
    "\n",
    "# Prompt 2: Reflections and Evaluation\n",
    "json_format2 = {\n",
    "    **json_format1,\n",
    "    \"reflections\": \"Reflections on the feedback received and improvements made.\"\n",
    "}\n",
    "\n",
    "# Prompt 3: Brainstorming for Research Question\n",
    "json_format3 = {\n",
    "    **json_format2,\n",
    "    \"Paper\": \"Name of the paper.\",\n",
    "    \"Authors\": \"List of authors.\",\n",
    "    \"key points for further investigation\": \"Points to explore further.\",\n",
    "    \"reasoning\": \"Reasoning behind the selection of these points.\"\n",
    "}\n",
    "\n",
    "# Prompt 4: Final Deliverable Presentation\n",
    "json_format4 = {\n",
    "    **json_format3,\n",
    "    \"next steps\": \"Proposed next steps based on the research.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Researcher System Configuration\n",
    "researcher_system = {\n",
    "    \"persona\": \"World-class researcher\",\n",
    "    \"requirements\": \"Clear, precise answers with a confident tone.\",\n",
    "    \"responsibilities\": \"Researching a specific topic and question.\",\n",
    "    \"notice\": \"Collaboration with critics for alignment with project goals.\"\n",
    "}\n",
    "\n",
    "# Instruction Set for Researcher\n",
    "\n",
    "researcher_instruct1 = {\n",
    "    \"task step\": \"1\",\n",
    "    \"task name\": \"Read Paper Abstracts\",\n",
    "    \"task objective\": \"Initial understanding of papers.\",\n",
    "    \"deliverable\": {\n",
    "        \"delivery required\": \"yes\", \n",
    "        \"format\": json_format1\n",
    "    }\n",
    "}\n",
    "\n",
    "researcher_instruct2 = {\n",
    "    \"task step\": \"2\",\n",
    "    \"task name\": \"Reflect on Feedback\",\n",
    "    \"task objective\": \"Improve understanding and relevance.\",\n",
    "    \"deliverable\": {\n",
    "        \"delivery required\": \"yes\", \n",
    "        \"format\": json_format2\n",
    "    }\n",
    "}\n",
    "\n",
    "researcher_instruct3 = {\n",
    "    \"task step\": \"3\",\n",
    "    \"task name\": \"Brainstorm for Research Question\",\n",
    "    \"task objective\": \"Ideas for research question assistance.\",\n",
    "    \"deliverable\": {\n",
    "        \"delivery required\": \"yes\", \n",
    "        \"format\": json_format3\n",
    "    }\n",
    "}\n",
    "\n",
    "researcher_instruct4 = {\n",
    "    \"task step\": \"4\",\n",
    "    \"task name\": \"Final Deliverable Presentation\",\n",
    "    \"task objective\": \"Present final research output.\",\n",
    "    \"deliverable\": {\n",
    "        \"delivery required\": \"yes\", \n",
    "        \"format\": json_format4\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. PROMPTS - critic cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic_system(name_):\n",
    "    tool_manual = {\n",
    "        \"notice\": f\"\"\"\n",
    "            Use the QA bot for each task. It queries an index of {name_} \n",
    "            and allows up to 3 queries per task.\n",
    "        \"\"\"\n",
    "    }\n",
    "    system = {\n",
    "        \"name\": f\"Critic_{name_}\",\n",
    "        \"resource\": f\"Resource: {name_}\",\n",
    "        \"persona\": \"World-class researcher\",\n",
    "        \"responsibilities\": \"\"\"\n",
    "            Check the quality of plans and code. Give feedback.\n",
    "        \"\"\",\n",
    "        \"requirements\": f\"\"\"\n",
    "            As Critic_{name_}, verify the accuracy of plans and claims.\n",
    "        \"\"\",\n",
    "        \"tools\": tool_manual\n",
    "    }\n",
    "    return system\n",
    "\n",
    "def get_critic_stage1(name_):\n",
    "    return {\n",
    "        \"task step\": \"1\",\n",
    "        \"task name\": \"Verify Claim\",\n",
    "        \"description\": f\"\"\"\n",
    "            Research on a specific topic using the {name_} index. Verify claims and report findings.\n",
    "        \"\"\",\n",
    "        \"deliverable\": \"\"\"\n",
    "            Provide a detailed report with your findings. Include references and your name.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "def get_critic_stage2(step_num):\n",
    "    return {\n",
    "        \"task step\": str(step_num),\n",
    "        \"task name\": \"Peer-Review for Improvement\",\n",
    "        \"description\": \"\"\"\n",
    "            Review and argue against findings from steps 2, 3, and 4. Use facts and sources.\n",
    "        \"\"\",\n",
    "        \"deliverable\": \"\"\"\n",
    "            Provide detailed reviews for each critic. Include references and your name.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "def get_critic_stage3(step_num):\n",
    "    return {\n",
    "        \"task step\": str(step_num),\n",
    "        \"task name\": \"Finalize and Review\",\n",
    "        \"description\": \"\"\"\n",
    "            Write your final peer review. Consider all feedback and research relevance.\n",
    "        \"\"\",\n",
    "        \"deliverable\": \"\"\"\n",
    "            Present a comprehensive report of 1200+ words. Include your findings and name.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "def get_critic_config(name_, tools):\n",
    "    return {\n",
    "        \"name\": f\"critic_{name_}\",\n",
    "        \"system\": get_critic_system(name_),\n",
    "        'step1': get_critic_stage1(name_),\n",
    "        'step2': get_critic_stage2(2),\n",
    "        'step3': get_critic_stage2(3),\n",
    "        'step4': get_critic_stage2(4),\n",
    "        'step5': get_critic_stage3(5), \n",
    "        \"tools\": tools\n",
    "    }\n",
    "\n",
    "# [query_google, query_wiki, query_arxiv, query_d2l, query_bc]\n",
    "critic_configs = {\n",
    "    \"critic_google\": get_critic_config(\"google\", tools[0]),\n",
    "    \"critic_wiki\": get_critic_config(\"wiki\", tools[1]),\n",
    "    \"critic_arxiv\": get_critic_config(\"arxiv\", tools[2]),\n",
    "    \"critic_d2l\": get_critic_config(\"d2l\", tools[3]),\n",
    "    \"critic_bc\": get_critic_config(\"bc\", tools[4])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Drafting Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up session and branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each critic will be in a branch of the session\n",
    "# and they each have access to one corresponding query engine\n",
    "\n",
    "tool_names = ['google', 'wiki', 'arxiv', 'd2l', 'bc']\n",
    "critic_names = [f\"critic_{t_}\" for t_ in tool_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a researcher session\n",
    "# researcher will be in the default branch called 'main'\n",
    "researcher = li.Session(system=researcher_system)       # system message for the researcher\n",
    "                                                        \n",
    "# create a new branch for each critic\n",
    "for idx, name_ in enumerate(critic_names):\n",
    "    researcher.new_branch(\n",
    "        branch_name=li.nget(critic_configs, [name_, \"name\"]),       # name of the branch\n",
    "        tools=li.nget(critic_configs, [name_, \"tools\"]),            # tools to use for the branch\n",
    "        system=li.nget(critic_configs, [name_, \"system\"])           # branch system message for the critic\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compose Critic cluster workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "async def critic_workflow(context):\n",
    "    researcher.new_branch('critic_chat')\n",
    "\n",
    "    # this inner function represents one step in the critic's workflow\n",
    "    async def _inner(step_num=None, _context=None):        \n",
    "        f = lambda branch_, step_: [f\"{branch_}\", f\"step{step_}\"]\n",
    "        \n",
    "        # this _func function is to be called by a single critic\n",
    "        async def _func(name_):\n",
    "            config_ = {\n",
    "                'instruction': li.nget(critic_configs, f(name_, step_num)), \n",
    "                'num': 5, 'to_': name_, 'temperature': 0.3, 'tools': True,\n",
    "            }\n",
    "            if _context is not None:\n",
    "                config_.update({'context': _context})\n",
    "            try:\n",
    "                out = await researcher.auto_followup(**config_)\n",
    "                return out\n",
    "            except:\n",
    "                return 'somehow failed'\n",
    "\n",
    "        # then we run the _func asynchronously for all critics\n",
    "        await li.alcall(critic_names, _func)\n",
    "\n",
    "    # once after each critic finished their step\n",
    "    # we need to find a way for them to know each other's work, so they conduct peer review and discussion\n",
    "    def update_group_chat():\n",
    "        \n",
    "        # get all responses from critic (this does not include action request nor action response)\n",
    "        # only the finished output for each step after various querying\n",
    "        df_ = lambda name_: researcher.branches[name_].filter_messages_by(sender='assistant').copy()\n",
    "        \n",
    "        # we filter out empty responses and concat the valid ones into a dataframe\n",
    "        lst = li.to_list(\n",
    "            [\n",
    "                df_(name_) if (df_(name_) is not None and len(df_(name_)) >0)\n",
    "                else None for name_ in critic_names\n",
    "            ], \n",
    "                flatten=True, dropna=True)\n",
    "        \n",
    "        dfs = pd.concat(lst)\n",
    "        \n",
    "        # then we update the each critic as well as the group chat on the results\n",
    "        researcher.branches['critic_chat'].extend(dfs)\n",
    "        for name_ in critic_names:\n",
    "            researcher.branches[name_].extend(dfs)\n",
    "\n",
    "\n",
    "    # for the first step, we need to provide context for critics to work on\n",
    "    # which is the responses from the researcher\n",
    "    await _inner(1, _context=context)\n",
    "    update_group_chat()\n",
    "    \n",
    "    # you just need to put the same context once, each branch will keep track all messages\n",
    "    # in a pandas dataframe format, which allows easy manipulation and filtering\n",
    "    for i in range(1,5):\n",
    "        await _inner(i+1)\n",
    "        update_group_chat()\n",
    "    \n",
    "    # now the workflow is done, we are ready to output the final result\n",
    "    msgs = researcher.branches['critic_chat'].messages.copy()\n",
    "    \n",
    "    # and since we send out the whole group chat history, we can delete the group chat branch\n",
    "    # so it doesn't interfere with future workflows \n",
    "    # (branches within a session cannot have duplicated names)\n",
    "    researcher.delete_branch('critic_chat')\n",
    "    \n",
    "    return msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depending on your prompts, this might take a long while as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Instruction 1: read abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we provide the researcher with the an abstract of a paper as a starting point\n",
    "r_context1 = str(abstracts[1])\n",
    "\n",
    "r1 = await researcher.chat(\n",
    "    instruction=researcher_instruct1, \n",
    "    context=r_context1, \n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch critic_chat is deleted.\n"
     ]
    }
   ],
   "source": [
    "# then we provide both the abstract and researcher's response\n",
    "# as context for the critic cluster \n",
    "c_context1 = {\n",
    "    \"paper_summary\": r_context1, \n",
    "    \"researcher_work\": str(r1)\n",
    "}\n",
    "\n",
    "# run critic workflow, the outputs are all messages from the critic cluster group chat\n",
    "reports1 = await critic_workflow(c_context1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Instruction 2: reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch critic_chat is deleted.\n"
     ]
    }
   ],
   "source": [
    "# we get the last 10 valid messages from the group chat as feedback to the researcher\n",
    "r_context2 = str(\n",
    "    [i.content for _, i in reports1.iloc[-10:].iterrows()])\n",
    "\n",
    "r2 = await researcher.chat(\n",
    "    instruction=researcher_instruct2, \n",
    "    context=r_context2,\n",
    "    temperature=0.5)\n",
    "\n",
    "c_context2 = {\"researcher_work\": str(r2)}\n",
    "reports2 = await critic_workflow(c_context2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Instruction 3: brainstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch critic_chat is deleted.\n"
     ]
    }
   ],
   "source": [
    "r_context3 = str(\n",
    "    [i.content for _, i in reports1.iloc[-10:].iterrows()])\n",
    "\n",
    "r3 = await researcher.chat(\n",
    "    instruction=researcher_instruct3, \n",
    "    context=r_context3)\n",
    "\n",
    "c_context3 = {\"researcher_work\": str(r3)}\n",
    "reports3 = await critic_workflow(c_context3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Instruction 4: output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_context4 = str(\n",
    "    [i.content for _, i in reports1.iloc[-10:].iterrows()])\n",
    "\n",
    "r4 = await researcher.chat(\n",
    "    instruction=researcher_instruct4, \n",
    "    context=r_context4)\n",
    "\n",
    "r4_2 = await researcher.chat(\n",
    "    instruction = \"\"\"\n",
    "        integrate conversation, reflect on your work and present \n",
    "        final output to user, be as long as possible\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Check the overall stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_messages = [\n",
    "    i.messages.copy().dropna(how='all') for _, i in researcher.branches.items() \n",
    "    if len(i.messages) > 1\n",
    "]\n",
    "ttl_df = pd.concat(ttl_messages).drop_duplicates(subset=['node_id', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_df.to_csv('all_critics_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 396 entries, 0 to 106\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   node_id    396 non-null    object        \n",
      " 1   role       396 non-null    object        \n",
      " 2   sender     396 non-null    object        \n",
      " 3   timestamp  396 non-null    datetime64[ns]\n",
      " 4   content    396 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 18.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# there are 396 messages unique in total across all branches\n",
    "# covering all critics, researcher, system as well as user instructions\n",
    "ttl_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of reponses from all assistants\n",
    "\n",
    "len(ttl_df[ttl_df.sender == 'assistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of queries answered\n",
    "\n",
    "len(ttl_df[ttl_df.sender == 'action_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check each tool uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of arxiv queries: 25\n",
      "total number of d2l queries: 31\n",
      "total number of bc queries: 9\n",
      "total number of google queries: 18\n",
      "total number of wiki queries: 11\n"
     ]
    }
   ],
   "source": [
    "# you can trace back the source for each single successful query\n",
    "\n",
    "print(f\"total number of arxiv queries: {len(responses_arxiv)}\")\n",
    "print(f\"total number of d2l queries: {len(responses_d2l)}\")\n",
    "print(f\"total number of bc queries: {len(responses_bc)}\")\n",
    "print(f\"total number of google queries: {len(responses_google)}\")\n",
    "print(f\"total number of wiki queries: {len(responses_wiki)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Improve outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the results quality is less than expected, you can chat with the researcher to further improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvements = '''\n",
    "    please be more specific in terms of technical details, use your conversation context to supplement. for example, introduce which technologies, which references should be used where and include some reasoning. you can do it\n",
    "'''\n",
    "\n",
    "r5 = await researcher.chat(improvements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I appreciate your encouragement and the opportunity to delve into a more technically detailed research proposal. Utilizing the context of our previous conversations, letâ€™s articulate a comprehensive plan that integrates Large Language Models (LLMs) with blockchain technology to foster a system of trust.\n",
      "\n",
      "**Title:**\n",
      "\"Design and Implementation of a Trust-Enhancing Framework via LLM-Blockchain Integration\"\n",
      "\n",
      "**Abstract:**\n",
      "This research investigates the technical feasibility and implementation strategies for integrating LLMs with blockchain networks to construct a robust system of trust. By harnessing the linguistic and cognitive capabilities of LLMs and the immutable record-keeping features of blockchain, we aim to develop a platform that ensures data integrity, facilitates transparent transactions, and mitigates the risks of malicious activities across blockchain systems.\n",
      "\n",
      "**Introduction:**\n",
      "Blockchain technology has laid the foundation for creating secure and decentralized digital ledgers. However, as the technology proliferates, the need for more sophisticated trust mechanisms becomes evident. The integration of LLMs, with their advanced natural language processing and understanding capabilities, into blockchain ecosystems holds the promise of revolutionizing trust in these networks. This integration will allow for real-time analysis and validation of on-chain data, smart contracts, and transactions, thus enhancing the overall security and reliability of blockchain platforms.\n",
      "\n",
      "**Research Objectives:**\n",
      "1. Develop a standardized cross-chain query language enabling LLMs to access and interpret data from disparate blockchain networks.\n",
      "2. Engineer a framework that allows LLMs to autonomously perform trust operations, such as transaction verification, fraud detection, and smart contract compliance, across multiple blockchain infrastructures.\n",
      "3. Evaluate the added value of LLMs in the context of blockchain by measuring improvements in data integrity, fraud prevention, and operational compliance.\n",
      "\n",
      "**Literature Review:**\n",
      "An in-depth literature review will form the backbone of the research, focusing on areas such as blockchain interoperability protocols, consensus algorithms, data privacy and security in distributed networks, and the application of AI in trust and verification processes. Key references will be drawn from sources like the Bitcoin and Ethereum whitepapers, IBM's Hyperledger Fabric documentation, Google's BERT language model research, and case studies of cross-chain solutions like Cosmos and Polkadot.\n",
      "\n",
      "**Methodology:**\n",
      "The methodology for this research will encompass a multi-phased approach:\n",
      "\n",
      "1. **Requirement Analysis:**\n",
      "   - Cataloging interoperability challenges and trust issues identified within existing blockchain systems.\n",
      "   - Defining the technical requirements for the LLMs' operation within blockchain systems, including the necessary computational resources and access permissions.\n",
      "\n",
      "2. **Design of Cross-Chain Query Language:**\n",
      "   - Leveraging existing query languages like SQL and blockchain-specific languages such as Solidity to inform the development of a new language schema that encompasses the needs of multiple blockchain systems.\n",
      "   - Ensuring compatibility with different blockchain data models, including UTXO (used by Bitcoin) and account/balance (used by Ethereum).\n",
      "\n",
      "3. **LLM Adaptation and Training:**\n",
      "   - Customizing existing LLMs (e.g., GPT-3, BERT) to understand and generate blockchain-relevant text.\n",
      "   - Training these models on datasets constructed from blockchain transaction logs, smart contract code, and other relevant on-chain data.\n",
      "\n",
      "4. **System Architecture Development:**\n",
      "   - Designing a microservices-based architecture that allows LLMs to interface with blockchain nodes through APIs and webhooks.\n",
      "   - Incorporating event-driven triggers to enable LLMs to respond to real-time blockchain events, such as the creation of a new block or the submission of a smart contract.\n",
      "\n",
      "5. **Prototype Development and Iteration:**\n",
      "   - Developing a prototype system that integrates the cross-chain query language and LLMs in a controlled test environment.\n",
      "   - Applying agile development methodologies to iterate on the system based on real-world data and user feedback.\n",
      "\n",
      "6. **Security and Scalability Analysis:**\n",
      "   - Conducting thorough penetration testing and security analysis to identify and remediate potential vulnerabilities.\n",
      "   - Assessing the scalability of the system, with a focus on the LLMs' response times and accuracy as the volume of blockchain activity scales up.\n",
      "\n",
      "7. **Testing and Evaluation:**\n",
      "   - Deploying the prototype in a sandbox environment that simulates a real-world blockchain ecosystem.\n",
      "   - Evaluating the system's performance through quantitative metrics (e.g., transaction verification time, smart contract analysis accuracy) and qualitative user feedback.\n",
      "\n",
      "8. **Documentation and Dissemination:**\n",
      "   - Documenting the research findings, the developed cross-chain query language specifications, LLM training methods, and the overall system architecture.\n",
      "   - Publishing the results in academic journals, presenting at conferences, and releasing open-source code for community feedback and collaboration.\n",
      "\n",
      "**Expected Outcomes:**\n",
      "A comprehensive framework for LLM-blockchain integration, including a new query language, trained LLMs, and an extensible system architecture, will be the tangible outcomes of this research. The framework will serve as a blueprint for building trust-enhancing applications on blockchain networks.\n",
      "\n",
      "**Potential Challenges:**\n",
      "- Developing a query language that is secure, efficient, and compatible with a variety of blockchain protocols.\n",
      "- Training LLMs to accurately interpret the context and nuances of blockchain data while ensuring user privacy.\n",
      "- Ensuring the computational efficiency of LLMs within the resource-constrained blockchain environments.\n",
      "\n",
      "**Timeline:**\n",
      "The research will follow a detailed 24-month timeline, starting with a six-month period focused on requirements gathering and literature review, followed by phases of design, development, and iterative testing, each lasting approximately four to six months.\n",
      "\n",
      "**Budget:**\n",
      "A comprehensive budget will include allocations for personnel, computational resources (e.g., cloud services for LLM training and deployment), data acquisition costs, software development tools, security audits, and dissemination efforts.\n",
      "\n",
      "**Conclusion:**\n",
      "This research proposal presents a clear and actionable plan for addressing the challenge of building trust in blockchain systems through the integration of LLMs. Through meticulous planning, thorough technical exploration, and a commitment to open collaboration, this project aims to push the frontiers of what is possible at the intersection of AI and blockchain technology.\n",
      "\n",
      "---\n",
      "\n",
      "This highly detailed and technical research proposal incorporates elements from the previous discussion, including the importance of a cross-chain query language and the role of LLMs in trust operations on blockchain networks. It is designed to be a practical, feasible blueprint for groundbreaking research in the field.\n"
     ]
    }
   ],
   "source": [
    "print(r5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
