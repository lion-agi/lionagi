{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi.experimental.compressor.llm_compressor import LLMCompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = \"..........long text..........\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228126"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = aa.replace(\"<|endoftext|>\", \"\")\n",
    "aa = aa.replace(\"\\n\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224038"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lionagi as li\n",
    "\n",
    "imodel = li.iModel(\"gpt-3.5-turbo\", interval_tokens=1_800_000, interval_requests=10_000)\n",
    "compressor = LLMCompressor(\n",
    "    imodel=imodel, \n",
    "    system_msg=\"concisely compress the given text for remembering\", \n",
    "    n_samples=10, \n",
    "    target_ratio=0.08, \n",
    "    split_overlap=0.05, \n",
    "    split_threshold=5, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Token number: 56999\n",
      "Selected Token number: 4536\n",
      "Token Compression Ratio: 0.080\n",
      "Compression Time: 12.1889 seconds\n",
      "Compression Model: gpt-3.5-turbo\n",
      "Compression Method: perplexity\n",
      "Compression Usage: $0.39438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bb = await compressor.compress(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = li.Branch()\n",
    "\n",
    "res1 = await branch.chat(\n",
    "    instruction=\"what does temperature means\",  \n",
    "    context=aa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch.messages[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 61183, 'completion_tokens': 451, 'total_tokens': 61634, 'expense': 0.31268}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Temperature in the context of natural language processing (NLP) and machine learning, particularly when using models like OpenAI's GPT, refers to a parameter that controls the randomness of the model's output. It is a hyperparameter that influences the probability distribution over the possible next tokens (words or characters) generated by the model.\n",
       "\n",
       "Here's a step-by-step explanation of what temperature means and how it affects the model's output:\n",
       "\n",
       "1. **Probability Distribution**:\n",
       "   - When generating text, the model predicts the next token based on a probability distribution. Each token has a certain probability of being the next token in the sequence.\n",
       "\n",
       "2. **Temperature Parameter**:\n",
       "   - The temperature parameter adjusts the sharpness of this probability distribution.\n",
       "   - It is a value between 0 and 2, where:\n",
       "     - **Low Temperature (< 1)**: The model's output becomes more deterministic. The model will favor high-probability tokens, making the text more predictable and focused.\n",
       "     - **High Temperature (> 1)**: The model's output becomes more random. The model will favor a wider range of tokens, including those with lower probabilities, making the text more diverse and creative.\n",
       "\n",
       "3. **Effect on Output**:\n",
       "   - **Temperature = 0**: The model will always choose the token with the highest probability, resulting in very deterministic and repetitive text.\n",
       "   - **Temperature = 1**: The model will sample tokens according to their probabilities without any adjustment, providing a balance between randomness and determinism.\n",
       "   - **Temperature > 1**: The model will sample tokens more randomly, increasing the chances of selecting less probable tokens, which can lead to more varied and creative outputs.\n",
       "\n",
       "4. **Practical Usage**:\n",
       "   - Adjusting the temperature allows users to control the behavior of the model based on the desired outcome. For example:\n",
       "     - For factual and precise responses, a lower temperature is preferred.\n",
       "     - For creative writing or brainstorming, a higher temperature might be more suitable.\n",
       "\n",
       "In summary, the temperature parameter is a crucial tool for controlling the randomness and creativity of the text generated by language models. By adjusting the temperature, users can fine-tune the balance between deterministic and random outputs to suit their specific needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(branch.messages[-1].metadata['extra']['usage'])\n",
    "Markdown(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch2 = li.Branch()\n",
    "res2 = await branch2.chat(\n",
    "    instruction=\"what does vector store means\", \n",
    "    context=bb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 5005, 'completion_tokens': 649, 'total_tokens': 5654, 'expense': 0.03476}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A vector store in the context of data management, especially with AI and machine learning, typically refers to a storage solution designed to manage and efficiently retrieve high-dimensional vectors. These vectors represent data points in high-dimensional space and are often used in various AI tasks like natural language processing, recommendation systems, and computer vision. Here's a detailed explanation:\n",
       "\n",
       "### What is a Vector Store?\n",
       "\n",
       "1. **Definition**:\n",
       "   - A vector store is a specialized data storage system optimized for storing and retrieving vectors. These vectors can represent different types of data, such as words, images, or features produced by machine learning models.\n",
       "\n",
       "2. **Use Cases**:\n",
       "   - **Natural Language Processing (NLP)**: Storing word embeddings (like those from Word2Vec, GloVe, or BERT).\n",
       "   - **Recommendation Systems**: Storing user and item vectors for similarity searches.\n",
       "   - **Computer Vision**: Storing feature vectors of images.\n",
       "\n",
       "3. **Operations**:\n",
       "   - **Save Vectors**: Save high-dimensional vectors into the datastore.\n",
       "   - **Retrieve Vectors**: Retrieve vectors based on specific queries or similarity search.\n",
       "   - **Delete Vectors**: Remove vectors that are no longer needed.\n",
       "\n",
       "4. **Indexing and Retrieval**:\n",
       "   - Vector stores often use special indexing techniques like Approximate Nearest Neighbors (ANN) to enable fast retrieval of vectors similar to a given query vector.\n",
       "\n",
       "### Example API Operations\n",
       "\n",
       "The context provided in the API documentation indicates two operations related to vector stores:\n",
       "\n",
       "1. **Creating a Vector Store File**:\n",
       "   - **Endpoint**: `POST https://api.openai.com/v1/vector_stores/{vector_store_id}/files`\n",
       "   - **Purpose**: To attach a file containing vectors to an existing vector store.\n",
       "\n",
       "2. **Deleting a Vector Store File**:\n",
       "   - **Endpoint**: `DELETE https://api.openai.com/v1/vector_stores/{vector_store_id}`\n",
       "   - **Purpose**: To delete a vector store, which will remove the file associations but not the files themselves.\n",
       "\n",
       "### Example Request for Creating a Vector Store File\n",
       "\n",
       "```bash\n",
       "curl -X POST https://api.openai.com/v1/vector_stores/{vector_store_id}/files \\\n",
       "-H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
       "-H \"Content-Type: application/json\" \\\n",
       "-d '{\n",
       "  \"file_id\": \"file-abc123\"\n",
       "}'\n",
       "```\n",
       "\n",
       "### Example Request for Deleting a Vector Store\n",
       "\n",
       "```bash\n",
       "curl -X DELETE https://api.openai.com/v1/vector_stores/{vector_store_id} \\\n",
       "-H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
       "```\n",
       "\n",
       "### Key Points to Remember\n",
       "\n",
       "- **Vector Store ID**: Each vector store is uniquely identified by a `vector_store_id`.\n",
       "- **File ID**: Files containing vectors are identified by `file_id` and can be linked to or removed from vector stores.\n",
       "- **Efficient Retrieval**: Vector stores typically employ indexing mechanisms to ensure efficient similarity-based retrieval, which is crucial for tasks like recommendation and search.\n",
       "\n",
       "By understanding these concepts and the provided API operations, you can effectively manage and utilize vector stores in AI and machine learning workflows."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(branch2.messages[-1].metadata['extra']['usage'])\n",
    "Markdown(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
