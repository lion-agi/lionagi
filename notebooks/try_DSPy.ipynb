{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "from lionagi.core.prompt.prompt_template import PromptTemplate\n",
    "from lionagi.core.branch import Branch\n",
    "from lionagi.libs import func_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dspy\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "from typing import List, Any\n",
    "\n",
    "# #initial DSPy\n",
    "# turbo = dspy.OpenAI(model='gpt-3.5-turbo')\n",
    "# colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "class rUv(PromptTemplate):\n",
    "    \"\"\"\n",
    "    Recursive Unified Validators (rUv): Generate expert model outputs.\n",
    "    This is the primary function that generates outputs from multiple expert models.\n",
    "    \"\"\"\n",
    "\n",
    "    context: str | dict = Field(desc=\"The current context\")\n",
    "    prompt: str | dict = Field(desc=\"A prompt to guide the language model\")\n",
    "    max_tokens: int = Field(desc=\"Maximum number of tokens to generate\", default=\"1500\")\n",
    "    temperature: float = Field(\n",
    "        desc=\"Temperature for sampling (higher values make output more random)\",\n",
    "        default=\"0.1\",\n",
    "    )\n",
    "    top_k: float = Field(\n",
    "        desc=\"Top K words to sample from (higher values consider more words)\",\n",
    "        default=\"100\",\n",
    "    )\n",
    "    top_p: float = Field(\n",
    "        desc=\"Top P probability threshold (higher values make output more diverse)\",\n",
    "        default=\"0.9\",\n",
    "    )\n",
    "    frequency_penalty: float = Field(\n",
    "        desc=\"Frequency penalty (higher values penalize frequent words)\", default=\"0.0\"\n",
    "    )\n",
    "    presence_penalty: float = Field(\n",
    "        desc=\"Presence penalty (higher values penalize repeated words)\", default=\"0.0\"\n",
    "    )\n",
    "    output: Any | None = Field(None, desc=\"The generated expert model output\")\n",
    "    signature: str = (\n",
    "        \"context, prompt, max_tokens, temperature, top_k, top_p, frequency_penalty, presence_penalty -> output\"\n",
    "    )\n",
    "\n",
    "    # teleprompter = Field(desc=\"Additional context or instructions for the language model\", default=\"matter of fact\")\n",
    "\n",
    "\n",
    "class GatingModel(PromptTemplate):\n",
    "    \"\"\"Assess expert model relevance.\"\"\"\n",
    "\n",
    "    context: str | dict = Field(desc=\"The current context\")\n",
    "    expert_outputs: str = Field(\n",
    "        desc=\"Serialized string of generated expert model outputs\"\n",
    "    )\n",
    "    selected_expert: int | None = Field(\n",
    "        None, desc=\"The index of the selected expert model\"\n",
    "    )\n",
    "    # teleprompter = Field(desc=\"Additional context or instructions for the language model\", default=\"Select the index of the most relevant expert for the given context.\")\n",
    "    signature: str = \"context, experts_outputs -> selected_expert\"\n",
    "\n",
    "\n",
    "class IntrinsicRewardModel(PromptTemplate):\n",
    "    \"\"\"Evaluate the agent's performance intrinsically.\"\"\"\n",
    "\n",
    "    context: str | dict = Field(desc=\"The current context\")\n",
    "    expert_outputs: str = Field(\n",
    "        desc=\"Serialized string of generated expert model outputs\"\n",
    "    )\n",
    "    selected_expert_index: int = Field(desc=\"The index of the selected expert model\")\n",
    "    intrinsic_reward: str | None = Field(\n",
    "        None, desc=\"The intrinsic reward for the agent's performance\"\n",
    "    )\n",
    "    signature: str = (\n",
    "        \"context, expert_outputs, selected_expert_index -> intrinsic_reward\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_experts: int = 4,\n",
    "        min_iterations: int = 4,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        exploration_rate: float = 0.2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the MixtureOfExperts class with default values if the previous code cell isn't run.\n",
    "\n",
    "        Args:\n",
    "            num_experts (int, optional): Number of expert models to use. Defaults to 4.\n",
    "            min_iterations (int, optional): Minimum number of iterations to run. Defaults to 4.\n",
    "            learning_rate (float, optional): Learning rate for updating expert values. Defaults to 0.1.\n",
    "            discount_factor (float, optional): Discount factor for future rewards. Defaults to 0.99.\n",
    "            exploration_rate (float, optional): Exploration rate for selecting experts. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        self.num_experts: int = num_experts\n",
    "        self.expert_outputs: List[str] = []\n",
    "        self.min_iterations: int = min_iterations\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.discount_factor: float = discount_factor\n",
    "        self.exploration_rate: float = exploration_rate\n",
    "        self.expert_values: List[float] = [0.0] * num_experts\n",
    "        self.expert_architectures: List[dict] = [\n",
    "            self.initialize_expert_architecture() for _ in range(num_experts)\n",
    "        ]\n",
    "        self.gating_architecture: dict = self.initialize_gating_architecture()\n",
    "        self.selected_experts_history: List[int] = []\n",
    "\n",
    "    def initialize_expert_architecture(self) -> dict:\n",
    "        \"\"\"Initialize the architecture of an expert model.\"\"\"\n",
    "        return {\n",
    "            \"num_layers\": random.randint(1, 5),\n",
    "            \"hidden_size\": random.randint(32, 256),\n",
    "        }\n",
    "\n",
    "    def initialize_gating_architecture(self) -> dict:\n",
    "        \"\"\"Initialize the architecture of the gating model.\"\"\"\n",
    "        return {\n",
    "            \"num_layers\": random.randint(1, 5),\n",
    "            \"hidden_size\": random.randint(32, 256),\n",
    "        }\n",
    "\n",
    "    async def generate_expert_outputs(\n",
    "        self, context: str, prompt: str, max_tokens: int, guidance: str\n",
    "    ) -> str:\n",
    "        \"\"\"Generate expert outputs based on the given context and prompt.\"\"\"\n",
    "        logging.info(\"Starting to generate expert outputs...\")\n",
    "        print(\"Generating expert outputs...\")\n",
    "\n",
    "        # try:\n",
    "        # generate_expert = dspy.Predict(rUv)\n",
    "        # select_expert = dspy.Predict(GatingModel)\n",
    "        # evaluate_intrinsic_reward = dspy.Predict(IntrinsicRewardModel)\n",
    "        # except Exception as e:\n",
    "        #     logging.error(\"Error initializing DSPy Predict functions: %s\", e)\n",
    "        #     return \"Failed to initialize expert models.\"\n",
    "\n",
    "        for i in range(self.num_experts):\n",
    "            print(f\"Generating output for Expert {i+1}/{self.num_experts}...\")\n",
    "            logging.info(f\"Generating output for Expert {i+1}...\")\n",
    "\n",
    "            try:\n",
    "                expert_prompt = f\"Expert {i+1}: {prompt}\"\n",
    "\n",
    "                # Determine the desired output length based on the previous values\n",
    "                if self.expert_values[i] < 0.2:\n",
    "                    output_length = \"short\"\n",
    "                elif self.expert_values[i] < 0.5:\n",
    "                    output_length = \"medium\"\n",
    "                else:\n",
    "                    output_length = \"long\"\n",
    "\n",
    "                expert_output = \"\"\n",
    "                while True:\n",
    "                    _template = rUv(\n",
    "                        context=context,\n",
    "                        prompt=expert_prompt,\n",
    "                        max_tokens=str(max_tokens),\n",
    "                        temperature=str(random.uniform(0.7, 1.2)),\n",
    "                        top_k=str(random.randint(30, 70)),\n",
    "                        top_p=str(random.uniform(0.8, 1.0)),\n",
    "                        frequency_penalty=str(random.uniform(0.0, 0.5)),\n",
    "                        presence_penalty=str(random.uniform(0.0, 0.5)),\n",
    "                        task=f\"Focus on your area of expertise. Provide a {output_length} response using a {random.choice(['formal', 'casual', 'technical'])} tone.\",\n",
    "                    )\n",
    "\n",
    "                    branch = Branch()\n",
    "                    _out = await branch.chat(prompt_template=_template)\n",
    "\n",
    "                    expert_output += _out.out\n",
    "\n",
    "                    if self.check_output_completeness(expert_output):\n",
    "                        break\n",
    "\n",
    "                    expert_prompt = (\n",
    "                        f\"Expert {i+1} (continued): {prompt}\\n{expert_output}\"\n",
    "                    )\n",
    "\n",
    "                self.expert_outputs.append(expert_output)\n",
    "\n",
    "                print(f\"LLM Parameters for Expert {i+1}:\")\n",
    "                print(f\"Max Tokens per chunk: {max_tokens}\")\n",
    "                print(f\"Temperature: {random.uniform(0.7, 1.2)}\")\n",
    "                print(f\"Top K: {random.randint(30, 70)}\")\n",
    "                print(f\"Top P: {random.uniform(0.8, 1.0)}\")\n",
    "                print(f\"Frequency Penalty: {random.uniform(0.0, 0.5)}\")\n",
    "                print(f\"Presence Penalty: {random.uniform(0.0, 0.5)}\")\n",
    "                print(\"------------------------\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error generating output for Expert %d: %s\", i + 1, e)\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"Output for Expert {i+1} generated.\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            serialized_expert_outputs = \",\".join(self.expert_outputs)\n",
    "\n",
    "            if random.random() < self.exploration_rate:\n",
    "                selected_expert_index = random.randint(0, self.num_experts - 1)\n",
    "            else:\n",
    "                _template = GatingModel(\n",
    "                    context=context,\n",
    "                    expert_outputs=serialized_expert_outputs,\n",
    "                    task=\"Select the index of the most relevant expert for the given context.\",\n",
    "                )\n",
    "\n",
    "                branch.clear_messages()\n",
    "                _out = await branch.chat(prompt_template=_template)\n",
    "\n",
    "                # selected_expert_index = select_expert(context=context, expert_outputs=serialized_expert_outputs, teleprompter=\"Select the index of the most relevant expert for the given context.\").selected_expert\n",
    "                # selected_expert_index = int(_out.selected_expert) if selected_expert_index.isdigit() else 0\n",
    "\n",
    "                # Penalize selection of recently chosen experts\n",
    "                selected_expert_index = _out.selected_expert\n",
    "                if selected_expert_index in self.selected_experts_history:\n",
    "                    selected_expert_index = random.randint(0, self.num_experts - 1)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error selecting the most relevant expert: %s\", e)\n",
    "            return \"Failed to select the most relevant expert.\"\n",
    "\n",
    "        if selected_expert_index < 0 or selected_expert_index >= len(\n",
    "            self.expert_outputs\n",
    "        ):\n",
    "            selected_expert_index = 0\n",
    "\n",
    "        self.selected_experts_history.append(selected_expert_index)\n",
    "\n",
    "        try:\n",
    "            _template = IntrinsicRewardModel(\n",
    "                context=context,\n",
    "                expert_outputs=serialized_expert_outputs,\n",
    "                selected_expert_index=str(selected_expert_index),\n",
    "            )\n",
    "            branch.clear_messages()\n",
    "\n",
    "            _out = await branch.chat(prompt_template=_template)\n",
    "            intrinsic_reward_str = _out.intrinsic_reward\n",
    "\n",
    "            # intrinsic_reward_str = evaluate_intrinsic_reward(context=context, expert_outputs=serialized_expert_outputs, selected_expert_index=str(selected_expert_index)).intrinsic_reward\n",
    "\n",
    "            # Extract numeric reward value from the string\n",
    "            if \"highly effective\" in intrinsic_reward_str.lower():\n",
    "                intrinsic_reward = 1.0\n",
    "            elif \"effective\" in intrinsic_reward_str.lower():\n",
    "                intrinsic_reward = 0.7\n",
    "            elif \"moderate\" in intrinsic_reward_str.lower():\n",
    "                intrinsic_reward = 0.5\n",
    "            elif \"poor\" in intrinsic_reward_str.lower():\n",
    "                intrinsic_reward = 0.2\n",
    "            else:\n",
    "                intrinsic_reward = 0.0\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error evaluating intrinsic reward: %s\", e)\n",
    "            intrinsic_reward = 0.0\n",
    "\n",
    "        print(\"Expert output generation complete!\")\n",
    "        logging.info(\n",
    "            \"All expert outputs have been generated and the most relevant expert has been selected.\"\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.expert_outputs[selected_expert_index],\n",
    "            selected_expert_index,\n",
    "            intrinsic_reward,\n",
    "        )\n",
    "\n",
    "    def check_output_completeness(self, output: str) -> bool:\n",
    "        \"\"\"Check if the output ends with a proper conclusion.\"\"\"\n",
    "        if output.endswith(\".\") or output.endswith(\"!\") or output.endswith(\"?\"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def update_expert_values(self, selected_expert_index: int, reward: float):\n",
    "        \"\"\"Update the value estimate of the selected expert based on the received reward.\"\"\"\n",
    "        self.expert_values[selected_expert_index] += self.learning_rate * (\n",
    "            reward\n",
    "            + self.discount_factor * max(self.expert_values)\n",
    "            - self.expert_values[selected_expert_index]\n",
    "        )\n",
    "\n",
    "    def update_expert_architecture(self, expert_index: int):\n",
    "        \"\"\"Update the architecture of the specified expert model.\"\"\"\n",
    "        self.expert_architectures[expert_index] = self.initialize_expert_architecture()\n",
    "\n",
    "    def update_gating_architecture(self):\n",
    "        \"\"\"Update the architecture of the gating model.\"\"\"\n",
    "        self.gating_architecture = self.initialize_gating_architecture()\n",
    "\n",
    "    def check_termination_condition(self, iteration: int, total_reward: float) -> bool:\n",
    "        \"\"\"Check if the termination condition is met based on the iteration and total reward.\"\"\"\n",
    "        if iteration >= self.min_iterations and total_reward >= 10.0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def update_exploration_rate(self, iteration: int):\n",
    "        \"\"\"Update the exploration rate based on the current iteration.\"\"\"\n",
    "        self.exploration_rate = max(0.1, 1.0 - (iteration / self.min_iterations))\n",
    "\n",
    "\n",
    "# Example usage with adjustments for self-improvement and intrinsic motivation\n",
    "context = \"Acme Corporation is exploring investment opportunities in emerging technologies. The board seeks insights into which technologies could potentially transform their industry over the next decade.\"\n",
    "prompt = \"Evaluate the potential impact and investment viability of artificial intelligence (AI), blockchain, quantum computing, and biotechnology.\"\n",
    "\n",
    "# # Get values from widgets\n",
    "# num_experts = num_experts_widget.value\n",
    "# min_iterations = min_iterations_widget.value\n",
    "# learning_rate = learning_rate_widget.value\n",
    "# discount_factor = discount_factor_widget.value\n",
    "# exploration_rate = exploration_rate_widget.value\n",
    "# context = context_widget.value\n",
    "# prompt = prompt_widget.value\n",
    "# max_tokens = max_tokens_widget.value\n",
    "# guidance = guidance_widget.value\n",
    "\n",
    "\n",
    "num_experts = 5\n",
    "min_iterations = 2\n",
    "learning_rate = 0.2\n",
    "discount_factor = 0.75\n",
    "exploration_rate = 0.5\n",
    "context = \"\"\"\n",
    "Acme Corporation, a leading multinational conglomerate, is actively exploring strategic investment opportunities in emerging technologies to maintain its competitive edge and drive future growth. The board of directors has convened a special committee to conduct a comprehensive analysis of the technological landscape and identify the most promising areas for investment. The committee seeks in-depth insights and recommendations on which cutting-edge technologies have the potential to revolutionize Acme's core industries and create new market opportunities over the next decade.\n",
    "\"\"\"\n",
    "prompt = \"\"\"\n",
    "Conduct a thorough evaluation of the potential impact and investment viability of four key emerging technologies: artificial intelligence (AI), blockchain, quantum computing, and biotechnology. For each technology, provide a detailed assessment of its current state of development, major players in the field, and projected market growth. Analyze the specific applications and use cases within Acme's core industries, highlighting the potential benefits, challenges, and disruptions each technology could bring. Consider factors such as scalability, regulatory landscape, talent availability, and competitive dynamics when assessing the investment viability of each technology. Provide clear recommendations on which technologies Acme should prioritize for investment, along with a proposed allocation of resources and a high-level roadmap for integration into the company's existing operations.\n",
    "\"\"\"\n",
    "max_tokens = 100\n",
    "guidance = \"\"\"\n",
    "Provide a comprehensive and well-structured analysis, focusing on delivering clear, concise, and actionable insights. Use industry-specific terminology and cite relevant data and examples to support your recommendations. Maintain an objective and analytical tone throughout the report.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Instantiate MixtureOfExperts with widget values\n",
    "moe = MixtureOfExperts(\n",
    "    num_experts=num_experts,\n",
    "    min_iterations=min_iterations,\n",
    "    learning_rate=learning_rate,\n",
    "    discount_factor=discount_factor,\n",
    "    exploration_rate=exploration_rate,\n",
    ")\n",
    "for iteration in range(moe.min_iterations):\n",
    "    final_output, selected_expert_index, intrinsic_reward = (\n",
    "        await moe.generate_expert_outputs(context, prompt, max_tokens, guidance)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Iteration {iteration+1} - Selected Expert: {selected_expert_index}, Intrinsic Reward: {intrinsic_reward}\"\n",
    "    )\n",
    "    print(\"Expert Values:\", moe.expert_values)\n",
    "    print(\"Final Expert Output:\")\n",
    "    print(final_output)\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    # Get reward from an external expert\n",
    "    expert_reward = float(input(f\"Enter expert reward for iteration {iteration+1}: \"))\n",
    "\n",
    "    # Combine intrinsic and expert rewards\n",
    "    total_reward = intrinsic_reward + expert_reward\n",
    "\n",
    "    print(f\"Expert Reward: {expert_reward}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # Update the value estimate of the selected expert\n",
    "    moe.update_expert_values(selected_expert_index, total_reward)\n",
    "\n",
    "    # Update expert and gating architectures for self-improvement\n",
    "    if random.random() < 0.2:\n",
    "        moe.update_expert_architecture(selected_expert_index)\n",
    "    if random.random() < 0.1:\n",
    "        moe.update_gating_architecture()\n",
    "\n",
    "    # Update exploration rate based on the current iteration\n",
    "    moe.update_exploration_rate(iteration)\n",
    "\n",
    "    # Check termination condition\n",
    "    if moe.check_termination_condition(iteration, total_reward):\n",
    "        print(\"Termination condition met. Stopping the process.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = await moe.generate_expert_outputs(context, prompt, max_tokens, guidance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
